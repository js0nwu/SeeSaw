{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jwpilly/anaconda/lib/python3.5/site-packages/matplotlib/__init__.py:1069: UserWarning: Bad val \"TkAgg or echo backend : Agg\" on line #1\n",
      "\t\"backend : TkAgg or echo backend : Agg\n",
      "\"\n",
      "\tin file \"/Users/jwpilly/.matplotlib/matplotlibrc\"\n",
      "\tKey backend: Unrecognized backend string \"tkagg or echo backend : agg\": valid strings are ['GTKCairo', 'gdk', 'Qt4Agg', 'WXAgg', 'pdf', 'template', 'cairo', 'GTKAgg', 'ps', 'Qt5Agg', 'GTK3Agg', 'svg', 'GTK', 'WebAgg', 'TkAgg', 'nbAgg', 'WX', 'CocoaAgg', 'agg', 'MacOSX', 'GTK3Cairo', 'pgf', 'emf']\n",
      "  (val, error_details, msg))\n"
     ]
    }
   ],
   "source": [
    "%run imports.py\n",
    "%run lib_util.py\n",
    "#%run lib_plot.py\n",
    "%run lib_summary.py\n",
    "%run lib_misc.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Folder Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done loading raw data\n",
      "done loading subsample data\n",
      "done loading feature\n",
      "done loading offline\n",
      "done loading lag\n",
      "done loading flashes\n"
     ]
    }
   ],
   "source": [
    "# Get all data\n",
    "rawDataSet = loadRawData()\n",
    "subsampleDataSet = loadSubsampleData()\n",
    "featureDataSet = loadFeatureData()\n",
    "offlineCorrDataSet = loadOfflineCorrData()\n",
    "# onlineCorrDataSet = load\n",
    "lagDataSet = loadLagData()\n",
    "leftTicksSet, rightTicksSet = loadFlashData(rawDataSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key: t20180110004819_p2_sit_x1_raw.csv\n",
      "key: t20180110004849_p2_sit_x1_raw.csv\n",
      "key: t20180110004914_p2_sit_x1_raw.csv\n",
      "key: t20180110004944_p2_sit_x1_raw.csv\n",
      "key: t20180110005016_p2_sit_x1_raw.csv\n",
      "key: t20180110005045_p2_sit_x1_raw.csv\n",
      "key: t20180110005116_p2_sit_x1_raw.csv\n",
      "key: t20180110005249_p2_noise_x1_raw.csv\n",
      "key: t20180110005328_p2_noise_x1_raw.csv\n",
      "key: t20180110005402_p2_noise_x1_raw.csv\n",
      "key: t20180110005452_p2_noise_x1_raw.csv\n",
      "key: t20180110005522_p2_noise_x1_raw.csv\n",
      "key: t20180110005553_p2_noise_x1_raw.csv\n",
      "done plotting overview\n",
      "done plotting offline corr\n"
     ]
    }
   ],
   "source": [
    "vizOverview(rawDataSet, subsampleDataSet, featureDataSet, offlineCorrDataSet, lagDataSet, plotSubsample=False)\n",
    "# vizSubsampler(rawDataSet, subsampleDataSet)\n",
    "vizOfflineCorr(offlineCorrDataSet)\n",
    "# vizFeaturePlus(rawDataSet, featureDataSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done loading raw data\n",
      "done loading feature\n"
     ]
    }
   ],
   "source": [
    "rawDataSet = loadRawData()\n",
    "featureDataset = loadFeatureData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vizFeaturePlus(rawDataSet, featureDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vizMagnetometer(rawDataSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vizOverview(rawDataSet, subsampleDataSet, featureDataSet, offlineCorrDataSet, lagDataSet, plotSubsample=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "leftTicksSet, rightTicksSet = loadFlashData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fileToUse = \"t1489504654056_p1_sit_x2_raw.csv\"\n",
    "plt.clf()\n",
    "fig = plt.figure()\n",
    "\n",
    "\n",
    "\n",
    "for key in leftTicksSet.keys():\n",
    "    if key==fileToUse:\n",
    "        leftTicksSet[key]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "leftTicksSet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Overview\n",
    "\n",
    "fileToUse = \"t1489504654056_p1_sit_x2_raw.csv\"\n",
    "\n",
    "# Make folders\n",
    "paths = [figurePath + \"newfigs/\"]\n",
    "for path in paths:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "# Iterate the data set\n",
    "for key in rawDataSet:\n",
    "\n",
    "    if key==fileToUse:\n",
    "    \n",
    "        # Get data\n",
    "        rawData = rawDataSet[key]\n",
    "        subsampleData = subsampleDataSet[key]\n",
    "        featureData = featureDataSet[key[:-7]+\"feature.csv\"]\n",
    "        offlineCorrData = offlineCorrDataSet[key[:-7]+\"offline.csv\"]\n",
    "        lagData = lagDataSet[key[:-7]+\"lag.csv\"]\n",
    "\n",
    "        # Plot magnetometer and feature\n",
    "        path = figurePath + \"newfigs/\"\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        figureFile = path + key[:-4] + \".pdf\"\n",
    "\n",
    "        #df1 = rawdata, df2 = subsampledata, df3 = featuremap, df4 = offlinecorr, df5 = lag\n",
    "        fig, axs = plt.subplots(5, 1, sharex=True, sharey=False)\n",
    "        fig.set_size_inches(15,15)\n",
    "        \n",
    "        plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "        \n",
    "        subsampleData['magnitude'] = np.sqrt(np.square(subsampleData.x)+np.square(subsampleData.y)+np.square(subsampleData.z))\n",
    "        axs[0].plot(subsampleData.timestamp, subsampleData.x, '-k')\n",
    "        axs[1].plot(subsampleData.timestamp, subsampleData.y, '-k')\n",
    "        axs[2].plot(subsampleData.timestamp, subsampleData.z, '-k')\n",
    "        axs[3].plot(subsampleData.timestamp, subsampleData.magnitude, '-k')    \n",
    "        \n",
    "        axs[4].plot(featureData.timestamp, featureData.feature, '-k')\n",
    "    #     axs[5].plot(df4.timestamp, df4.correlation, '-b')\n",
    "    #     axs[5].set_ylim(-1,1)\n",
    "    #     axs[6].plot(df5.timestamp, df5.lagtime, '-b')\n",
    "        axs[0].set_title('x')\n",
    "        axs[1].set_title('y')\n",
    "        axs[2].set_title('z')\n",
    "        axs[3].set_title('magnitude')\n",
    "        axs[4].set_title('feature')\n",
    "    #     axs[5].set_title('offlinecorr')\n",
    "    #     axs[6].set_title('xcorr')\n",
    "        rawLeft = [x for x in rawData.left if x != 0]\n",
    "        rawRight = [x for x in rawData.right if x != 0]    \n",
    "        for i in rawLeft:\n",
    "            axs[0].axvline(i, color='b')\n",
    "            axs[1].axvline(i, color='b')\n",
    "            axs[2].axvline(i, color='b')      \n",
    "            axs[3].axvline(i, color='b')    \n",
    "            axs[4].axvline(i, color='b')\n",
    "    #         axs[6].axvline(i, color='b')\n",
    "        for i in rawRight:\n",
    "            axs[0].axvline(i, color='r')\n",
    "            axs[1].axvline(i, color='r')\n",
    "            axs[2].axvline(i, color='r')    \n",
    "            axs[3].axvline(i, color='r')    \n",
    "            axs[4].axvline(i, color='r')    \n",
    "    #         axs[6].axvline(i, color='r')\n",
    "    #     if(saveToFile):\n",
    "    #         fig.savefig(fileName)\n",
    "    #     else:\n",
    "        plt.show()        \n",
    "        plt.close()    \n",
    "    \n",
    "print(\"done plotting overview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "onlineCorrDataSet = loadLiveCorrData()\n",
    "    \n",
    "# Make folders\n",
    "paths = [figurePath + \"livecorr/\"]\n",
    "for path in paths:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "fig, axs = plt.subplots(1, 1, sharex=True)\n",
    "fig.set_size_inches(15, 15) \n",
    "        \n",
    "# Get all data\n",
    "for key in onlineCorrDataSet:\n",
    "\n",
    "    # Get data\n",
    "    liveCorrData = onlineCorrDataSet[key]\n",
    "\n",
    "    \n",
    "    df = liveCorrData    \n",
    "    df.timestamp = df.timestamp - df.timestamp[0]\n",
    "    axs.plot(df.timestamp, df.correlation)\n",
    "    axs.set_ylim(-1,1)\n",
    "    axs.axhline(0.9)\n",
    "\n",
    "figureFile = figurePath + \"livecorr/\" + \"pilot1corr\" + \".pdf\"\n",
    "plt.show()\n",
    "\n",
    "print(\"done plotting online corr\"    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "offlineCorrDataSet = loadOfflineCorrData()\n",
    "vizOfflineCorr(offlineCorrDataSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "thresholds = [0.70, 0.75, 0.8, 0.85, 0.9, 0.95]\n",
    "outputSet = {}\n",
    "for i in thresholds:\n",
    "    averageTimes = []\n",
    "    for key in onlineCorrDataSet.keys():\n",
    "        df = onlineCorrDataSet[key]\n",
    "        st = df.timestamp[0]\n",
    "        dfsync = df[(df.correlation >= i) & (df.timestamp - st > IGNORE_TIME)]\n",
    "        sync_time = dfsync.timestamp.values[0] - st if len(dfsync > 0) else -1\n",
    "#         print(sync_time)\n",
    "        if sync_time > 0:\n",
    "            averageTimes.append(sync_time)\n",
    "    outputSet[i] = np.mean(averageTimes) / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this block is used to specify which activity to use when generating the 3d plots\n",
    "ACTIVITY = 'activatenotif'\n",
    "xmax = None\n",
    "swipe_time = None\n",
    "# swipe_time = 2.45 # sit\n",
    "# swipe_time = 2.20 # walk\n",
    "thresholds = np.linspace(0.2, 1, 17)\n",
    "times = np.linspace(0, 10, 51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jwpilly/Desktop/seesaw_data/in-the-wild/Synchro/P21/synchro/t20180211144500_p21_activatenotif/\n",
      "/Users/jwpilly/Desktop/seesaw_data/in-the-wild/Synchro/P21/synchro/t20180211144500_p21_activatenotif/\n",
      "/Users/jwpilly/Desktop/seesaw_data/in-the-wild/Synchro/P22/synchro/t20180213173856_p22_activatenotif/\n",
      "/Users/jwpilly/Desktop/seesaw_data/in-the-wild/Synchro/P22/synchro/t20180213173856_p22_activatenotif/\n",
      "/Users/jwpilly/Desktop/seesaw_data/in-the-wild/Synchro/P24/synchro/t20180214200913_p24_activatenotif/\n",
      "/Users/jwpilly/Desktop/seesaw_data/in-the-wild/Synchro/P24/synchro/t20180214200913_p24_activatenotif/\n",
      "/Users/jwpilly/Desktop/seesaw_data/in-the-wild/Synchro/P23/synchro/t20180211133024_p23_activatenotif/\n",
      "/Users/jwpilly/Desktop/seesaw_data/in-the-wild/Synchro/P23/synchro/t20180211133024_p23_activatenotif/\n"
     ]
    }
   ],
   "source": [
    "# this block is used to calculate the true positives and false positives of the 3d plots the data is stored in tps and fps\n",
    "\n",
    "# tp = syncRateFile(\"/Users/jwpilly/Research/Synchro/Study_v2/P1/synchro/t20170401112834_p1_sit/\")\n",
    "# fp = syncRateFile(\"/Users/jwpilly/Research/Synchro/Study_v2/P1/synchro/t20170401112834_p1_sit/\", False)\n",
    "# baseFilePath = \"/Users/jwpilly/Research/Synchro/Study_v2/\"\n",
    "# baseFilePath = \"/Users/jwpilly/Research/Synchro/Study_v1_750/\"\n",
    "# baseFilePath = \"/media/jwpilly/PillowDisk/Research/Synchro/figuredata/figure12/Study_v2\"\n",
    "# baseFilePath = \"/home/jwpilly/Downloads/seesaw_data/data1/Synchro/\"\n",
    "# baseFilePath = \"/Users/jwpilly/Desktop/seesaw_data/data1/Synchro/\"\n",
    "baseFilePath = \"/Users/jwpilly/Desktop/seesaw_data/in-the-wild/Synchro/\"\n",
    "tps = {}\n",
    "fps = {}\n",
    "ttot = {}\n",
    "ftot = {}\n",
    "for root, folders, files in os.walk(baseFilePath):\n",
    "    if not root.endswith(\"synchro\") or \"MACOSX\" in root:\n",
    "        continue\n",
    "    sfolders = sorted(folders, key=lambda x : int(x.split(\"_\")[0].replace(\"t\", \"\")))\n",
    "    efolders = [f for f in sfolders[:] if \"prep\" not in f in f]\n",
    "    if ACTIVITY != \"all\":\n",
    "        efolderpaths = [root + \"/\" + f + \"/\" for f in efolders if ACTIVITY in f]\n",
    "    else:\n",
    "        efolderpaths = [root + \"/\" + f + \"/\" for f in efolders]\n",
    "    etps = [syncRateFile(efolderpaths[i], xm=xmax) for i in range(len(efolderpaths))]\n",
    "    efps = [syncRateFile(efolderpaths[i], True, xm=xmax) for i in range(len(efolderpaths))]\n",
    "    for th in thresholds:\n",
    "        if th not in tps:\n",
    "            tps[th] = np.array([0 for i in times])\n",
    "        if th not in fps:\n",
    "            fps[th] = np.array([0 for i in times])\n",
    "        if th not in ttot:\n",
    "            ttot[th] = np.array([0 for i in times])\n",
    "        if th not in ftot:\n",
    "            ftot[th] = np.array([0 for i in times])\n",
    "        for stps in etps:\n",
    "            tps[th] += np.array(stps[th][0])\n",
    "            ttot[th] += np.array(stps[th][1])\n",
    "        for sfps in efps:\n",
    "            fps[th] += np.array(sfps[th][0])\n",
    "            ftot[th] += np.array(sfps[th][1])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jwpilly/Desktop/seesaw_data/in-the-wild/Synchro/P21/synchro/t20180211144500_p21_activatenotif/\n",
      "/Users/jwpilly/Desktop/seesaw_data/in-the-wild/Synchro/P22/synchro/t20180213173856_p22_activatenotif/\n",
      "/Users/jwpilly/Desktop/seesaw_data/in-the-wild/Synchro/P24/synchro/t20180214200913_p24_activatenotif/\n",
      "/Users/jwpilly/Desktop/seesaw_data/in-the-wild/Synchro/P23/synchro/t20180211133024_p23_activatenotif/\n"
     ]
    }
   ],
   "source": [
    "# this block is used to calculate the true positives and false positives of the 3d plots the data is stored in tps and fps\n",
    "\n",
    "# tp = syncRateFile(\"/Users/jwpilly/Research/Synchro/Study_v2/P1/synchro/t20170401112834_p1_sit/\")\n",
    "# fp = syncRateFile(\"/Users/jwpilly/Research/Synchro/Study_v2/P1/synchro/t20170401112834_p1_sit/\", False)\n",
    "# baseFilePath = \"/Users/jwpilly/Research/Synchro/Study_v2/\"\n",
    "# baseFilePath = \"/Users/jwpilly/Research/Synchro/Study_v1_750/\"\n",
    "# baseFilePath = \"/media/jwpilly/PillowDisk/Research/Synchro/figuredata/figure12/Study_v2\"\n",
    "# baseFilePath = \"/home/jwpilly/Downloads/seesaw_data/data1/Synchro/\"\n",
    "# baseFilePath = \"/Users/jwpilly/Desktop/seesaw_data/data1/Synchro/\"\n",
    "baseFilePath = \"/Users/jwpilly/Desktop/seesaw_data/in-the-wild/Synchro/\"\n",
    "fps = {}\n",
    "for root, folders, files in os.walk(baseFilePath):\n",
    "    if not root.endswith(\"synchro\") or \"MACOSX\" in root:\n",
    "        continue\n",
    "    sfolders = sorted(folders, key=lambda x : int(x.split(\"_\")[0].replace(\"t\", \"\")))\n",
    "    efolders = [f for f in sfolders[:] if \"prep\" not in f in f]\n",
    "    if ACTIVITY != \"all\":\n",
    "        efolderpaths = [root + \"/\" + f + \"/\" for f in efolders if ACTIVITY in f]\n",
    "    else:\n",
    "        efolderpaths = [root + \"/\" + f + \"/\" for f in efolders]\n",
    "    efps = [fpCountFile(efolderpaths[i]) for i in range(len(efolderpaths))]\n",
    "    for th in thresholds:\n",
    "        if th not in fps:\n",
    "            fps[th] = 0\n",
    "        #fps[th] += efps[th]\n",
    "        for efp in efps:\n",
    "            fps[th] += efp[th]\n",
    "        \n",
    "for th in fps: # 3 hours of noise for each condition\n",
    "    fps[th] = fps[th] / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fps1 = fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fps2 = fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.20000000000000001: 27.666666666666668, 0.25: 26.0, 0.35000000000000003: 25.0, 0.90000000000000013: 0.0, 0.60000000000000009: 15.666666666666666, 1.0: 0.0, 0.69999999999999996: 12.333333333333334, 0.85000000000000009: 1.0, 0.75: 9.0, 0.65000000000000002: 14.0, 0.94999999999999996: 0.0, 0.30000000000000004: 25.333333333333332, 0.55000000000000004: 17.333333333333332, 0.45000000000000001: 22.0, 0.40000000000000002: 23.666666666666668, 0.80000000000000004: 5.0, 0.5: 19.666666666666668}\n"
     ]
    }
   ],
   "source": [
    "print(fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jwpilly/anaconda/lib/python3.5/site-packages/matplotlib/axes/_axes.py:519: UserWarning: No labelled objects found. Use label='...' kwarg on individual plots.\n",
      "  warnings.warn(\"No labelled objects found. \"\n"
     ]
    }
   ],
   "source": [
    "plt.clf()\n",
    "ind = np.arange(len(thresholds))\n",
    "labels = [\"{0:.2f}\".format(thresholds[i]) for i in range(len(thresholds))]\n",
    "y1 = [fps1[thresholds[i]] for i in range(len(thresholds))]\n",
    "y2 = [fps2[thresholds[i]] for i in range(len(thresholds))]\n",
    "plt.plot(thresholds, y1, marker=\"o\", linestyle=\"None\")\n",
    "plt.plot(thresholds, y2, marker=\"^\", linestyle=\"None\")\n",
    "#plt.xticks( 0.1 + ind + width / 1.5, labels)\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"False Positives per Hour\")\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.2))\n",
    "plt.savefig(\"inthewildfph.png\", bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jwpilly/anaconda/lib/python3.5/site-packages/numpy/ma/core.py:6385: MaskedArrayFutureWarning: In the future the default for ma.maximum.reduce will be axis=0, not the current None, to match np.maximum.reduce. Explicitly pass 0 or None to silence this warning.\n",
      "  return self.reduce(a)\n",
      "/Users/jwpilly/anaconda/lib/python3.5/site-packages/numpy/ma/core.py:6385: MaskedArrayFutureWarning: In the future the default for ma.minimum.reduce will be axis=0, not the current None, to match np.minimum.reduce. Explicitly pass 0 or None to silence this warning.\n",
      "  return self.reduce(a)\n"
     ]
    }
   ],
   "source": [
    "# this block is used to generate the precision graph with 0.5 * tp + 0.5 tn\n",
    "\n",
    "precisions = []\n",
    "for th in thresholds:\n",
    "    for ti in range(len(times)):\n",
    "        ptime = times[ti]\n",
    "        if ptime < IGNORE_TIME / 1000:\n",
    "            continue\n",
    "        tp_rate = tps[th][ti] / ttot[th][ti]\n",
    "        fp_rate = fps[th][ti] / ftot[th][ti]\n",
    "        prec = 0.5 * tp_rate + 0.5 * (1 - fp_rate)\n",
    "#         prec = tp_rate\n",
    "        point = [th, ptime, prec]\n",
    "        precisions.append(point)\n",
    "precisions = np.array(precisions)\n",
    "xd = precisions[:,0]\n",
    "yd = precisions[:,1]\n",
    "zd = precisions[:,2]\n",
    "\n",
    "import matplotlib.mlab as mlab\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.collections import PolyCollection\n",
    "from matplotlib.colors import colorConverter\n",
    "from matplotlib import cm\n",
    "\n",
    "\n",
    "plt.clf()\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "# ax = fig.gca(projection='3d')\n",
    "ax = fig.gca()\n",
    "xi = np.linspace(np.min(xd), np.max(xd))\n",
    "yi = np.linspace(np.min(yd), np.max(yd))\n",
    "X, Y = np.meshgrid(xi, yi)\n",
    "Z = mlab.griddata(xd, yd, zd, xi, yi, interp='linear')\n",
    "# Z = precisions\n",
    "# surf = ax.plot_surface(X, Y, Z, cmap=cm.jet)\n",
    "# ax.set_zlim3d(np.min(Z), np.max(Z))\n",
    "# fig.colorbar(surf)\n",
    "# contour = ax.contour(xd, yd, zd, cmap=cm.jet)\n",
    "contour = ax.contourf(X, Y, Z, np.linspace(0.0, 1.0, 21, endpoint=True), cmap=cm.jet)\n",
    "# contour = ax.contourf(X, Y, precisions, np.linspace(0.0, 1.0, 21, endpoint=True), cmap=cm.jet)\n",
    "\n",
    "if swipe_time is not None:\n",
    "    ax.axhline(y=swipe_time, linestyle=\"dashed\", color=\"black\", lw=2.5)\n",
    "\n",
    "fig.colorbar(contour)\n",
    "\n",
    "\n",
    "ax.set_xlabel(\"Threshold\")\n",
    "ax.set_ylabel(\"Time (s)\")\n",
    "plt.savefig(\"3d_all_\" + ACTIVITY + \".png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jwpilly/anaconda/lib/python3.5/site-packages/numpy/ma/core.py:6385: MaskedArrayFutureWarning: In the future the default for ma.maximum.reduce will be axis=0, not the current None, to match np.maximum.reduce. Explicitly pass 0 or None to silence this warning.\n",
      "  return self.reduce(a)\n",
      "/Users/jwpilly/anaconda/lib/python3.5/site-packages/numpy/ma/core.py:6385: MaskedArrayFutureWarning: In the future the default for ma.minimum.reduce will be axis=0, not the current None, to match np.minimum.reduce. Explicitly pass 0 or None to silence this warning.\n",
      "  return self.reduce(a)\n"
     ]
    }
   ],
   "source": [
    "# this graph is used to generate the true positive graph\n",
    "\n",
    "precisions = []\n",
    "for th in thresholds:\n",
    "    for ti in range(len(times)):\n",
    "        ptime = times[ti]\n",
    "        if ptime < IGNORE_TIME / 1000:\n",
    "            continue\n",
    "        tp_rate = tps[th][ti] / ttot[th][ti]\n",
    "        prec = tp_rate\n",
    "        point = [th, ptime, prec]\n",
    "        precisions.append(point)\n",
    "precisions = np.array(precisions)\n",
    "xd = precisions[:,0]\n",
    "yd = precisions[:,1]\n",
    "zd = precisions[:,2]\n",
    "\n",
    "import matplotlib.mlab as mlab\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.collections import PolyCollection\n",
    "from matplotlib.colors import colorConverter\n",
    "from matplotlib import cm\n",
    "\n",
    "\n",
    "plt.clf()\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "# ax = fig.gca(projection='3d')\n",
    "ax = fig.gca()\n",
    "xi = np.linspace(np.min(xd), np.max(xd))\n",
    "yi = np.linspace(np.min(yd), np.max(yd))\n",
    "X, Y = np.meshgrid(xi, yi)\n",
    "Z = mlab.griddata(xd, yd, zd, xi, yi, interp='linear')\n",
    "# Z = precisions\n",
    "# surf = ax.plot_surface(X, Y, Z, cmap=cm.jet)\n",
    "# ax.set_zlim3d(np.min(Z), np.max(Z))\n",
    "# fig.colorbar(surf)\n",
    "# contour = ax.contour(xd, yd, zd, cmap=cm.jet)\n",
    "contour = ax.contourf(X, Y, Z, np.linspace(0.0, 1.0, 21, endpoint=True), cmap=cm.jet)\n",
    "# contour = ax.contourf(X, Y, precisions, np.linspace(0.0, 1.0, 21, endpoint=True), cmap=cm.jet)\n",
    "\n",
    "if swipe_time is not None:\n",
    "    ax.axhline(y=swipe_time, linestyle=\"dashed\", color=\"black\", lw=2.5)\n",
    "\n",
    "fig.colorbar(contour)\n",
    "\n",
    "\n",
    "ax.set_xlabel(\"Threshold\")\n",
    "ax.set_ylabel(\"Time (s)\")\n",
    "plt.savefig(\"3d_tp_\" + ACTIVITY + \".png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jwpilly/anaconda/lib/python3.5/site-packages/numpy/ma/core.py:6385: MaskedArrayFutureWarning: In the future the default for ma.maximum.reduce will be axis=0, not the current None, to match np.maximum.reduce. Explicitly pass 0 or None to silence this warning.\n",
      "  return self.reduce(a)\n",
      "/Users/jwpilly/anaconda/lib/python3.5/site-packages/numpy/ma/core.py:6385: MaskedArrayFutureWarning: In the future the default for ma.minimum.reduce will be axis=0, not the current None, to match np.minimum.reduce. Explicitly pass 0 or None to silence this warning.\n",
      "  return self.reduce(a)\n"
     ]
    }
   ],
   "source": [
    "# this graph is used to generate the true positive graph\n",
    "\n",
    "precisions = []\n",
    "for th in thresholds:\n",
    "    for ti in range(len(times)):\n",
    "        ptime = times[ti]\n",
    "        if ptime < IGNORE_TIME / 1000:\n",
    "            continue\n",
    "        tp_rate = fps[th][ti] / ttot[th][ti]\n",
    "        prec = tp_rate\n",
    "        point = [th, ptime, prec]\n",
    "        precisions.append(point)\n",
    "precisions = np.array(precisions)\n",
    "xd = precisions[:,0]\n",
    "yd = precisions[:,1]\n",
    "zd = precisions[:,2]\n",
    "\n",
    "import matplotlib.mlab as mlab\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.collections import PolyCollection\n",
    "from matplotlib.colors import colorConverter\n",
    "from matplotlib import cm\n",
    "\n",
    "\n",
    "plt.clf()\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "# ax = fig.gca(projection='3d')\n",
    "ax = fig.gca()\n",
    "xi = np.linspace(np.min(xd), np.max(xd))\n",
    "yi = np.linspace(np.min(yd), np.max(yd))\n",
    "X, Y = np.meshgrid(xi, yi)\n",
    "Z = mlab.griddata(xd, yd, zd, xi, yi, interp='linear')\n",
    "# Z = precisions\n",
    "# surf = ax.plot_surface(X, Y, Z, cmap=cm.jet)\n",
    "# ax.set_zlim3d(np.min(Z), np.max(Z))\n",
    "# fig.colorbar(surf)\n",
    "# contour = ax.contour(xd, yd, zd, cmap=cm.jet)\n",
    "contour = ax.contourf(X, Y, Z, np.linspace(0.0, 1.0, 21, endpoint=True), cmap=cm.jet)\n",
    "# contour = ax.contourf(X, Y, precisions, np.linspace(0.0, 1.0, 21, endpoint=True), cmap=cm.jet)\n",
    "\n",
    "if swipe_time is not None:\n",
    "    ax.axhline(y=swipe_time, linestyle=\"dashed\", color=\"black\", lw=2.5)\n",
    "\n",
    "fig.colorbar(contour)\n",
    "\n",
    "\n",
    "ax.set_xlabel(\"Threshold\")\n",
    "ax.set_ylabel(\"Time (s)\")\n",
    "plt.savefig(\"3d_fp_\" + ACTIVITY + \".png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jwpilly/anaconda/lib/python3.5/site-packages/numpy/ma/core.py:6385: MaskedArrayFutureWarning: In the future the default for ma.maximum.reduce will be axis=0, not the current None, to match np.maximum.reduce. Explicitly pass 0 or None to silence this warning.\n",
      "  return self.reduce(a)\n",
      "/Users/jwpilly/anaconda/lib/python3.5/site-packages/numpy/ma/core.py:6385: MaskedArrayFutureWarning: In the future the default for ma.minimum.reduce will be axis=0, not the current None, to match np.minimum.reduce. Explicitly pass 0 or None to silence this warning.\n",
      "  return self.reduce(a)\n",
      "/Users/jwpilly/anaconda/lib/python3.5/site-packages/matplotlib/contour.py:1538: UserWarning: Log scale: values of z <= 0 have been masked\n",
      "  warnings.warn('Log scale: values of z <= 0 have been masked')\n"
     ]
    },
    {
     "ename": "SystemError",
     "evalue": "<built-in method create_filled_contour of matplotlib.QuadContourGenerator object at 0x11d6c5f60> returned a result with an error set",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;31mValueError\u001b[0m: filled contour levels must be increasing",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-57a58a62a816>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogNorm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mlvls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m26\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mcontour\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontourf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlvls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLogNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;31m# contour = ax.contourf(X, Y, precisions, np.linspace(0.0, 1.0, 21, endpoint=True), cmap=cm.jet)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jwpilly/anaconda/lib/python3.5/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1810\u001b[0m                     warnings.warn(msg % (label_namer, func.__name__),\n\u001b[1;32m   1811\u001b[0m                                   RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1812\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1813\u001b[0m         \u001b[0mpre_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1814\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpre_doc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jwpilly/anaconda/lib/python3.5/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mcontourf\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   5650\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcla\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5651\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'filled'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5652\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmcontour\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQuadContourSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5653\u001b[0m     \u001b[0mcontourf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmcontour\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQuadContourSet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontour_doc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jwpilly/anaconda/lib/python3.5/site-packages/matplotlib/contour.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, ax, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1422\u001b[0m         \u001b[0mare\u001b[0m \u001b[0mdescribed\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mQuadContourSet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontour_doc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1423\u001b[0m         \"\"\"\n\u001b[0;32m-> 1424\u001b[0;31m         \u001b[0mContourSet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_process_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jwpilly/anaconda/lib/python3.5/site-packages/matplotlib/contour.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, ax, *args, **kwargs)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_colors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallsegs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallkinds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_allsegs_and_allkinds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jwpilly/anaconda/lib/python3.5/site-packages/matplotlib/contour.py\u001b[0m in \u001b[0;36m_get_allsegs_and_allkinds\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1500\u001b[0m                     \u001b[0mvertices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkinds\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1501\u001b[0m                         self._contour_generator.create_filled_contour(\n\u001b[0;32m-> 1502\u001b[0;31m                                                            level, level_upper)\n\u001b[0m\u001b[1;32m   1503\u001b[0m                 \u001b[0mallsegs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvertices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1504\u001b[0m                 \u001b[0mallkinds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkinds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSystemError\u001b[0m: <built-in method create_filled_contour of matplotlib.QuadContourGenerator object at 0x11d6c5f60> returned a result with an error set"
     ]
    }
   ],
   "source": [
    "# this block is used to generate the false positives per hour 3d graph\n",
    "\n",
    "precisions = []\n",
    "for th in thresholds:\n",
    "    for ti in range(len(times)):\n",
    "        ptime = times[ti]\n",
    "        if ptime < IGNORE_TIME / 1000:\n",
    "            continue\n",
    "        prec = (fps[th][ti] / ftot[th][ti]) / 6 * 3600\n",
    "        point = [th, ptime, prec]\n",
    "        precisions.append(point)\n",
    "precisions = np.array(precisions)\n",
    "xd = precisions[:,0]\n",
    "yd = precisions[:,1]\n",
    "zd = precisions[:,2]\n",
    "\n",
    "import matplotlib.mlab as mlab\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.collections import PolyCollection\n",
    "from matplotlib.colors import colorConverter\n",
    "from matplotlib import cm\n",
    "\n",
    "\n",
    "plt.clf()\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "# ax = fig.gca(projection='3d')\n",
    "ax = fig.gca()\n",
    "xi = np.linspace(np.min(xd), np.max(xd))\n",
    "yi = np.linspace(np.min(yd), np.max(yd))\n",
    "X, Y = np.meshgrid(xi, yi)\n",
    "Z = mlab.griddata(xd, yd, zd, xi, yi, interp='linear')\n",
    "# Z = precisions\n",
    "# surf = ax.plot_surface(X, Y, Z, cmap=cm.jet)\n",
    "# ax.set_zlim3d(np.min(Z), np.max(Z))\n",
    "# fig.colorbar(surf)\n",
    "# contour = ax.contour(xd, yd, zd, cmap=cm.jet\n",
    "from matplotlib.colors import LogNorm\n",
    "lvls = np.logspace(0, np.log10(np.max(zd)), 26, endpoint=True)\n",
    "contour = ax.contourf(X, Y, Z, levels=lvls, cmap=cm.jet, norm=LogNorm())\n",
    "# contour = ax.contourf(X, Y, precisions, np.linspace(0.0, 1.0, 21, endpoint=True), cmap=cm.jet)\n",
    "\n",
    "if swipe_time is not None:\n",
    "    ax.axhline(y=swipe_time, linestyle=\"dashed\", color=\"black\", lw=2.5)\n",
    "\n",
    "fig.colorbar(contour, ticks=lvls, format=\"%.1f\")\n",
    "\n",
    "\n",
    "ax.set_xlabel(\"Threshold\")\n",
    "ax.set_ylabel(\"Time (s)\")\n",
    "plt.savefig(\"3d_fph_\" + ACTIVITY + \".png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this graph is used to generate the false positive rate graph\n",
    "\n",
    "precisions = []\n",
    "for th in thresholds:\n",
    "    for ti in range(len(times)):\n",
    "        ptime = times[ti]\n",
    "        if ptime < IGNORE_TIME / 1000:\n",
    "            continue\n",
    "        fp_rate = fps[th][ti] / ftot[th][ti]\n",
    "        prec = fp_rate\n",
    "        point = [th, ptime, prec]\n",
    "        precisions.append(point)\n",
    "precisions = np.array(precisions)\n",
    "xd = precisions[:,0]\n",
    "yd = precisions[:,1]\n",
    "zd = precisions[:,2]\n",
    "\n",
    "import matplotlib.mlab as mlab\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.collections import PolyCollection\n",
    "from matplotlib.colors import colorConverter\n",
    "from matplotlib import cm\n",
    "\n",
    "\n",
    "plt.clf()\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "# ax = fig.gca(projection='3d')\n",
    "ax = fig.gca()\n",
    "xi = np.linspace(np.min(xd), np.max(xd))\n",
    "yi = np.linspace(np.min(yd), np.max(yd))\n",
    "X, Y = np.meshgrid(xi, yi)\n",
    "Z = mlab.griddata(xd, yd, zd, xi, yi, interp='linear')\n",
    "# Z = precisions\n",
    "# surf = ax.plot_surface(X, Y, Z, cmap=cm.jet)\n",
    "# ax.set_zlim3d(np.min(Z), np.max(Z))\n",
    "# fig.colorbar(surf)\n",
    "# contour = ax.contour(xd, yd, zd, cmap=cm.jet)\n",
    "contour = ax.contourf(X, Y, Z, np.linspace(0.0, 1.0, 21, endpoint=True), cmap=cm.jet)\n",
    "# contour = ax.contourf(X, Y, precisions, np.linspace(0.0, 1.0, 21, endpoint=True), cmap=cm.jet)\n",
    "\n",
    "if swipe_time is not None:\n",
    "    ax.axhline(y=swipe_time, linestyle=\"dashed\", color=\"black\", lw=2.5)\n",
    "\n",
    "fig.colorbar(contour)\n",
    "\n",
    "\n",
    "ax.set_xlabel(\"Threshold\")\n",
    "ax.set_ylabel(\"Time (s)\")\n",
    "plt.savefig(\"3d_fpr_\" + ACTIVITY + \".png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tp = syncRateFile(\"/Users/jwpilly/Research/Synchro/Study_v2/P1/synchro/t20170401112834_p1_sit/\", True)\n",
    "tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# correlateSyncAccelFile(\"/Users/jwpilly/Research/Synchro/Study_v2/P1/synchro/t20170401112834_p1_sit/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "processSyncFile(\"/Users/jwpilly/Research/Synchro/Study_v2/P1/synchro/t20170401112834_p1_sit/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this block iterates through all of the study data files to generate the sync times graph\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "# baseFilePath = \"/Users/jwpilly/Documents/Research/Synchro/Study_v2/\"\n",
    "# baseFilePath = \"/Users/jwpilly/Research/Synchro/Study_v2/\"\n",
    "baseFilePath = \"/Volumes/HanSolo/Dropbox/Georgia Tech/Synchro/Data/Study_v1/Flat/sync1000/\"\n",
    "presults = []\n",
    "for root, folders, files in os.walk(baseFilePath):\n",
    "    if not root.endswith(\"synchro\") or \"MACOSX\" in root:\n",
    "        continue\n",
    "    sfolders = sorted(folders, key=lambda x : int(x.split(\"_\")[0].replace(\"t\", \"\")))\n",
    "    efolders = [f for f in sfolders[:] if \"prep\" not in f]\n",
    "    efolderpaths = [root + \"/\" + f + \"/\" for f in efolders]\n",
    "#     print(efolderpaths)\n",
    "    efoldertimes = [(efolderpaths[i].split(\"_\")[-1][:-1] + str(i), processSyncFile(efolderpaths[i])) for i in range(len(efolderpaths))]\n",
    "    presults.append(efoldertimes)\n",
    "thresholds = [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.60, 0.65, 0.70, 0.75, 0.8, 0.85, 0.9, 0.95]\n",
    "# print(efoldertimes)\n",
    "syncresults = []\n",
    "for th in thresholds:\n",
    "    presultsa = [[i[1][th] for i in t] for t in [sorted(r) for r in presults]]\n",
    "    paverages = reduce(lambda x, y : np.array(x) + np.array(y), presultsa) / len(presultsa)\n",
    "    syncresults.append(paverages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this block iterates through all of the study data files to generate the sync times graph but for the noise files\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "# baseFilePath = \"/Users/jwpilly/Documents/Research/Synchro/Study_v2/\"\n",
    "baseFilePath = \"/Users/jwpilly/Research/Synchro/Study_v2/\"\n",
    "presults = []\n",
    "for root, folders, files in os.walk(baseFilePath):\n",
    "    if not root.endswith(\"synchro\") or \"MACOSX\" in root:\n",
    "        continue\n",
    "    sfolders = sorted(folders, key=lambda x : int(x.split(\"_\")[0].replace(\"t\", \"\")))\n",
    "    efolders = [f for f in sfolders[:] if \"prep\" not in f]\n",
    "    efolderpaths = [root + \"/\" + f + \"/\" for f in efolders]\n",
    "#     print(efolderpaths)\n",
    "    efoldertimes = [(efolderpaths[i].split(\"_\")[-1][:-1] + str(i), processSyncFile(efolderpaths[i], True)) for i in range(len(efolderpaths))]\n",
    "    presults.append(efoldertimes)\n",
    "thresholds = [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.60, 0.65, 0.70, 0.75, 0.8, 0.85, 0.9, 0.95]\n",
    "# print(efoldertimes)\n",
    "syncresults = []\n",
    "for th in thresholds:\n",
    "    presultsa = [[i[1][th] for i in t] for t in [sorted(r) for r in presults]]\n",
    "    paverages = reduce(lambda x, y : np.array(x) + np.array(y), presultsa) / len(presultsa)\n",
    "    syncresults.append(paverages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this block plots the results of the sync times graph. depending on which block was run before this one, the graph outputted is for sync data/noise data\n",
    "\n",
    "%matplotlib inline\n",
    "plt.clf()\n",
    "x = thresholds\n",
    "figure = plt.figure(figsize=(15, 7))\n",
    "figure.add_subplot(121)\n",
    "plt.title(\"Sync Times - Sitting\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Threshold\")\n",
    "plt.ylim(0, 1)\n",
    "plt.plot([syncresults[i][0] for i in range(len(x))],x, label=\"Sit Practice 1\")\n",
    "plt.plot([syncresults[i][1] for i in range(len(x))],x, label=\"Sit Practice 2\")\n",
    "plt.plot([syncresults[i][2] for i in range(len(x))],x, label=\"Sit Eval 1\")\n",
    "plt.plot([syncresults[i][3] for i in range(len(x))],x, label=\"Sit Eval 2\")\n",
    "plt.legend(loc=\"best\")\n",
    "figure.add_subplot(122)\n",
    "plt.title(\"Sync Times - Walking\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Threshold\")\n",
    "plt.ylim(0, 1)\n",
    "plt.plot([syncresults[i][4] for i in range(len(x))], x, label=\"Walk Practice 1\")\n",
    "plt.plot([syncresults[i][5] for i in range(len(x))], x, label=\"Walk Practice 2\")\n",
    "plt.plot([syncresults[i][6] for i in range(len(x))], x, label=\"Walk Eval 1\")\n",
    "plt.plot([syncresults[i][7] for i in range(len(x))], x, label=\"Walk Eval 2\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.savefig(\"synctimes.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this block iterates through the study data and calculates the data needed for the correlation over time graph\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "# baseFilePath = \"/Users/jwpilly/Documents/Research/Synchro/Study_v2/\"\n",
    "baseFilePath = \"/Users/jwpilly/Research/Synchro/Study_v2/\"\n",
    "# baseFilePath = \"/Volumes/HanSolo/Dropbox/Georgia Tech/Synchro/Data/Study_v1/Flat/\"\n",
    "# baseFilePath = \"/Volumes/HanSolo/Dropbox/Georgia Tech/Synchro/Data/Study_v2/\"\n",
    "presults = []\n",
    "for root, folders, files in os.walk(baseFilePath):\n",
    "    if not root.endswith(\"synchro\") or \"MACOSX\" in root:\n",
    "        continue\n",
    "    sfolders = sorted(folders, key=lambda x : int(x.split(\"_\")[0].replace(\"t\", \"\")))\n",
    "    efolders = [f for f in sfolders[:] if \"prep\" not in f]\n",
    "    efolderpaths = [root + \"/\" + f + \"/\" for f in efolders]\n",
    "#     print(efolderpaths)\n",
    "    efoldertimes = [(efolderpaths[i].split(\"_\")[-1][:-1] + str(i), correlateSyncFile(efolderpaths[i])) for i in range(len(efolderpaths))]\n",
    "#     print(efoldertimes)\n",
    "    presults.append(efoldertimes)\n",
    "presultsa = [[i[1] for i in t] for t in [sorted(r) for r in presults]]\n",
    "paverages = reduce(lambda x, y : np.array(x) + np.array(y), presultsa) / len(presultsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this block iterates through the study data and calculates the data needed for the correlation over time graph\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "# baseFilePath = \"/Users/jwpilly/Documents/Research/Synchro/Study_v2/\"\n",
    "# baseFilePath = \"/Users/jwpilly/Research/Synchro/Study_v2/\"\n",
    "IGNORE_TIME = 0\n",
    "sync750 = \"/Volumes/HanSolo/Dropbox/Georgia Tech/Synchro/Data/Study_v1/Flat/sync750/\"\n",
    "sync1000 = \"/Volumes/HanSolo/Dropbox/Georgia Tech/Synchro/Data/Study_v1/Flat/sync1000/\"\n",
    "sync1250 = \"/Volumes/HanSolo/Dropbox/Georgia Tech/Synchro/Data/Study_v1/Flat/sync1250/\"\n",
    "noise750 = \"/Volumes/HanSolo/Dropbox/Georgia Tech/Synchro/Data/Study_v1/Flat/noise750/\"\n",
    "noise1000 = \"/Volumes/HanSolo/Dropbox/Georgia Tech/Synchro/Data/Study_v1/Flat/noise1000/\"\n",
    "noise1250 = \"/Volumes/HanSolo/Dropbox/Georgia Tech/Synchro/Data/Study_v1/Flat/noise1250/\"\n",
    "sit1000 = \"/Volumes/HanSolo/Dropbox/Georgia Tech/Synchro/Data/Study_v1/Flat/sync1000sitting/\"\n",
    "walk1000 = \"/Volumes/HanSolo/Dropbox/Georgia Tech/Synchro/Data/Study_v1/Flat/sync1000walking/\"\n",
    "browse1000 = \"/Volumes/HanSolo/Dropbox/Georgia Tech/Synchro/Data/Study_v1/Flat/sync1000browsing/\"\n",
    "# presults = []\n",
    "# for root, folders, files in os.walk(baseFilePath):\n",
    "#     if not root.endswith(\"synchro\") or \"MACOSX\" in root:\n",
    "#         continue\n",
    "#     sfolders = sorted(folders)    \n",
    "#     efolders = [f for f in sfolders[:] if \"prep\" not in f]\n",
    "#     efolderpaths = [root + \"/\" + f + \"/\" for f in efolders]\n",
    "# #     print(efolderpaths)\n",
    "#     efoldertimes = [(efolderpaths[i].split(\"_\")[-1][:-1] + str(i), correlateSyncFile(efolderpaths[i])) for i in range(len(efolderpaths))]\n",
    "# #     print(efoldertimes)\n",
    "#     presults.append(efoldertimes)\n",
    "#     print presults\n",
    "# presultsa = [[i[1] for i in t] for t in [sorted(r) for r in presults]]\n",
    "# print reduce(lambda x, y : np.array(x) + np.array(y), presultsa)\n",
    "# paverages = reduce(lambda x, y : np.array(x) + np.array(y), presultsa) / len(presultsa)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.ticker import MultipleLocator, FormatStrFormatter\n",
    "\n",
    "# majorLocator = MultipleLocator(1)\n",
    "# majorFormatter = FormatStrFormatter('%d')\n",
    "# minorLocator = MultipleLocator(5)\n",
    "\n",
    "\n",
    "# s750 = correlateSyncFile(sync750)\n",
    "s1000 = correlateSyncFile(sync1000)\n",
    "# s1250 = correlateSyncFile(sync1250)\n",
    "# n750 = correlateSyncFile(noise750)\n",
    "# n1000 = correlateSyncFile(noise1000)\n",
    "# n1250 = correlateSyncFile(noise1250)\n",
    "# figure, ax = plt.subplots()\n",
    "# (figsize=(22,15))\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# plt.plot(t, s)\n",
    "\n",
    "# ax.xaxis.set_major_locator(majorLocator)\n",
    "# ax.xaxis.set_major_formatter(majorFormatter)\n",
    "\n",
    "# for the minor ticks, use no labels; default NullFormatter\n",
    "# ax.xaxis.set_minor_locator(minorLocator)\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# loc = plticker.MultipleLocator(base=1.0) # this locator puts ticks at regular intervals\n",
    "# plt.x.set_major_locator(loc)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)                                                      \n",
    "\n",
    "# major ticks every 20, minor ticks every 5                                      \n",
    "major_ticks = np.arange(0, 20000, 1000)                                              \n",
    "# minor_ticks = np.arange(0, 20000, 500)                                               \n",
    "\n",
    "ax.set_xticks(major_ticks)                                                       \n",
    "# ax.set_xticks(minor_ticks, minor=True)                                           \n",
    "# ax.set_yticks(major_ticks)                                                       \n",
    "# ax.set_yticks(minor_ticks, minor=True)                                           \n",
    "\n",
    "# and a corresponding grid                                                       \n",
    "\n",
    "# ax.grid(which='both')                                                            \n",
    "\n",
    "# or if you want differnet settings for the grids:                               \n",
    "# ax.grid(which='minor', alpha=0.2)                                                \n",
    "# ax.grid(which='major', alpha=0.5) \n",
    "\n",
    "\n",
    "\n",
    "# plt.plot(s1250, '-r', label='Sync 0.8 Hz', linewidth=3)\n",
    "# plt.plot(s1000, '-b', label='Sync 1 Hz', linewidth=3)\n",
    "# plt.plot(s750, '-b', label='Sync 1.33 Hz', linewidth=3)\n",
    "# plt.plot(n1250, '--r', label='Noise 0.8 Hz', linewidth=3)\n",
    "plt.plot(n1000, '--g', label='Noise 1 Hz', linewidth=3)\n",
    "# plt.plot(n750, '--b', label='Noise 1.33 Hz', linewidth=3)\n",
    "legend = plt.legend(loc='lower right', shadow=False)\n",
    "\n",
    "# The frame is matplotlib.patches.Rectangle instance surrounding the legend.\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('0.90')\n",
    "\n",
    "# Set the fontsize\n",
    "for label in legend.get_texts():\n",
    "    label.set_fontsize('large')\n",
    "\n",
    "for label in legend.get_lines():\n",
    "    label.set_linewidth(3)  # the legend line width\n",
    "\n",
    "plt.ylim(0,1)\n",
    "plt.rcParams.update({'font.size': 9})\n",
    "plt.savefig(\"/Users/gareyes/Downloads/figure.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.ticker import MultipleLocator, FormatStrFormatter\n",
    "\n",
    "# majorLocator = MultipleLocator(1)\n",
    "# majorFormatter = FormatStrFormatter('%d')\n",
    "# minorLocator = MultipleLocator(5)\n",
    "\n",
    "\n",
    "# s750 = correlateSyncFile(sync750)\n",
    "s1000 = correlateSyncFile(sync1000)\n",
    "# s1250 = correlateSyncFile(sync1250)\n",
    "# n750 = correlateSyncFile(noise750)\n",
    "# n1000 = correlateSyncFile(noise1000)\n",
    "# n1250 = correlateSyncFile(noise1250)\n",
    "# figure, ax = plt.subplots()\n",
    "# (figsize=(22,15))\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# plt.plot(t, s)\n",
    "\n",
    "# ax.xaxis.set_major_locator(majorLocator)\n",
    "# ax.xaxis.set_major_formatter(majorFormatter)\n",
    "\n",
    "# for the minor ticks, use no labels; default NullFormatter\n",
    "# ax.xaxis.set_minor_locator(minorLocator)\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# loc = plticker.MultipleLocator(base=1.0) # this locator puts ticks at regular intervals\n",
    "# plt.x.set_major_locator(loc)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)                                                      \n",
    "\n",
    "# major ticks every 20, minor ticks every 5                                      \n",
    "# major_ticks = np.arange(0, 20000, 1000)                                              \n",
    "# minor_ticks = np.arange(0, 20000, 500)                                               \n",
    "\n",
    "# ax.set_xticks(major_ticks)                                                       \n",
    "# ax.set_xticks(minor_ticks, minor=True)                                           \n",
    "# ax.set_yticks(major_ticks)                                                       \n",
    "# ax.set_yticks(minor_ticks, minor=True)                                           \n",
    "\n",
    "# and a corresponding grid                                                       \n",
    "\n",
    "# ax.grid(which='both')                                                            \n",
    "\n",
    "# or if you want differnet settings for the grids:                               \n",
    "# ax.grid(which='minor', alpha=0.2)                                                \n",
    "# ax.grid(which='major', alpha=0.5) \n",
    "\n",
    "\n",
    "\n",
    "# plt.plot(s1250, '-r', label='Sync 0.8 Hz', linewidth=3)\n",
    "# plt.plot(s1000, '-b', label='Sync 1 Hz', linewidth=3)\n",
    "# plt.plot(s750, '-b', label='Sync 1.33 Hz', linewidth=3)\n",
    "plt.plot(n1000, '--g', label='Reference Signal', linewidth=3)\n",
    "plt.plot(n750, 'm', label='Feature Signal', linewidth=3)\n",
    "plt.plot(n1250, 'g', label='Adj Reference Signal', linewidth=3)\n",
    "# plt.plot(n750, '--b', label='Noise 1.33 Hz', linewidth=3)\n",
    "legend = plt.legend(loc='lower right', shadow=False)\n",
    "\n",
    "# The frame is matplotlib.patches.Rectangle instance surrounding the legend.\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('0.90')\n",
    "\n",
    "# Set the fontsize\n",
    "for label in legend.get_texts():\n",
    "    label.set_fontsize('large')\n",
    "\n",
    "for label in legend.get_lines():\n",
    "    label.set_linewidth(3)  # the legend line width\n",
    "\n",
    "plt.ylim(0,1)\n",
    "plt.rcParams.update({'font.size': 9})\n",
    "plt.savefig(\"/Users/gareyes/Downloads/figure.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sit1000 = correlateSyncFile(sit1000)\n",
    "walk1000 = correlateSyncFile(walk1000)\n",
    "browse1000 = correlateSyncFile(browse1000)\n",
    "plt.plot(browse1000, '-r')\n",
    "plt.plot(sit1000, '-g')\n",
    "plt.plot(walk1000, '-b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this block iterates through the study data and calculates the data needed for the magnitude over time graph\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "# baseFilePath = \"/Users/jwpilly/Documents/Research/Synchro/Study_v2/\"\n",
    "baseFilePath = \"/Users/jwpilly/Research/Synchro/Study_v2/\"\n",
    "presults = []\n",
    "for root, folders, files in os.walk(baseFilePath):\n",
    "    if not root.endswith(\"synchro\") or \"MACOSX\" in root:\n",
    "        continue\n",
    "    sfolders = sorted(folders, key=lambda x : int(x.split(\"_\")[0].replace(\"t\", \"\")))\n",
    "    efolders = [f for f in sfolders[:] if \"prep\" not in f]\n",
    "    efolderpaths = [root + \"/\" + f + \"/\" for f in efolders]\n",
    "#     print(efolderpaths)\n",
    "    efoldertimes = [(efolderpaths[i].split(\"_\")[-1][:-1] + str(i), magSyncFile(efolderpaths[i])) for i in range(len(efolderpaths))]\n",
    "#     print(efoldertimes)\n",
    "    presults.append(efoldertimes)\n",
    "presultsa = [[i[1] for i in t] for t in [sorted(r) for r in presults]]\n",
    "paverages = reduce(lambda x, y : np.array(x) + np.array(y), presultsa) / len(presultsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this block generates the graph for the magnitude over time data. \n",
    "\n",
    "%matplotlib inline\n",
    "plt.clf()\n",
    "x = corr_times / 1000\n",
    "figure = plt.figure(figsize=(15, 7))\n",
    "figure.add_subplot(121)\n",
    "plt.title(\"Magnitude Over Time - Sitting\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Magnitude\")\n",
    "# plt.ylim(0, 1)\n",
    "plt.plot(x, [paverages[0][i] for i in range(len(x))], label=\"Sit Practice 1\")\n",
    "plt.plot(x, [paverages[1][i] for i in range(len(x))], label=\"Sit Practice 2\")\n",
    "plt.plot(x, [paverages[2][i] for i in range(len(x))], label=\"Sit Eval 1\")\n",
    "plt.plot(x, [paverages[3][i] for i in range(len(x))], label=\"Sit Eval 2\")\n",
    "plt.legend(loc=\"best\")\n",
    "figure.add_subplot(122)\n",
    "plt.title(\"Magnitude Over Time - Walking\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Magnitude\")\n",
    "# plt.ylim(0, 1)\n",
    "plt.plot(x, [paverages[4][i] for i in range(len(x))], label=\"Walk Practice 1\")\n",
    "plt.plot(x, [paverages[5][i] for i in range(len(x))], label=\"Walk Practice 2\")\n",
    "plt.plot(x, [paverages[6][i] for i in range(len(x))], label=\"Walk Eval 1\")\n",
    "plt.plot(x, [paverages[7][i] for i in range(len(x))], label=\"Walk Eval 2\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.savefig(\"magnitude_over_time.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this block iterates through the study data and calculates the data needed for the magnitude over time graph but for noise\n",
    "\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "# baseFilePath = \"/Users/jwpilly/Documents/Research/Synchro/Study_v2/\"\n",
    "baseFilePath = \"/Users/jwpilly/Research/Synchro/Study_v2/\"\n",
    "presults = []\n",
    "for root, folders, files in os.walk(baseFilePath):\n",
    "    if not root.endswith(\"synchro\") or \"MACOSX\" in root:\n",
    "        continue\n",
    "    sfolders = sorted(folders, key=lambda x : int(x.split(\"_\")[0].replace(\"t\", \"\")))\n",
    "    efolders = [f for f in sfolders[:] if \"prep\" not in f]\n",
    "    efolderpaths = [root + \"/\" + f + \"/\" for f in efolders]\n",
    "#     print(efolderpaths)\n",
    "    efoldertimes = [(efolderpaths[i].split(\"_\")[-1][:-1] + str(i), magSyncFile(efolderpaths[i], True)) for i in range(len(efolderpaths))]\n",
    "#     print(efoldertimes)\n",
    "    presults.append(efoldertimes)\n",
    "presultsa = [[i[1] for i in t] for t in [sorted(r) for r in presults]]\n",
    "paverages = reduce(lambda x, y : np.array(x) + np.array(y), presultsa) / len(presultsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this block generates the graph for the magnitude over time data but for noise\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.clf()\n",
    "x = corr_times / 1000\n",
    "figure = plt.figure(figsize=(15, 7))\n",
    "figure.add_subplot(121)\n",
    "plt.title(\"Magnitude Over Time - Sitting\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Magnitude\")\n",
    "# plt.ylim(0, 1)\n",
    "plt.plot(x, [paverages[0][i] for i in range(len(x))], label=\"Sit Practice 1\")\n",
    "plt.plot(x, [paverages[1][i] for i in range(len(x))], label=\"Sit Practice 2\")\n",
    "plt.plot(x, [paverages[2][i] for i in range(len(x))], label=\"Sit Eval 1\")\n",
    "plt.plot(x, [paverages[3][i] for i in range(len(x))], label=\"Sit Eval 2\")\n",
    "plt.legend(loc=\"best\")\n",
    "figure.add_subplot(122)\n",
    "plt.title(\"Magnitude Over Time - Walking\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Magnitude\")\n",
    "# plt.ylim(0, 1)\n",
    "plt.plot(x, [paverages[4][i] for i in range(len(x))], label=\"Walk Practice 1\")\n",
    "plt.plot(x, [paverages[5][i] for i in range(len(x))], label=\"Walk Practice 2\")\n",
    "plt.plot(x, [paverages[6][i] for i in range(len(x))], label=\"Walk Eval 1\")\n",
    "plt.plot(x, [paverages[7][i] for i in range(len(x))], label=\"Walk Eval 2\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.savefig(\"magnitude_over_time_noise.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "magSyncFile(\"/Users/jwpilly/Research/Synchro/Study_v2/P1/synchro/t20170401112834_p1_sit/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this block iterates through the study data and calculates the data needed for the correlation over time graph\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "# baseFilePath = \"/Users/jwpilly/Documents/Research/Synchro/Study_v2/\"\n",
    "baseFilePath = \"/Users/jwpilly/Research/Synchro/Study_v2/\"\n",
    "# baseFilePath = \"/Volumes/HanSolo/Dropbox/Georgia Tech/Synchro/Data/Study_v1/Flat/\"\n",
    "presults = []\n",
    "for root, folders, files in os.walk(baseFilePath):\n",
    "    if not root.endswith(\"synchro\") or \"MACOSX\" in root:\n",
    "        continue\n",
    "    sfolders = sorted(folders, key=lambda x : int(x.split(\"_\")[0].replace(\"t\", \"\")))\n",
    "    efolders = [f for f in sfolders[:] if \"prep\" not in f]\n",
    "    efolderpaths = [root + \"/\" + f + \"/\" for f in efolders]\n",
    "#     print(efolderpaths)\n",
    "    efoldertimes = [(efolderpaths[i].split(\"_\")[-1][:-1] + str(i), correlateSyncFile(efolderpaths[i])) for i in range(len(efolderpaths))]\n",
    "#     print(efoldertimes)\n",
    "    presults.append(efoldertimes)\n",
    "presultsa = [[i[1] for i in t] for t in [sorted(r) for r in presults]]\n",
    "paverages = reduce(lambda x, y : np.array(x) + np.array(y), presultsa) / len(presultsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this block iterates through the study data and calculates the data needed for the correlation over time graph but for noise\n",
    "\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "# baseFilePath = \"/Users/jwpilly/Documents/Research/Synchro/Study_v2/\"\n",
    "baseFilePath = \"/Users/jwpilly/Research/Synchro/Study_v2/\"\n",
    "presults = []\n",
    "for root, folders, files in os.walk(baseFilePath):\n",
    "    if not root.endswith(\"synchro\") or \"MACOSX\" in root:\n",
    "        continue\n",
    "    sfolders = sorted(folders, key=lambda x : int(x.split(\"_\")[0].replace(\"t\", \"\")))\n",
    "    efolders = [f for f in sfolders[:] if \"prep\" not in f]\n",
    "    efolderpaths = [root + \"/\" + f + \"/\" for f in efolders]\n",
    "#     print(efolderpaths)\n",
    "    efoldertimes = [(efolderpaths[i].split(\"_\")[-1][:-1] + str(i), correlateSyncFile(efolderpaths[i], True)) for i in range(len(efolderpaths))]\n",
    "#     print(efoldertimes)\n",
    "    presults.append(efoldertimes)\n",
    "presultsa = [[i[1] for i in t] for t in [sorted(r) for r in presults]]\n",
    "paverages = reduce(lambda x, y : np.array(x) + np.array(y), presultsa) / len(presultsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this block generates the graph for the correlation over time graphs depending on which block was run before this, it is for sync/noise\n",
    "\n",
    "%matplotlib inline\n",
    "plt.clf()\n",
    "x = corr_times / 1000\n",
    "figure = plt.figure(figsize=(15, 7))\n",
    "figure.add_subplot(121)\n",
    "plt.title(\"Correlation Over Time - Sitting\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Correlation\")\n",
    "plt.ylim(0, 1)\n",
    "plt.plot(x, [paverages[0][i] for i in range(len(x))], label=\"Sit Practice 1\")\n",
    "plt.plot(x, [paverages[1][i] for i in range(len(x))], label=\"Sit Practice 2\")\n",
    "plt.plot(x, [paverages[2][i] for i in range(len(x))], label=\"Sit Eval 1\")\n",
    "plt.plot(x, [paverages[3][i] for i in range(len(x))], label=\"Sit Eval 2\")\n",
    "plt.legend(loc=\"best\")\n",
    "figure.add_subplot(122)\n",
    "plt.title(\"Correlation Over Time - Walking\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Correlation\")\n",
    "plt.ylim(0, 1)\n",
    "plt.plot(x, [paverages[4][i] for i in range(len(x))], label=\"Walk Practice 1\")\n",
    "plt.plot(x, [paverages[5][i] for i in range(len(x))], label=\"Walk Practice 2\")\n",
    "plt.plot(x, [paverages[6][i] for i in range(len(x))], label=\"Walk Eval 1\")\n",
    "plt.plot(x, [paverages[7][i] for i in range(len(x))], label=\"Walk Eval 2\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.savefig(\"correlation_over_time.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this block iterates through the study data and calculates the data needed for the correlation over time graph with accel\n",
    "\n",
    "\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "# baseFilePath = \"/Users/jwpilly/Documents/Research/Synchro/Study_v2/\"\n",
    "# baseFilePath = \"/Users/jwpilly/Research/Synchro/Study_v2/\"\n",
    "baseFilePath = \"/media/jwpilly/PillowDisk/Research/Synchro/figuredata/figure12/Study_v2/\"\n",
    "presults = []\n",
    "for root, folders, files in os.walk(baseFilePath):\n",
    "    if not root.endswith(\"synchro\") or \"MACOSX\" in root:\n",
    "        continue\n",
    "    sfolders = sorted(folders, key=lambda x : int(x.split(\"_\")[0].replace(\"t\", \"\")))\n",
    "    efolders = [f for f in sfolders[:] if \"prep\" not in f]\n",
    "    efolderpaths = [root + \"/\" + f + \"/\" for f in efolders]\n",
    "#     print(efolderpaths)\n",
    "    efoldertimes = [(efolderpaths[i].split(\"_\")[-1][:-1] + str(i), correlateSyncAccelFile(efolderpaths[i])) for i in range(len(efolderpaths))]\n",
    "#     print(efoldertimes)\n",
    "    presults.append(efoldertimes)\n",
    "presultsa = [[i[1] for i in t] for t in [sorted(r) for r in presults]]\n",
    "paverages = reduce(lambda x, y : np.array(x) + np.array(y), presultsa) / len(presultsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this block iterates through the study data and calculates the data needed for the correlation over time graph with accel but for noise\n",
    "\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "# baseFilePath = \"/Users/jwpilly/Documents/Research/Synchro/Study_v2/\"\n",
    "baseFilePath = \"/Users/jwpilly/Research/Synchro/Study_v2/\"\n",
    "presults = []\n",
    "for root, folders, files in os.walk(baseFilePath):\n",
    "    if not root.endswith(\"synchro\") or \"MACOSX\" in root:\n",
    "        continue\n",
    "    sfolders = sorted(folders, key=lambda x : int(x.split(\"_\")[0].replace(\"t\", \"\")))\n",
    "    efolders = [f for f in sfolders[:] if \"prep\" not in f]\n",
    "    efolderpaths = [root + \"/\" + f + \"/\" for f in efolders]\n",
    "#     print(efolderpaths)\n",
    "    efoldertimes = [(efolderpaths[i].split(\"_\")[-1][:-1] + str(i), correlateSyncAccelFile(efolderpaths[i], True)) for i in range(len(efolderpaths))]\n",
    "#     print(efoldertimes)\n",
    "    presults.append(efoldertimes)\n",
    "presultsa = [[i[1] for i in t] for t in [sorted(r) for r in presults]]\n",
    "paverages = reduce(lambda x, y : np.array(x) + np.array(y), presultsa) / len(presultsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this block generates the graph for the correlation over time graphs with accel. depending on which block was run before this, it is for sync/noise\n",
    "\n",
    "%matplotlib inline\n",
    "plt.clf()\n",
    "plt.figure(figsize=(8,7))\n",
    "x = corr_times / 1000\n",
    "# figure = plt.figure(figsize=(15, 7))\n",
    "# figure.add_subplot(121)\n",
    "plt.title(\"Correlation Over Time - Sitting\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Correlation\")\n",
    "plt.ylim(-1, 1)\n",
    "plt.plot(x, [paverages[0][0][i] for i in range(len(x))], label=\"Sit Practice 1\")\n",
    "plt.plot(x, [paverages[1][0][i] for i in range(len(x))], label=\"Sit Practice 2\")\n",
    "plt.plot(x, [paverages[2][0][i] for i in range(len(x))], label=\"Sit Eval 1\")\n",
    "plt.plot(x, [paverages[3][0][i] for i in range(len(x))], label=\"Sit Eval 2\")\n",
    "plt.plot(x, np.mean([paverages[pi][1][0] for pi in range(len(paverages) // 2)], axis=0) / np.ptp([paverages[pi][1][0] for pi in range(len(paverages))]) / 2, label=\"Accel x\")\n",
    "plt.plot(x, np.mean([paverages[pi][1][1] for pi in range(len(paverages) // 2)], axis=0) / np.ptp([paverages[pi][1][1] for pi in range(len(paverages))]) / 2, label=\"Accel y\")\n",
    "plt.plot(x, np.mean([paverages[pi][1][2] for pi in range(len(paverages) // 2)], axis=0) / np.ptp([paverages[pi][1][2] for pi in range(len(paverages))]) / 2, label=\"Accel z\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.savefig(\"correlation_over_time_accel_1.png\", bbox_inches=\"tight\")\n",
    "# figure.add_subplot(122)\n",
    "plt.clf()\n",
    "plt.figure(figsize=(8,7))\n",
    "plt.title(\"Correlation Over Time - Walking\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Correlation\")\n",
    "plt.ylim(-1, 1)\n",
    "plt.plot(x, [paverages[4][0][i] for i in range(len(x))], label=\"Walk Practice 1\")\n",
    "plt.plot(x, [paverages[5][0][i] for i in range(len(x))], label=\"Walk Practice 2\")\n",
    "plt.plot(x, [paverages[6][0][i] for i in range(len(x))], label=\"Walk Eval 1\")\n",
    "plt.plot(x, [paverages[7][0][i] for i in range(len(x))], label=\"Walk Eval 2\")\n",
    "plt.plot(x, np.mean([paverages[pi][1][0] for pi in range(len(paverages) // 2 + 4)], axis=0) / np.ptp([paverages[pi][1][0] for pi in range(len(paverages))]) / 2, label=\"Accel x\")\n",
    "plt.plot(x, np.mean([paverages[pi][1][1] for pi in range(len(paverages) // 2 + 4)], axis=0) / np.ptp([paverages[pi][1][1] for pi in range(len(paverages))]) / 2, label=\"Accel y\")\n",
    "plt.plot(x, np.mean([paverages[pi][1][2] for pi in range(len(paverages) // 2 + 4)], axis=0) / np.ptp([paverages[pi][1][2] for pi in range(len(paverages))]) / 2, label=\"Accel z\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.savefig(\"correlation_over_time_accel_2.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this block iterates through the study data and calculates the data needed for the correlation over time graph with accel\n",
    "\n",
    "\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "# baseFilePath = \"/Users/jwpilly/Documents/Research/Synchro/Study_v2/\"\n",
    "baseFilePath = \"/Users/jwpilly/Research/Synchro/Study_v2/\"\n",
    "presults = []\n",
    "for root, folders, files in os.walk(baseFilePath):\n",
    "    if not root.endswith(\"synchro\") or \"MACOSX\" in root:\n",
    "        continue\n",
    "    sfolders = sorted(folders, key=lambda x : int(x.split(\"_\")[0].replace(\"t\", \"\")))\n",
    "    efolders = [f for f in sfolders[:] if \"prep\" not in f]\n",
    "    efolderpaths = [root + \"/\" + f + \"/\" for f in efolders]\n",
    "#     print(efolderpaths)\n",
    "    efoldertimes = [(efolderpaths[i].split(\"_\")[-1][:-1] + str(i), correlateSyncDeltaFile(efolderpaths[i], True)) for i in range(len(efolderpaths))]\n",
    "#     print(efoldertimes)\n",
    "    presults.append(efoldertimes)\n",
    "presultsa = [[i[1] for i in t] for t in [sorted(r) for r in presults]]\n",
    "paverages = reduce(lambda x, y : np.array(x) + np.array(y), presultsa) / len(presultsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this block generates the graph for the correlation over time graphs with accel. depending on which block was run before this, it is for sync/noise\n",
    "\n",
    "%matplotlib inline\n",
    "plt.clf()\n",
    "plt.figure(figsize=(8,7))\n",
    "x = corr_times / 1000\n",
    "# figure = plt.figure(figsize=(15, 7))\n",
    "# figure.add_subplot(121)\n",
    "plt.title(\"Correlation Over Time - Sitting\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Correlation\")\n",
    "# plt.ylim(-1, 1)\n",
    "plt.plot(x, [paverages[0][0][i] for i in range(len(x))], label=\"Sit Practice 1\")\n",
    "plt.plot(x, [paverages[1][0][i] for i in range(len(x))], label=\"Sit Practice 2\")\n",
    "plt.plot(x, [paverages[2][0][i] for i in range(len(x))], label=\"Sit Eval 1\")\n",
    "plt.plot(x, [paverages[3][0][i] for i in range(len(x))], label=\"Sit Eval 2\")\n",
    "plt.plot(x, np.mean([paverages[pi][1] for pi in range(len(paverages) // 2)], axis=0), label=\"Deltas\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.savefig(\"correlation_over_time_delta_1.png\", bbox_inches=\"tight\")\n",
    "# figure.add_subplot(122)\n",
    "plt.clf()\n",
    "plt.figure(figsize=(8,7))\n",
    "plt.title(\"Correlation Over Time - Walking\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Correlation\")\n",
    "# plt.ylim(-1, 1)\n",
    "plt.plot(x, [paverages[4][0][i] for i in range(len(x))], label=\"Walk Practice 1\")\n",
    "plt.plot(x, [paverages[5][0][i] for i in range(len(x))], label=\"Walk Practice 2\")\n",
    "plt.plot(x, [paverages[6][0][i] for i in range(len(x))], label=\"Walk Eval 1\")\n",
    "plt.plot(x, [paverages[7][0][i] for i in range(len(x))], label=\"Walk Eval 2\")\n",
    "plt.plot(x, np.mean([paverages[pi][1] for pi in range(len(paverages) // 2 + 4)], axis=0), label=\"Deltas\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.savefig(\"correlation_over_time_delta_2.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this block iterates through the study data and calculates the data needed for the correlation over time graph with accel\n",
    "\n",
    "\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "# baseFilePath = \"/Users/jwpilly/Documents/Research/Synchro/Study_v2/\"\n",
    "baseFilePath = \"/Users/jwpilly/Research/Synchro/Study_v2/\"\n",
    "presults = []\n",
    "for root, folders, files in os.walk(baseFilePath):\n",
    "    if not root.endswith(\"synchro\") or \"MACOSX\" in root:\n",
    "        continue\n",
    "    sfolders = sorted(folders, key=lambda x : int(x.split(\"_\")[0].replace(\"t\", \"\")))\n",
    "    efolders = [f for f in sfolders[:] if \"prep\" not in f]\n",
    "    efolderpaths = [root + \"/\" + f + \"/\" for f in efolders]\n",
    "#     print(efolderpaths)\n",
    "    efoldertimes = [(efolderpaths[i].split(\"_\")[-1][:-1] + str(i), correlateSyncMagFile(efolderpaths[i], True)) for i in range(len(efolderpaths))]\n",
    "#     print(efoldertimes)\n",
    "    presults.append(efoldertimes)\n",
    "presultsa = [[i[1] for i in t] for t in [sorted(r) for r in presults]]\n",
    "paverages = reduce(lambda x, y : np.array(x) + np.array(y), presultsa) / len(presultsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this block generates the graph for the correlation over time graphs with accel. depending on which block was run before this, it is for sync/noise\n",
    "\n",
    "%matplotlib inline\n",
    "plt.clf()\n",
    "plt.figure(figsize=(8,7))\n",
    "x = corr_times / 1000\n",
    "# figure = plt.figure(figsize=(15, 7))\n",
    "# figure.add_subplot(121)\n",
    "plt.title(\"Correlation Over Time - Sitting\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Correlation\")\n",
    "# plt.ylim(-1, 1)\n",
    "plt.plot(x, [paverages[0][0][i] for i in range(len(x))], label=\"Sit Practice 1\")\n",
    "plt.plot(x, [paverages[1][0][i] for i in range(len(x))], label=\"Sit Practice 2\")\n",
    "plt.plot(x, [paverages[2][0][i] for i in range(len(x))], label=\"Sit Eval 1\")\n",
    "plt.plot(x, [paverages[3][0][i] for i in range(len(x))], label=\"Sit Eval 2\")\n",
    "plt.plot(x, np.mean([paverages[pi][1] for pi in range(len(paverages) // 2)], axis=0), label=\"Mags\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.savefig(\"correlation_over_time_mag_1.png\", bbox_inches=\"tight\")\n",
    "# figure.add_subplot(122)\n",
    "plt.clf()\n",
    "plt.figure(figsize=(8,7))\n",
    "plt.title(\"Correlation Over Time - Walking\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Correlation\")\n",
    "# plt.ylim(-1, 1)\n",
    "plt.plot(x, [paverages[4][0][i] for i in range(len(x))], label=\"Walk Practice 1\")\n",
    "plt.plot(x, [paverages[5][0][i] for i in range(len(x))], label=\"Walk Practice 2\")\n",
    "plt.plot(x, [paverages[6][0][i] for i in range(len(x))], label=\"Walk Eval 1\")\n",
    "plt.plot(x, [paverages[7][0][i] for i in range(len(x))], label=\"Walk Eval 2\")\n",
    "plt.plot(x, np.mean([paverages[pi][1] for pi in range(len(paverages) // 2 + 4)], axis=0), label=\"Mags\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.savefig(\"correlation_over_time_mag_2.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this block generates the graph for the correlation over time graphs with accel. depending on which block was run before this, it is for sync/noise\n",
    "\n",
    "%matplotlib inline\n",
    "plt.clf()\n",
    "plt.figure(figsize=(8,7))\n",
    "x = corr_times / 1000\n",
    "# figure = plt.figure(figsize=(15, 7))\n",
    "# figure.add_subplot(121)\n",
    "plt.title(\"Correlation Over Time - Sitting\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Correlation\")\n",
    "# plt.ylim(-1, 1)\n",
    "plt.plot(x, [paverages[0][0][i] for i in range(len(x))], label=\"Sit Practice 1\")\n",
    "plt.plot(x, [paverages[1][0][i] for i in range(len(x))], label=\"Sit Practice 2\")\n",
    "plt.plot(x, [paverages[2][0][i] for i in range(len(x))], label=\"Sit Eval 1\")\n",
    "plt.plot(x, [paverages[3][0][i] for i in range(len(x))], label=\"Sit Eval 2\")\n",
    "plt.plot(x, np.mean([paverages[pi][1][0] for pi in range(len(paverages) // 2)], axis=0), label=\"Accel x\")\n",
    "plt.plot(x, np.mean([paverages[pi][1][1] for pi in range(len(paverages) // 2)], axis=0), label=\"Accel y\")\n",
    "plt.plot(x, np.mean([paverages[pi][1][2] for pi in range(len(paverages) // 2)], axis=0), label=\"Accel z\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.savefig(\"correlation_over_time_accel_1.png\", bbox_inches=\"tight\")\n",
    "# figure.add_subplot(122)\n",
    "plt.clf()\n",
    "plt.figure(figsize=(8,7))\n",
    "plt.title(\"Correlation Over Time - Walking\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Correlation\")\n",
    "# plt.ylim(-1, 1)\n",
    "plt.plot(x, [paverages[4][0][i] for i in range(len(x))], label=\"Walk Practice 1\")\n",
    "plt.plot(x, [paverages[5][0][i] for i in range(len(x))], label=\"Walk Practice 2\")\n",
    "plt.plot(x, [paverages[6][0][i] for i in range(len(x))], label=\"Walk Eval 1\")\n",
    "plt.plot(x, [paverages[7][0][i] for i in range(len(x))], label=\"Walk Eval 2\")\n",
    "plt.plot(x, np.mean([paverages[pi][1][0] for pi in range(len(paverages) // 2 + 4)], axis=0), label=\"Accel x\")\n",
    "plt.plot(x, np.mean([paverages[pi][1][1] for pi in range(len(paverages) // 2 + 4)], axis=0), label=\"Accel y\")\n",
    "plt.plot(x, np.mean([paverages[pi][1][2] for pi in range(len(paverages) // 2 + 4)], axis=0), label=\"Accel z\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.savefig(\"correlation_over_time_accel_2.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "# baseFilePath = \"/Users/jwpilly/Documents/Research/Synchro/Study_v2/\"\n",
    "baseFilePath = \"/Users/jwpilly/Research/Synchro/Study_v2/\"\n",
    "presults = []\n",
    "presultsn = []\n",
    "for root, folders, files in os.walk(baseFilePath):\n",
    "    if not root.endswith(\"synchro\") or \"MACOSX\" in root:\n",
    "        continue\n",
    "    sfolders = sorted(folders, key=lambda x : int(x.split(\"_\")[0].replace(\"t\", \"\")))\n",
    "    efolders = [f for f in sfolders[:] if \"prep\" not in f]\n",
    "    efolderpaths = [root + \"/\" + f + \"/\" for f in efolders]\n",
    "#     print(efolderpaths)\n",
    "    efoldertimes = [(efolderpaths[i].split(\"_\")[-1][:-1] + str(i), generateMagDist(efolderpaths[i])) for i in range(len(efolderpaths))]\n",
    "#     print(efoldertimes)\n",
    "    presults.append(efoldertimes)\n",
    "    efoldertimesn = [(efolderpaths[i].split(\"_\")[-1][:-1] + str(i), generateMagDist(efolderpaths[i], True)) for i in range(len(efolderpaths))]\n",
    "    presultsn.append(efoldertimesn)\n",
    "presultsa = [[i[1] for i in t] for t in [sorted(r) for r in presults]]\n",
    "presultsaname = [[i[0] for i in t] for t in [sorted(r) for r in presults]]\n",
    "pmags_sit = reduce(lambda x, y : x + y, [[presultsa[i][li] for li in range(len(presultsa[i])) if \"sit\" in presultsaname[i][li]] for i in range(len(presults))])\n",
    "pmags_walk = reduce(lambda x, y : x + y, [[presultsa[i][li] for li in range(len(presultsa[i])) if \"walk\" in presultsaname[i][li]] for i in range(len(presults))])\n",
    "\n",
    "\n",
    "\n",
    "presultsan = [[i[1] for i in t] for t in [sorted(r) for r in presultsn]]\n",
    "presultsanname = [[i[0] for i in t] for t in [sorted(r) for r in presultsn]]\n",
    "pmags_sitn = reduce(lambda x, y : x + y, [[presultsan[i][li] for li in range(len(presultsan[i])) if \"sit\" in presultsanname[i][li]] for i in range(len(presultsn))])\n",
    "pmags_walkn = reduce(lambda x, y : x + y, [[presultsan[i][li] for li in range(len(presultsan[i])) if \"walk\" in presultsanname[i][li]] for i in range(len(presultsn))])\n",
    "\n",
    "# Calculate overlap between the two KDEs.\n",
    "def y_pts(ker_a, ker_b, pt):\n",
    "    y_pt = min(ker_a(pt), ker_b(pt))\n",
    "    return y_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(presultsaname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "sm = stats.gaussian_kde(flatten(pmags_sit))\n",
    "smn = stats.gaussian_kde(flatten(pmags_sitn))\n",
    "wm = stats.gaussian_kde(flatten(pmags_walk))\n",
    "wmn = stats.gaussian_kde(flatten(pmags_walkn))\n",
    "\n",
    "xs = np.linspace(-20, 50, 201)\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(xs, sm(xs))\n",
    "plt.plot(xs, smn(xs))\n",
    "plt.show()\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(xs, wm(xs))\n",
    "plt.plot(xs, wmn(xs))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "# baseFilePath = \"/Users/jwpilly/Documents/Research/Synchro/Study_v2/\"\n",
    "baseFilePath = \"/Users/jwpilly/Research/Synchro/Study_v2/\"\n",
    "presults = []\n",
    "presultsn = []\n",
    "for root, folders, files in os.walk(baseFilePath):\n",
    "    if not root.endswith(\"synchro\") or \"MACOSX\" in root:\n",
    "        continue\n",
    "    sfolders = sorted(folders, key=lambda x : int(x.split(\"_\")[0].replace(\"t\", \"\")))\n",
    "    efolders = [f for f in sfolders[:] if \"prep\" not in f]\n",
    "    efolderpaths = [root + \"/\" + f + \"/\" for f in efolders]\n",
    "#     print(efolderpaths)\n",
    "    efoldertimes = [(efolderpaths[i].split(\"_\")[-1][:-1] + str(i), generateDeltaDist(efolderpaths[i])) for i in range(len(efolderpaths))]\n",
    "#     print(efoldertimes)\n",
    "    presults.append(efoldertimes)\n",
    "    efoldertimesn = [(efolderpaths[i].split(\"_\")[-1][:-1] + str(i), generateDeltaDist(efolderpaths[i], True)) for i in range(len(efolderpaths))]\n",
    "    presultsn.append(efoldertimesn)\n",
    "presultsa = [[i[1] for i in t] for t in [sorted(r) for r in presults]]\n",
    "presultsaname = [[i[0] for i in t] for t in [sorted(r) for r in presults]]\n",
    "pmags_sit = reduce(lambda x, y : x + y, [[presultsa[i][li] for li in range(len(presultsa[i])) if \"sit\" in presultsaname[i][li]] for i in range(len(presults))])\n",
    "pmags_walk = reduce(lambda x, y : x + y, [[presultsa[i][li] for li in range(len(presultsa[i])) if \"walk\" in presultsaname[i][li]] for i in range(len(presults))])\n",
    "\n",
    "\n",
    "\n",
    "presultsan = [[i[1] for i in t] for t in [sorted(r) for r in presultsn]]\n",
    "presultsanname = [[i[0] for i in t] for t in [sorted(r) for r in presultsn]]\n",
    "pmags_sitn = reduce(lambda x, y : x + y, [[presultsan[i][li] for li in range(len(presultsan[i])) if \"sit\" in presultsanname[i][li]] for i in range(len(presultsn))])\n",
    "pmags_walkn = reduce(lambda x, y : x + y, [[presultsan[i][li] for li in range(len(presultsan[i])) if \"walk\" in presultsanname[i][li]] for i in range(len(presultsn))])\n",
    "\n",
    "# Calculate overlap between the two KDEs.\n",
    "def y_pts(ker_a, ker_b, pt):\n",
    "    y_pt = min(ker_a(pt), ker_b(pt))\n",
    "    return y_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "sm = stats.gaussian_kde(flatten(pmags_sit))\n",
    "smn = stats.gaussian_kde(flatten(pmags_sitn))\n",
    "wm = stats.gaussian_kde(flatten(pmags_walk))\n",
    "wmn = stats.gaussian_kde(flatten(pmags_walkn))\n",
    "\n",
    "xs = np.linspace(-20, 50, 201)\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(xs, sm(xs))\n",
    "plt.plot(xs, smn(xs))\n",
    "plt.show()\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(xs, wm(xs))\n",
    "plt.plot(xs, wmn(xs))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "# baseFilePath = \"/Users/jwpilly/Documents/Research/Synchro/Study_v2/\"\n",
    "baseFilePath = \"/Users/jwpilly/Research/Synchro/Study_v2/\"\n",
    "presults = []\n",
    "presultsn = []\n",
    "for root, folders, files in os.walk(baseFilePath):\n",
    "    if not root.endswith(\"synchro\") or \"MACOSX\" in root:\n",
    "        continue\n",
    "    sfolders = sorted(folders, key=lambda x : int(x.split(\"_\")[0].replace(\"t\", \"\")))\n",
    "    efolders = [f for f in sfolders[:] if \"prep\" not in f]\n",
    "    efolderpaths = [root + \"/\" + f + \"/\" for f in efolders]\n",
    "#     print(efolderpaths)\n",
    "    efoldertimes = [(efolderpaths[i].split(\"_\")[-1][:-1] + str(i), generateAccelDist(efolderpaths[i])) for i in range(len(efolderpaths))]\n",
    "#     print(efoldertimes)\n",
    "    presults.append(efoldertimes)\n",
    "    efoldertimesn = [(efolderpaths[i].split(\"_\")[-1][:-1] + str(i), generateAccelDist(efolderpaths[i], True)) for i in range(len(efolderpaths))]\n",
    "    presultsn.append(efoldertimesn)\n",
    "presultsa = [[i[1] for i in t] for t in [sorted(r) for r in presults]]\n",
    "presultsaname = [[i[0] for i in t] for t in [sorted(r) for r in presults]]\n",
    "paccels_sit = reduce(lambda x, y : [x[d] + y[d] for d in range(3)], [[presultsa[i][li] for li in range(len(presultsa[i])) if \"sit\" in presultsaname[i][li]] for i in range(len(presults))])\n",
    "paccels_walk = reduce(lambda x, y : [x[d] + y[d] for d in range(3)], [[presultsa[i][li] for li in range(len(presultsa[i])) if \"walk\" in presultsaname[i][li]] for i in range(len(presults))])\n",
    "\n",
    "\n",
    "\n",
    "presultsan = [[i[1] for i in t] for t in [sorted(r) for r in presultsn]]\n",
    "presultsanname = [[i[0] for i in t] for t in [sorted(r) for r in presultsn]]\n",
    "paccels_sitn = reduce(lambda x, y : [x[d] + y[d] for d in range(3)], [[presultsan[i][li] for li in range(len(presultsan[i])) if \"sit\" in presultsanname[i][li]] for i in range(len(presultsn))])\n",
    "paccels_walkn = reduce(lambda x, y : [x[d] + y[d] for d in range(3)], [[presultsan[i][li] for li in range(len(presultsan[i])) if \"walk\" in presultsanname[i][li]] for i in range(len(presultsn))])\n",
    "\n",
    "# Calculate overlap between the two KDEs.\n",
    "def y_pts(ker_a, ker_b, pt):\n",
    "    y_pt = min(ker_a(pt), ker_b(pt))\n",
    "    return y_pt\n",
    "# Store overlap value.\n",
    "# overlap = quad(lambda x : y_pts(gd[1], gdn[1], x), -np.inf, np.inf)\n",
    "# print(overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "presults[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[[i[0] for i in t] for t in [sorted(r) for r in presults]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sx = np.mean(np.array((flatten(paccels_sit[0]))))\n",
    "sy = np.mean(np.array((flatten(paccels_sit[1]))))\n",
    "sz = np.mean(np.array((flatten(paccels_sit[2]))))\n",
    "\n",
    "wx = np.mean(np.array((flatten(paccels_walk[0]))))\n",
    "wy = np.mean(np.array((flatten(paccels_walk[1]))))\n",
    "wz = np.mean(np.array((flatten(paccels_walk[2]))))\n",
    "\n",
    "sxn = np.mean(np.array((flatten(paccels_sitn[0]))))\n",
    "syn = np.mean(np.array((flatten(paccels_sitn[1]))))\n",
    "szn = np.mean(np.array((flatten(paccels_sitn[2]))))\n",
    "\n",
    "wxn = np.mean(np.array((flatten(paccels_walkn[0]))))\n",
    "wyn = np.mean(np.array((flatten(paccels_walkn[1]))))\n",
    "wzn = np.mean(np.array((flatten(paccels_walkn[2]))))\n",
    "              \n",
    "print(sx, \",\", sy, \",\", sz)\n",
    "print(wx, \",\", wy, \",\", wz)\n",
    "\n",
    "print(sxn, \",\", syn, \",\", szn)\n",
    "print(wxn, \",\", wyn, \",\", wzn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "sx = stats.gaussian_kde(flatten(paccels_sit[0]))\n",
    "sy = stats.gaussian_kde(flatten(paccels_sit[1]))\n",
    "sz = stats.gaussian_kde(flatten(paccels_sit[2]))\n",
    "\n",
    "wx = stats.gaussian_kde(flatten(paccels_walk[0]))\n",
    "wy = stats.gaussian_kde(flatten(paccels_walk[1]))\n",
    "wz = stats.gaussian_kde(flatten(paccels_walk[2]))\n",
    "\n",
    "sxn = stats.gaussian_kde(flatten(paccels_sitn[0]))\n",
    "syn = stats.gaussian_kde(flatten(paccels_sitn[1]))\n",
    "szn = stats.gaussian_kde(flatten(paccels_sitn[2]))\n",
    "\n",
    "wxn = stats.gaussian_kde(flatten(paccels_walkn[0]))\n",
    "wyn = stats.gaussian_kde(flatten(paccels_walkn[1]))\n",
    "wzn = stats.gaussian_kde(flatten(paccels_walkn[2]))\n",
    "\n",
    "xs = np.linspace(-20, 20, 201)\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(xs, sx(xs))\n",
    "plt.plot(xs, sxn(xs))\n",
    "plt.show()\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(xs, sy(xs))\n",
    "plt.plot(xs, syn(xs))\n",
    "plt.show()\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(xs, sz(xs))\n",
    "plt.plot(xs, szn(xs))\n",
    "plt.show()\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(xs, wx(xs))\n",
    "plt.plot(xs, wxn(xs))\n",
    "plt.show()\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(xs, wy(xs))\n",
    "plt.plot(xs, wyn(xs))\n",
    "plt.show()\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(xs, wz(xs))\n",
    "plt.plot(xs, wzn(xs))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graphPath = \"/Users/jwpilly/Research/Synchro/Study_v2/P6/synchro/t20170403152443_p6_walk/\"\n",
    "\n",
    "ssg = generateAccelDist(graphPath)\n",
    "ssgn = generateAccelDist(graphPath, True)\n",
    "ssx = stats.gaussian_kde(ssg[0])\n",
    "ssy = stats.gaussian_kde(ssg[1])\n",
    "ssz = stats.gaussian_kde(ssg[2])\n",
    "ssxn = stats.gaussian_kde(ssgn[0])\n",
    "ssyn = stats.gaussian_kde(ssgn[1])\n",
    "sszn = stats.gaussian_kde(ssgn[2])\n",
    "plt.clf()\n",
    "plt.plot(xs, ssx(xs), color=\"r\")\n",
    "plt.plot(xs, ssxn(xs), color=\"g\")\n",
    "plt.show()\n",
    "plt.clf()\n",
    "plt.plot(xs, ssy(xs), color=\"r\")\n",
    "plt.plot(xs, ssyn(xs), color=\"g\")\n",
    "plt.show()\n",
    "plt.clf()\n",
    "plt.plot(xs, ssz(xs), color=\"r\")\n",
    "plt.plot(xs, sszn(xs), color=\"g\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "baseFilePath = \"/Users/jwpilly/Research/Synchro/Study_v2/\"\n",
    "for root, folders, files in os.walk(baseFilePath):\n",
    "    if not root.endswith(\"synchro\") or \"MACOSX\" in root:\n",
    "        continue\n",
    "    sfolders = sorted(folders, key=lambda x : int(x.split(\"_\")[0].replace(\"t\", \"\")))\n",
    "    efolders = [f for f in sfolders[:] if \"prep\" not in f]\n",
    "    efolderpaths = [root + \"/\" + f + \"/\" for f in efolders]\n",
    "    efoldertimes = [segmentNoiseFile(efolderpaths[i]) for i in range(len(efolderpaths))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "baseFilePath = \"/Users/jwpilly/Research/Synchro/Study_v2/\"\n",
    "presults = []\n",
    "for root, folders, files in os.walk(baseFilePath):\n",
    "    if not root.endswith(\"swipe\"):\n",
    "        continue\n",
    "    sfolders = sorted(folders, key=lambda x : int(x.split(\"_\")[0].replace(\"t\", \"\")))\n",
    "    efolders = [f for f in sfolders[:] if \"prep\" not in f]\n",
    "    efolderpaths = [root + \"/\" + f + \"/\" for f in efolders]\n",
    "    efoldertimes = [(efolderpaths[i].split(\"_\")[-1][:-1] + str(i), processSwipeFile(efolderpaths[i])) for i in range(len(efolderpaths))]\n",
    "    presults.append(efoldertimes)\n",
    "presultsa = [[i[1] for i in t] for t in [sorted(r) for r in presults]]\n",
    "paverages = reduce(lambda x, y : np.array(x) + np.array(y), presultsa) / len(presultsa)\n",
    "print(paverages)\n",
    "swipe_sit = np.mean([paverages[i] for i in range(4)])\n",
    "swipe_walk = np.mean([paverages[i + 4] for i in range(4)])\n",
    "swipe_times = paverages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.clf()\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "objects = ('Practice 1', 'Practice 2', 'Eval 1', 'Eval 2')\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = paverages\n",
    "# plt.bar(y_pos, performance, align='center', alpha=0.5)\n",
    "plt.plot(y_pos, [paverages[i] for i in range(4)], label=\"Sit\")\n",
    "plt.plot(y_pos, [paverages[i + 4] for i in range(4)], label=\"Walk\")\n",
    "plt.xticks(y_pos, objects)\n",
    "plt.ylim((1.5,2.5))\n",
    "plt.ylabel('Time (s)')\n",
    "plt.title('Swipe Times')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.savefig(\"swipetimes.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(syncresults)\n",
    "print(thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 10, 6\n",
    "plt.clf()\n",
    "labels = [\"Swipe\", \"0.6\", \"0.7\", \"0.8\", \"0.9\"]\n",
    "width= 0.3\n",
    "ind = np.arange(len(labels))\n",
    "plt.bar(ind, [np.mean(swipe_times[:4]), np.mean(syncresults[2][:4]), np.mean(syncresults[4][:4]), np.mean(syncresults[6][:4]), np.mean(syncresults[8][:4])], width=width, label=\"Sit\")\n",
    "plt.bar(ind + width * 1.2, [np.mean(swipe_times[4:]), np.mean(syncresults[2][4:]), np.mean(syncresults[4][4:]), np.mean(syncresults[6][4:]), np.mean(syncresults[8][4:])], width=width, label=\"Walk\")\n",
    "plt.xticks( 0.1 + ind + width / 1.5, labels)\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Sync Time (s)\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.savefig(\"overall_performance_bar.png\", bbox_inches='tight')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install -U matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l = [0,1,2,2,4,4,56]\n",
    "# [x+1 for x in l if x >= 45]\n",
    "list2 = [ind for ind, x in enumerate(l) if x > 45]\n",
    "print(list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list2 = [x for ind, x in enumerate(list1) if 4 > ind > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "windowDataSet = loadWindows()\n",
    "vizWindows(windowDataSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "liveCorrDataSet = loadLiveCorrData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(x for x in liveCorrDataSet[liveCorrDataSet.keys()[0]].direction if x != null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate all reference graph\n",
    "syncList = []\n",
    "for f in windowDataSet.keys():        \n",
    "    syncList.append('_'.join(f.split(\"_\")[0:3]))\n",
    "    syncList = sorted(set(syncList))\n",
    "\n",
    "for i, element in enumerate(syncList):    \n",
    "    \n",
    "    # Make folders\n",
    "    paths = [figurePath + \"reference/\"]\n",
    "    for path in paths:\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "\n",
    "    figureFile = figurePath + \"reference/\" + element + \"_summary.pdf\"\n",
    "    fig, axs = plt.subplots(2, 1, sharex=True, sharey=False)\n",
    "    fig.set_size_inches(15,15)\n",
    "    for key in windowDataSet:\n",
    "        if syncList[i] in key:\n",
    "            df = windowDataSet[key]\n",
    "            axs[0].plot(df.timestamp, df.feature, '-r', label=\"feature\")\n",
    "            axs[0].plot(df.timestamp, df.reference, '-b', label=\"reference\")\n",
    "            axs[1].plot(df.timestamp, df.feature, '-g', label=\"feature\")\n",
    "            axs[1].plot(df.timestamp, df.adjusted, '-b', label=\"adjusted\")    \n",
    "    axs[0].set_title(\"feat-ref:\")\n",
    "    axs[1].set_title(\"feat-adjref:\")\n",
    "    fig.savefig(figureFile)\n",
    "    plt.close()\n",
    "print(\"done generating all references\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize sets\n",
    "avg={}\n",
    "leftPeaksSet={}\n",
    "rightPeaksSet={}\n",
    "angles={}\n",
    "\n",
    "for key in rawDataSet:\n",
    "    \n",
    "    print(key)\n",
    "\n",
    "    # Initialize data structures\n",
    "    anglesList=[]\n",
    "    avgList=[]\n",
    "    left=[]\n",
    "    leftPeaks = []\n",
    "    rightPeaks = []\n",
    "\n",
    "    # Get data\n",
    "    df = subsampleDataSet[key]\n",
    "    left = leftTicksSet[key]\n",
    "    right = rightTicksSet[key]\n",
    "    \n",
    "    \n",
    "    # Per window based on number of left peaks\n",
    "    for i in range(1, len(left)):\n",
    "\n",
    "        # Get slice of window from all data\n",
    "        dataFrame = df[df['timestamp'].between(left[i-1], left[i], inclusive=True)]\n",
    "\n",
    "        # Calculate average time and vector across window\n",
    "        avgTime = np.sum(dataFrame[0:].timestamp)/len(dataFrame)\n",
    "        avgVector = [np.sum(dataFrame[0:].x)/len(dataFrame), np.sum(dataFrame[0:].y)/len(dataFrame), np.sum(dataFrame[0:].z)/len(dataFrame)]\n",
    "        averageVector=np.array(avgVector)\n",
    "\n",
    "        windowAngles = []\n",
    "        windowTimes=[]\n",
    "        \n",
    "        # Per point in window\n",
    "        for j in range(0,len(dataFrame)):\n",
    "            \n",
    "            # Save timestamps per window\n",
    "            windowTimes.append(dataFrame.timestamp.values[j])\n",
    "\n",
    "            # Save angle per window \n",
    "            currentVector = []\n",
    "            currentVector = np.array([dataFrame.x.values[j], dataFrame.y.values[j], dataFrame.z.values[j]])            \n",
    "            angle = angle_between(currentVector, averageVector) \n",
    "            windowAngles.append(angle)\n",
    "            \n",
    "            # Construct list with timestamp and angle\n",
    "            anglesList.append(np.array([dataFrame.timestamp.values[j], angle]))                         \n",
    "\n",
    "        angle1index = windowAngles.index(np.max(windowAngles[0:5]))\n",
    "        angle2index = windowAngles.index(np.max(windowAngles[5:10]))\n",
    "        angle1vector = np.array([dataFrame.x.values[angle1index], dataFrame.y.values[angle1index], dataFrame.z.values[angle1index]])            \n",
    "        angle2vector = np.array([dataFrame.x.values[angle2index], dataFrame.y.values[angle2index], dataFrame.z.values[angle2index]])            \n",
    "        angle1time = windowTimes[windowAngles.index(np.max(windowAngles[0:5]))]\n",
    "        angle2time = windowTimes[windowAngles.index(np.max(windowAngles[5:10]))]\n",
    "  \n",
    "        meanmag = get_magnitude(averageVector)\n",
    "        angle1mag = get_magnitude(angle1vector)\n",
    "        angle2mag = get_magnitude(angle2vector)\n",
    "        \n",
    "#         print(angle1mag, meanmag, angle2mag)\n",
    "        \n",
    "        if angle1mag > meanmag:\n",
    "            print(\"right\")\n",
    "        elif angle2mag > meanmag:\n",
    "            print(\"left\")\n",
    "\n",
    "        left1time = left[i-1]\n",
    "        righttime = right[i-1]\n",
    "        left2time = left[i]\n",
    "        \n",
    "#         print(left1time < righttime < left2time)\n",
    "\n",
    "        A = np.square(left1time - angle1time)\n",
    "        B = np.square(righttime - angle2time)\n",
    "        C = np.square(left2time - angle2time) \n",
    "        D = np.square(righttime - angle1time)\n",
    "        \n",
    "        score1 = np.sqrt(A + C)\n",
    "        score2 = np.sqrt(B + D)\n",
    "        \n",
    "#         print(score1, score2)\n",
    "        \n",
    "#         if(score1 < score2):\n",
    "#             print(\"left\")\n",
    "#         else:\n",
    "#             print(\"right\")\n",
    "        \n",
    "#         print(\"----------\")\n",
    "\n",
    "#         print(score1, score2)\n",
    "            \n",
    "#         previousLefts = []\n",
    "#         for i in range(0, len(left)):\n",
    "#             if left[i] < firstTimestamp:\n",
    "#                 previousLefts.append(left[i])\n",
    "        \n",
    "#         previousRights = []\n",
    "#         for i in range(0, len(right)):\n",
    "#             if right[i] < secondTimestamp:\n",
    "#                 previousRights.append(right[i])        \n",
    "        \n",
    "#         leftIndicator = np.min(np.abs(previousLefts - firstTimestamp))\n",
    "#         rightIndicator = np.min(np.abs(previousRights - secondTimestamp))        \n",
    "        \n",
    "#         if leftIndicator < rightIndicator:\n",
    "# #             windowLabels.append(\"left\")\n",
    "#             print(\"left\")\n",
    "#         else:\n",
    "# #             windowLabels.append(\"right\")\n",
    "#             print(\"right\")\n",
    "\n",
    "        # Find left and right peak using max angle difference\n",
    "        leftPeaks.append(np.array([angle1time]))\n",
    "        rightPeaks.append(np.array([angle2time]))\n",
    "#         leftPeaks.append(np.array([windowTimes[windowAngles.index(np.sort(windowAngles[0:5])[-3:][0])]]))\n",
    "#         rightPeaks.append(np.array([windowTimes[windowAngles.index(np.sort(windowAngles[5:10])[-3:][0])]]))        \n",
    "\n",
    "        # Save average time and vector\n",
    "        tempList=[]\n",
    "        tempList.append(avgTime)\n",
    "        tempList.extend(avgVector)\n",
    "        avgList.append(np.array(tempList))    \n",
    "\n",
    "    # Save sets\n",
    "    angles[key] = anglesList\n",
    "    avg[key] = avgList\n",
    "    leftPeaksSet[key] = leftPeaks\n",
    "    rightPeaksSet[key] = rightPeaks    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get mean feature\n",
    "for key in rawDataSet:\n",
    "#     fig = plt.figure()\n",
    "    df = subsampleDataSet[key]\n",
    "    lefts = leftTicksSet[key]\n",
    "    rights = rightTicksSet[key]\n",
    "    avgs = avg[key]\n",
    "    angleVal = angles[key]\n",
    "    leftPeak = leftPeaksSet[key]\n",
    "    rightPeak = rightPeaksSet[key]\n",
    "    \n",
    "    fig, axs = plt.subplots(2, 1, sharex=True, sharey=False)\n",
    "    \n",
    "    axs[0].plot(df.timestamp, df.x, '-r')\n",
    "    axs[0].plot(df.timestamp, df.y, '-g')\n",
    "    axs[0].plot(df.timestamp, df.z, '-b')\n",
    "    \n",
    "    for i in lefts:\n",
    "        axs[0].axvline(i, color='b')\n",
    "        axs[1].axvline(i, color='b')\n",
    "    for i in rights:\n",
    "        axs[0].axvline(i, color='r')\n",
    "        axs[1].axvline(i, color='r')        \n",
    "    \n",
    "    for i in leftPeak:\n",
    "        axs[0].axvline(i, color='g')\n",
    "        axs[1].axvline(i, color='g')\n",
    "\n",
    "    for i in rightPeak:\n",
    "        axs[0].axvline(i, color='k')\n",
    "        axs[1].axvline(i, color='k')\n",
    "    \n",
    "    for i in range(0, len(avgs)):\n",
    "        axs[0].scatter(avgs[i][0], avgs[i][1])\n",
    "        axs[0].scatter(avgs[i][0], avgs[i][2])\n",
    "        axs[0].scatter(avgs[i][0], avgs[i][3])\n",
    "        \n",
    "    for i in range(0, len(angleVal)):\n",
    "        axs[1].scatter(angleVal[i][0], angleVal[i][1])\n",
    "#         axs[1].text(angleVal[i][0], math.degrees(angleVal[i][1])/10, str(angleVal[i][2]))\n",
    "        \n",
    "    plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize sets\n",
    "avg={}\n",
    "leftPeaksSet={}\n",
    "rightPeaksSet={}\n",
    "angles={}\n",
    "\n",
    "for key in rawDataSet:\n",
    "\n",
    "    # Initialize data structures\n",
    "    anglesList=[]\n",
    "    avgList=[]\n",
    "    left=[]\n",
    "    leftPeaks = []\n",
    "    rightPeaks = []\n",
    "\n",
    "    # Get data\n",
    "    df = subsampleDataSet[key]\n",
    "    left = leftTicksSet[key]\n",
    "    right = rightTicksSet[key]\n",
    "    \n",
    "    # Per window based on number of left peaks\n",
    "    for i in range(1, len(left)):\n",
    "\n",
    "        # Get slice of window from all data\n",
    "        dataFrame = df[df['timestamp'].between(left[i-1], left[i], inclusive=True)]\n",
    "\n",
    "        # Calculate average time and vector across window\n",
    "        avgTime = np.sum(dataFrame[0:].timestamp)/len(dataFrame)\n",
    "        avgVector = [np.sum(dataFrame[0:].x)/len(dataFrame), np.sum(dataFrame[0:].y)/len(dataFrame), np.sum(dataFrame[0:].z)/len(dataFrame)]\n",
    "        averageVector=np.array(avgVector)\n",
    "\n",
    "        windowAngles = []\n",
    "        windowTimes=[]\n",
    "        \n",
    "        # Per point in window\n",
    "        for i in range(0,len(dataFrame)):\n",
    "            \n",
    "            # Save timestamps per window\n",
    "            windowTimes.append(dataFrame.timestamp.values[i])\n",
    "\n",
    "            # Save angle per window \n",
    "            currentVector = []\n",
    "            currentVector = np.array([dataFrame.x.values[i], dataFrame.y.values[i], dataFrame.z.values[i]])            \n",
    "            angle = angle_between(currentVector, averageVector) \n",
    "            windowAngles.append(angle)\n",
    "            \n",
    "            # Construct list with timestamp and angle\n",
    "            anglesList.append(np.array([dataFrame.timestamp.values[i], angle]))                         \n",
    "\n",
    "        firstTimestamp = windowTimes[windowAngles.index(np.max(windowAngles[0:5]))]\n",
    "        secondTimestamp = windowTimes[windowAngles.index(np.max(windowAngles[5:10]))]\n",
    "            \n",
    "        # Find left and right peak using max angle difference\n",
    "        leftPeaks.append(np.array([firstTimestamp]))\n",
    "        rightPeaks.append(np.array([windowTimes[windowAngles.index(np.max(windowAngles[5:10]))]]))        \n",
    "#         leftPeaks.append(np.array([windowTimes[windowAngles.index(np.sort(windowAngles[0:5])[-3:][0])]]))\n",
    "#         rightPeaks.append(np.array([windowTimes[windowAngles.index(np.sort(windowAngles[5:10])[-3:][0])]]))        \n",
    "\n",
    "        # Save average time and vector\n",
    "        tempList=[]\n",
    "        tempList.append(avgTime)\n",
    "        tempList.extend(avgVector)\n",
    "        avgList.append(np.array(tempList))    \n",
    "\n",
    "    # Save sets\n",
    "    angles[key] = anglesList\n",
    "    avg[key] = avgList\n",
    "    leftPeaksSet[key] = leftPeaks\n",
    "    rightPeaksSet[key] = rightPeaks    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize sets\n",
    "avg={}\n",
    "leftPeaksSet={}\n",
    "rightPeaksSet={}\n",
    "angles={}\n",
    "\n",
    "for key in rawDataSet:\n",
    "    \n",
    "    print(key)\n",
    "\n",
    "    # Initialize data structures\n",
    "    anglesList=[]\n",
    "    avgList=[]\n",
    "    left=[]\n",
    "    leftPeaks = []\n",
    "    rightPeaks = []\n",
    "\n",
    "    # Get data\n",
    "    df = subsampleDataSet[key]\n",
    "    left = leftTicksSet[key]\n",
    "    right = rightTicksSet[key]\n",
    "    \n",
    "    # Per window based on number of left peaks\n",
    "    for i in range(1, len(left)):\n",
    "\n",
    "        # Get slice of window from all data\n",
    "        dataFrame = df[df['timestamp'].between(left[i-1], left[i], inclusive=True)]\n",
    "\n",
    "        # Calculate average time and vector across window\n",
    "        avgTime = np.sum(dataFrame[0:].timestamp)/len(dataFrame)\n",
    "        avgVector = [np.sum(dataFrame[0:].x)/len(dataFrame), np.sum(dataFrame[0:].y)/len(dataFrame), np.sum(dataFrame[0:].z)/len(dataFrame)]\n",
    "        averageVector=np.array(avgVector)\n",
    "\n",
    "        windowAngles = []\n",
    "        windowTimes=[]\n",
    "        \n",
    "        # Per point in window\n",
    "        for i in range(0,len(dataFrame)):\n",
    "            \n",
    "            # Save timestamps per window\n",
    "            windowTimes.append(dataFrame.timestamp.values[i])\n",
    "\n",
    "            # Save angle per window \n",
    "            currentVector = []\n",
    "            currentVector = np.array([dataFrame.x.values[i], dataFrame.y.values[i], dataFrame.z.values[i]])            \n",
    "            angle = angle_between(currentVector, averageVector) \n",
    "            windowAngles.append(angle)\n",
    "            \n",
    "            # Construct list with timestamp and angle\n",
    "            anglesList.append(np.array([dataFrame.timestamp.values[i], angle]))                         \n",
    "\n",
    "        firstTimestamp = windowTimes[windowAngles.index(np.max(windowAngles[0:5]))]\n",
    "        secondTimestamp = windowTimes[windowAngles.index(np.max(windowAngles[5:10]))]\n",
    "            \n",
    "        previousLefts = []\n",
    "        for i in range(0, len(left)):\n",
    "            if left[i] < firstTimestamp:\n",
    "                previousLefts.append(left[i])\n",
    "        \n",
    "        previousRights = []\n",
    "        for i in range(0, len(right)):\n",
    "            if right[i] < secondTimestamp:\n",
    "                previousRights.append(right[i])        \n",
    "        \n",
    "        leftIndicator = np.min(np.abs(previousLefts - firstTimestamp))\n",
    "        rightIndicator = np.min(np.abs(previousRights - secondTimestamp))        \n",
    "        \n",
    "        if leftIndicator < rightIndicator:\n",
    "#             windowLabels.append(\"left\")\n",
    "            print(\"left\")\n",
    "        else:\n",
    "#             windowLabels.append(\"right\")\n",
    "            print(\"right\")\n",
    "\n",
    "        # Find left and right peak using max angle difference\n",
    "        leftPeaks.append(np.array([firstTimestamp]))\n",
    "        rightPeaks.append(np.array([secondTimestamp]))\n",
    "#         leftPeaks.append(np.array([windowTimes[windowAngles.index(np.sort(windowAngles[0:5])[-3:][0])]]))\n",
    "#         rightPeaks.append(np.array([windowTimes[windowAngles.index(np.sort(windowAngles[5:10])[-3:][0])]]))        \n",
    "\n",
    "        # Save average time and vector\n",
    "        tempList=[]\n",
    "        tempList.append(avgTime)\n",
    "        tempList.extend(avgVector)\n",
    "        avgList.append(np.array(tempList))    \n",
    "\n",
    "    # Save sets\n",
    "    angles[key] = anglesList\n",
    "    avg[key] = avgList\n",
    "    leftPeaksSet[key] = leftPeaks\n",
    "    rightPeaksSet[key] = rightPeaks    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize sets\n",
    "avg={}\n",
    "leftPeaksSet={}\n",
    "rightPeaksSet={}\n",
    "angles={}\n",
    "\n",
    "for key in rawDataSet:\n",
    "\n",
    "    # Initialize data structures\n",
    "    anglesList=[]\n",
    "    avgList=[]\n",
    "    left=[]\n",
    "    leftVectors = []\n",
    "    rightVectors = []\n",
    "    windowLabels = []\n",
    "\n",
    "    # Get data\n",
    "    df = subsampleDataSet[key]\n",
    "    left = leftTicksSet[key]\n",
    "    right = rightTicksSet[key]\n",
    "    \n",
    "    print(key)\n",
    "    print(\"-----------------\")\n",
    "\n",
    "    \n",
    "    # Per window based on number of left peaks\n",
    "    for i in range(1, len(left)):\n",
    "\n",
    "        # Get slice of window from all data\n",
    "        dataFrame = df[df['timestamp'].between(left[i-1], left[i], inclusive=True)]\n",
    "\n",
    "        # Calculate average time and vector across window\n",
    "        avgTime = np.sum(dataFrame[0:].timestamp)/len(dataFrame)\n",
    "        avgVector = [np.sum(dataFrame[0:].x)/len(dataFrame), np.sum(dataFrame[0:].y)/len(dataFrame), np.sum(dataFrame[0:].z)/len(dataFrame)]\n",
    "        averageVector=np.array(avgVector)\n",
    "\n",
    "        windowAngles = []\n",
    "        windowTimes=[]\n",
    "        \n",
    "        maxAngle = 0\n",
    "        firstIndex = 0\n",
    "        secondIndex = 0\n",
    "        \n",
    "        # Per point in window\n",
    "        for i in range(0,len(dataFrame)):\n",
    "            \n",
    "            for j in range(0,len(dataFrame)):\n",
    "                \n",
    "                if j<i:                    \n",
    "                    # Construct list with timestamp and angle\n",
    "#                     anglesList.append(np.array([dataFrame.timestamp.values[i], angle, -1]))                         \n",
    "                    continue\n",
    "                elif i==j:\n",
    "                    continue\n",
    "                else:  \n",
    "                    \n",
    "                    # Save timestamps per window\n",
    "                    windowTimes.append(dataFrame.timestamp.values[i])\n",
    "\n",
    "                    # Save angle per window \n",
    "                    currentVector = []\n",
    "                    currentVector = np.array([dataFrame.x.values[i], dataFrame.y.values[i], dataFrame.z.values[i]])\n",
    "                    nextVector = np.array([dataFrame.x.values[j], dataFrame.y.values[j], dataFrame.z.values[j]])\n",
    "                    angle = angle_between(currentVector, nextVector) \n",
    "#                     print(i, j, angle, maxAngle)\n",
    "                    if angle > maxAngle:\n",
    "                        maxAngle = angle\n",
    "                        firstIndex = i\n",
    "                        secondIndex = j\n",
    "                        firstTimestamp = dataFrame.timestamp.values[i]\n",
    "                        secondTimestamp = dataFrame.timestamp.values[j]\n",
    "                        \n",
    "\n",
    "                    windowAngles.append(np.array([angle, i, j]))\n",
    "                    \n",
    "                    # Construct list with timestamp and angle\n",
    "                    anglesList.append(np.array([dataFrame.timestamp.values[i], angle, j]))                         \n",
    "#         print(firstIndex, secondIndex)\n",
    "\n",
    "#         print(windowAngles)\n",
    "#         print(windowAngles.index(np.max(windowAngles[0].all())), len(windowAngles))\n",
    "#         highestAngleTime = windowTimes[windowAngles.index(np.max(windowAngles))]\n",
    "#         lowestAngleTime = windowTimes[windowAngles.index(np.min(windowAngles))]\n",
    "        newLefts1 = firstTimestamp - left\n",
    "        newLefts2 = firstTimestamp - right\n",
    "\n",
    "        newRight1 = secondTimestamp - left\n",
    "        newRight2 = secondTimestamp - right\n",
    "        \n",
    "#         print(left)\n",
    "        \n",
    "        previousLefts = []\n",
    "        for i in range(0, len(left)):\n",
    "            if left[i] < firstTimestamp:\n",
    "                previousLefts.append(left[i])\n",
    "        \n",
    "        previousRights = []\n",
    "        for i in range(0, len(right)):\n",
    "            if right[i] < secondTimestamp:\n",
    "                previousRights.append(right[i])        \n",
    "        \n",
    "        leftIndicator = np.min(np.abs(previousLefts - firstTimestamp))\n",
    "        rightIndicator = np.min(np.abs(previousRights - secondTimestamp))        \n",
    "        \n",
    "        if leftIndicator < rightIndicator:\n",
    "            windowLabels.append(\"left\")\n",
    "            print(\"left\")\n",
    "        else:\n",
    "            windowLabels.append(\"right\")\n",
    "            print(\"right\")\n",
    "\n",
    "#         print(np.min(np.abs(previousRights - firstTimestamp))        )\n",
    "#         print(np.min(np.abs(previousLefts - secondTimestamp)))\n",
    "\n",
    "#         min(previousLefts, firstTimestamp)\n",
    "        \n",
    "#         print(left[x]left[for x in left < firstTimestamp]])\n",
    "        \n",
    "#         print(\"first vector distance from left: \" + str(np.min(newLefts1)))\n",
    "#         print(\"first vector distance from right: \" + str(np.min(newLefts2)))\n",
    "#         print(\"second vector distance from left: \" + str(np.min(newRight1)))\n",
    "#         print(\"second vector distance from right: \" + str(np.min(newRight2)))\n",
    "#         if np.abs(np.min(newLefts1)) <= np.abs(np.min(newRight1)):\n",
    "#             print(\"left\")\n",
    "#         else:\n",
    "#             print(\"right\")\n",
    "#         if np.abs(np.min(newLefts2)) <= np.abs(np.min(newRight2)):\n",
    "#             print(\"right\")\n",
    "#         else:\n",
    "#             print(\"left\")\n",
    "        \n",
    "        \n",
    "#         print(np.abs(np.min(newLefts1)), np.abs(np.min(newRight1)))\n",
    "#         print(np.abs(np.min(newLefts2)), np.abs(np.min(newRight2)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         closestTimeFromRight = right - highestAngleTime\n",
    "#         minLeft = np.min(closestTimeFromLeft)\n",
    "#         minRight = np.min(closestTimeFromRight) \n",
    "#         if(np.abs(minLeft) < np.abs(minRight)):\n",
    "#             print(\"left\")\n",
    "#             print(minLeft, minRight)\n",
    "#         else:\n",
    "#             print(\"right\")\n",
    "#             print(minLeft, minRight)\n",
    "\n",
    "        # Find left and right peak using max angle difference\n",
    "#         highestAngleTime = windowTimes[windowAngles.index(np.max(windowAngles[0:len(windowAngles)/2]))]]\n",
    "#         left\n",
    "        \n",
    "#         maxAngle1 = np.array([)\n",
    "#         maxAngle2 = np.array([windowTimes[windowAngles.index(np.max(windowAngles[len(windowAngles)/2:len(windowAngles)]))]]) \n",
    "#         leftPeaks.append(maxAngle1)\n",
    "#         rightPeaks.append(maxAngle2)\n",
    "        leftVectors.append(np.array([firstTimestamp]))\n",
    "        rightVectors.append(np.array([secondTimestamp]))\n",
    "\n",
    "        # Save average time and vector\n",
    "        tempList=[]\n",
    "        tempList.append(avgTime)\n",
    "        tempList.extend(avgVector)\n",
    "        avgList.append(np.array(tempList))    \n",
    "\n",
    "    # Save sets\n",
    "    angles[key] = anglesList\n",
    "    avg[key] = avgList\n",
    "    leftPeaksSet[key] = leftVectors\n",
    "    rightPeaksSet[key] = rightVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(windowAngles)\n",
    "value = np.max([windowAngles[x][0] for x in range(0,len(windowAngles))])\n",
    "print(windowAngles.index(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = [1, 3, 5]\n",
    "b = a\n",
    "a[:] = [x + 2 for x in a]\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate some data that lies along a line\n",
    "\n",
    "x = np.mgrid[-2:5:120j]\n",
    "y = np.mgrid[1:9:120j]\n",
    "z = np.mgrid[-5:3:120j]\n",
    "# \n",
    "data = np.concatenate((x[:, np.newaxis], \n",
    "                       y[:, np.newaxis], \n",
    "                       z[:, np.newaxis]), \n",
    "                      axis=1)\n",
    "\n",
    "# Perturb with some Gaussian noise\n",
    "data += np.random.normal(size=data.shape) * 0.4\n",
    "\n",
    "# Calculate the mean of the points, i.e. the 'center' of the cloud\n",
    "datamean = data.mean(axis=0)\n",
    "\n",
    "# Do an SVD on the mean-centered data.\n",
    "uu, dd, vv = np.linalg.svd(data - datamean)\n",
    "\n",
    "# Now vv[0] contains the first principal component, i.e. the direction\n",
    "# vector of the 'best fit' line in the least squares sense.\n",
    "\n",
    "# Now generate some points along this best fit line, for plotting.\n",
    "\n",
    "# I use -7, 7 since the spread of the data is roughly 14\n",
    "# and we want it to have mean 0 (like the points we did\n",
    "# the svd on). Also, it's a straight line, so we only need 2 points.\n",
    "linepts = vv[0] * np.mgrid[-7:7:2j][:, np.newaxis]\n",
    "\n",
    "# shift by the mean to get the line in the right place\n",
    "linepts += datamean\n",
    "\n",
    "# Verify that everything looks right.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mpl_toolkits.mplot3d as m3d\n",
    "\n",
    "ax = m3d.Axes3D(plt.figure())\n",
    "ax.scatter3D(*data.T)\n",
    "ax.plot3D(*linepts.T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "highestAngleTime = windowTimes[windowAngles.index(np.max(windowAngles))]\n",
    "closestTimeFromLeft = left - highestAngleTime\n",
    "closestTimeFromRight = right - highestAngleTime\n",
    "minLeft = np.min(closestTimeFromLeft)\n",
    "minRight = np.min(closestTimeFromRight)\n",
    "if(minLeft < minRight):\n",
    "    print(\"left\")\n",
    "    print(minLeft, minRight)\n",
    "else:\n",
    "    print(\"right\")\n",
    "    print(minLeft, minRight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folderPath = \"/Users/gareyes/Downloads/\"\n",
    "lefts = pd.read_csv(folderPath + \"lefts.csv\", delimiter=\",\", header=None)\n",
    "lefts.columns = ['x', 'y', 'z']\n",
    "rights = pd.read_csv(folderPath + \"rights.csv\", delimiter=\",\", header=None)\n",
    "rights.columns = ['x', 'y', 'z']\n",
    "means = pd.read_csv(folderPath + \"means.csv\", delimiter=\",\", header=None)\n",
    "means.columns = ['x', 'y', 'z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, sharex=True, sharey=False)\n",
    "axs[0].plot(lefts, '-r')\n",
    "axs[0].plot(means, '-g')\n",
    "axs[0].plot(rights, '-b')\n",
    "axs[1].plot(np.sqrt(np.square(lefts.x)+np.square(lefts.y)+np.square(lefts.z)), '-r')\n",
    "axs[1].plot(np.sqrt(np.square(means.x)+np.square(means.y)+np.square(means.z)), '-g')     \n",
    "axs[1].plot(np.sqrt(np.square(rights.x)+np.square(rights.y)+np.square(rights.z)), '-b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "for i in range(0,len(vectors)):    \n",
    "    plt.plot([0, vectors[i][0]], [0, vectors[i][1]], [0, vectors[i][2]], '-r')    \n",
    "# avgVector = np.array([np.sum(dataFrame[0:].x), np.sum(dataFrame[0:].y), np.sum(dataFrame[0:].z)])/len(dataFrame)\n",
    "plt.plot([0, avgVector[0]], [0, avgVector[1]], [0, avgVector[2]], '-k')\n",
    "plt.show()\n",
    "# plt.plot(dataFrame[0:].x)\n",
    "# plt.plot(dataFrame[0:].y)\n",
    "# plt.plot(dataFrame[0:].z)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "for i in range(1,len(dataFrame)+1):\n",
    "    plt.plot([0, dataFrame[i-1:i].x], [0, dataFrame[i-1:i].y], [0, dataFrame[i-1:i].z], '-r')    \n",
    "avgVector = np.array([np.sum(dataFrame[0:].x), np.sum(dataFrame[0:].y), np.sum(dataFrame[0:].z)])/len(dataFrame)\n",
    "plt.plot([0, avgVector[0]], [0, avgVector[1]], [0, avgVector[2]], '-k')\n",
    "plt.show()\n",
    "# plt.plot(dataFrame[0:].x)\n",
    "# plt.plot(dataFrame[0:].y)\n",
    "# plt.plot(dataFrame[0:].z)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lefts = np.array(lefts)\n",
    "rights = np.array(rights)\n",
    "# lm = lefts.mean(axis = 0)\n",
    "# rm = rights.mean(axis = 0)\n",
    "# dm = lm - rm\n",
    "deltas = [lefts[vi] - rights[vi] for vi in range(min(len(lefts), len(rights)))]# deltas = [lefts[vi] - rights[vi] \n",
    "deltas = np.array(deltas)\n",
    "dm = deltas.mean(axis=0)\n",
    "\n",
    "vectors = np.array(vectors)\n",
    "soa =np.array(plotVector) \n",
    "def get_mid(v):\n",
    "    return np.min(v) + ((np.max(v) - np.min(v)) / 2)\n",
    "\n",
    "X,Y,Z,U,V,W = zip(*soa)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.quiver(X,Y,Z,U,V,W, pivot=\"tail\", arrow_length_ratio=0.02, length=magv(vectors.mean(axis=0)), linewidths=0.3)\n",
    "# dv = np.array([np.array(get_mid(U)), get_mid(np.array(V)), get_mid(np.array(W))])\n",
    "dv = rights.mean(axis=0)\n",
    "pvectors = [[dv[0], dv[1], dv[2], dm[0], dm[1], dm[2]]]\n",
    "# pvectors = [[0, 0, 0, dm[0], dm[1], dm[2]]]\n",
    "pvectors = np.array(pvectors)\n",
    "X2,Y2,Z2,U2,V2,W2 = zip(*pvectors)\n",
    "ax.quiver(X2,Y2,Z2,U2,V2,W2, pivot=\"tail\", arrow_length_ratio=0.02,color=\"red\", length=magv(deltas.mean(axis=0)), linewidths=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot([0,1],[0,1],[0,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "timevectors = np.array(timevectors)\n",
    "# from sklearn.decomposition import PCA\n",
    "# pca = PCA(n_components=1)\n",
    "# pca.fit(vectors)\n",
    "# pa = pca.components_[0]\n",
    "# pvectors = [[0,0,0,pa[0],pa[1],pa[2]]]\n",
    "# pvectors = np.array(pvectors)\n",
    "soa =np.array(plotVector) \n",
    "\n",
    "X,Y,Z,U,V,W = zip(*soa)\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "# ax.quiver(X,Y,Z,U,V,W, pivot=\"tail\", arrow_length_ratio=0.05)\n",
    "# X2,Y2,Z2,U2,V2,W2 = zip(*pvectors)\n",
    "# ax.quiver(X2,Y2,Z2,U2,V2,W2, pivot=\"tail\", arrow_length_ratio=0.05,color=\"red\")\n",
    "# ax.set_xlim([-1,1])\n",
    "# ax.set_ylim([-1,1])\n",
    "# ax.set_zlim([-1,1])\n",
    "\n",
    "plt.ion()\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.set_xlim(-50, 50)\n",
    "ax.set_ylim(-50, 50)\n",
    "ax.set_zlim(-50, 50)\n",
    "prev_time = timevectors[0][0] - .001\n",
    "total_time = timevectors[timevectors.shape[0]-1][0] - timevectors[0][0]\n",
    "print((total_time))\n",
    "# for i in range(timevectors.shape[0]):\n",
    "i = 0\n",
    "import time as system_time\n",
    "while i < timevectors.shape[0]:\n",
    "    # print((timevectors[i]))\n",
    "    time = timevectors[i][0]\n",
    "    x = timevectors[i][1]\n",
    "    y = timevectors[i][2]\n",
    "    z = timevectors[i][3]\n",
    "    ax.scatter(x, y, z)\n",
    "    pause_time = time - prev_time\n",
    "    pause_time /= 100000\n",
    "    if pause_time == 0.0:\n",
    "        pause_time = .00001\n",
    "    print((pause_time))\n",
    "    # system_time.sleep(pause_time)\n",
    "    plt.pause(pause_time)\n",
    "    prev_time = timevectors[i][0]\n",
    "    i += 5\n",
    "while True:\n",
    "    plt.pause(0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "timevectors = np.array(timevectors)\n",
    "timevectors[:,1] = timevectors[:,1][::-1]\n",
    "timevectors[:,2] = timevectors[:,2][::-1]\n",
    "timevectors[:,3] = timevectors[:,3][::-1]\n",
    "soa = np.array(plotVector)\n",
    "\n",
    "X,Y,Z,U,V,W = zip(*soa)\n",
    "\n",
    "plt.ion()\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.set_xlim(-200, 200)\n",
    "ax.set_ylim(-200, 200)\n",
    "ax.set_zlim(-300, 200)\n",
    "# prev_time = timevectors[0][0]\n",
    "# total_time = timevectors[timevectors.shape[0]-1][0] - timevectors[0][0]\n",
    "# time = timevectors[i][0]\n",
    "i=0\n",
    "while i < timevectors.shape[0]:\n",
    "    x = timevectors[i][1]\n",
    "    y = timevectors[i][2]\n",
    "    z = timevectors[i][3]\n",
    "    ax.scatter(x, y, z)\n",
    "    plt.pause(0.0000001)\n",
    "    i += 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ax.quiver(X, Y, Z, U, V, W)\n",
    "\n",
    "soa = np.array([[0, 0, 3, 2], [0, 0, 1, 1], [0, 0, 9, 9]])\n",
    "X, Y, U, V = zip(*soa)\n",
    "plt.figure()\n",
    "ax = plt.gca()\n",
    "ax.quiver(X, Y, U, V, angles='xy', scale_units='xy', scale=1)\n",
    "ax.set_xlim([-1, 10])\n",
    "ax.set_ylim([-1, 10])\n",
    "plt.draw()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = pd.Series([0, 3, 0, 4])\n",
    "print(s)\n",
    "print(s.nonzero())\n",
    "print(s.iloc[s.nonzero()[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # Get all data\n",
    "    for key in rawSet:\n",
    "\n",
    "        # Get data\n",
    "        print(key)\n",
    "        liveCorrData = liveCorrSet[key[:-7]+\"corr.csv\"]\n",
    "        offlineCorrData = offlineCorrSet[key[:-7]+\"offline.csv\"]\n",
    "\n",
    "        # Plot combined correlation\n",
    "        figureFile = figurePath + \"correlation/\" + key[:-7] + \"correlation.pdf\"\n",
    "        plot_corrs(liveCorrData, offlineCorrData, fileName=figureFile, saveToFile=True)        \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Get all data\n",
    "for key in windowDataSet:\n",
    "\n",
    "    # Plot windows\n",
    "    fig, axs = plt.subplots(2, 1, sharex=True, sharey=False)\n",
    "    fig.set_size_inches(15,15)\n",
    "\n",
    "    \n",
    "    # Get data\n",
    "    df = windowDataSet[key]    \n",
    "    axs[0].plot(df.timestamp, df.feature, '-r', label=\"feature\")\n",
    "    axs[0].plot(df.timestamp, df.reference, '-b', label=\"reference\")\n",
    "    axs[1].plot(df.timestamp, df.feature, '-g', label=\"feature\")\n",
    "    axs[1].plot(df.timestamp, df.adjusted, '-b', label=\"adjusted\")    \n",
    "axs[0].set_title(\"feat-ref: \" + str(scipy.stats.pearsonr(df.feature, df.reference)[0]))\n",
    "axs[1].set_title(\"feat-adjref: \" + str(scipy.stats.pearsonr(df.feature, df.adjusted)[0]))\n",
    "plt.show()\n",
    "print(\"done plotting windows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(dataFrame.left.nonzero())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "windowDataSet = loadWindows()\n",
    "# windowDataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mpl.rcParams['legend.fontsize'] = 10\n",
    "\n",
    "# fig = plt.figure()\n",
    "# ax = fig.gca(projection='3d')\n",
    "theta = np.linspace(-4 * np.pi, 4 * np.pi, 100)\n",
    "z = np.linspace(-2, 2, 100)\n",
    "r = z**2 + 1\n",
    "x = r * np.sin(theta)\n",
    "y = r * np.cos(theta)\n",
    "# ax.plot(x, y, z, label='parametric curve')\n",
    "# ax.legend()\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "\n",
    "soa = np.array([[0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1, 2]])\n",
    "\n",
    "X, Y, Z, U, V, W = zip(*soa)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.quiver(X, Y, Z, U, V, W)\n",
    "ax.set_xlim([-1, 1])\n",
    "ax.set_ylim([-1, 1])\n",
    "ax.set_zlim([-1, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get all data\n",
    "rawDataSet = loadRawData()\n",
    "featureDataSet = loadFeatureData()\n",
    "offlineCorrDataSet = loadOfflineCorrData()\n",
    "\n",
    "# Iterate the data set\n",
    "for key in rawDataSet:\n",
    "\n",
    "    lefts = []\n",
    "    rights = []\n",
    "    \n",
    "    # Get data\n",
    "    rawData, lefts, rights = rawDataSet[key]\n",
    "    \n",
    "    # Plot magnetometer and feature\n",
    "    print(lefts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for User Study Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Rename all study files\n",
    "\n",
    "path = \"/Volumes/HanSolo/Dropbox/Georgia Tech/Synchro/Data/User Study/Flat/noise/\"\n",
    "for root, folders, files in os.walk(path):\n",
    "    for f in files:\n",
    "        oldName = root + f\n",
    "        newName = root + f.replace(\"ssensorData\", \"raw\")\n",
    "        os.rename(oldName, newName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove extra comma at the end of csv file\n",
    "\n",
    "path = \"/Volumes/HanSolo/Dropbox/Georgia Tech/Synchro/Data/User Study/Flat/noise/\"\n",
    "inputPath = path + \"data/\"\n",
    "outputPath = path + \"out/\"\n",
    "if not os.path.exists(outputPath):\n",
    "    os.makedirs(outputPath)\n",
    "for root, folders, files in os.walk(inputPath):\n",
    "    for f in files:       \n",
    "        with open(root + f, 'rb') as input_file:\n",
    "            with open(outputPath + f, 'wb') as output_file:\n",
    "                data = csv.reader(input_file)\n",
    "                writer = csv.writer(output_file)\n",
    "                for line in data:\n",
    "                    newLine = line[:-1]\n",
    "                    writer.writerow(newLine)            \n",
    "        input_file.close()\n",
    "        output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Synthetic Data and Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EWMA Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputData = [6.0,8.0,1.0,2.0,3.0,10.0,5.0,7.0,1.0,3.0]\n",
    "# 6.0,6.2,5.68,5.311999999999999,5.080799999999999,5.5727199999999995,5.515447999999999,5.663903199999999,5.19751288,4.977761591999999,\n",
    "# 6.0,7.800000000000001,1.6800000000000002,1.968,2.8968000000000003,9.28968,5.428968,6.8428968,1.58428968,2.858428968,\n",
    "# outputData = [7.0,6.8,6.62,6.258,6.4322,5.98898,5.490082,5.741073800000001,6.066966420000001,5.560269778]\n",
    "\n",
    "# Python version\n",
    "def get_ewma(df, alpha=1):\n",
    "    # Compute alpha and exponential moving average\n",
    "    span = (2-alpha)/alpha\n",
    "    df_smooth = pd.ewma(df, span=span, adjust=False)\n",
    "    return df_smooth\n",
    "newDf = pd.DataFrame(inputData, columns=[\"data\"])\n",
    "pythonValues = get_ewma(newDf, alpha=0.1)\n",
    "# print(pythonValues)\n",
    "\n",
    "# Android version\n",
    "alpha = 0.1\n",
    "androidValues = []\n",
    "for i in inputData:\n",
    "    avgValue = (alpha * avgValue) + ((1 - alpha)*i)\n",
    "    androidValues.append(avgValue)\n",
    "# print(androidValues)\n",
    "\n",
    "# Java version\n",
    "alpha = 0.1\n",
    "javaValues = []\n",
    "oldValue = inputData[0]\n",
    "for i in inputData:\n",
    "    # Jason\n",
    "    newValue = oldValue + alpha * (i - oldValue);\n",
    "    oldValue = newValue;\n",
    "    javaValues.append(newValue)\n",
    "# print(javaValues)\n",
    "\n",
    "plt.plot(inputData, '-r')\n",
    "plt.plot(androidValues, '-g')\n",
    "plt.plot(javaValues, '-b')\n",
    "plt.plot(pythonValues, '-k')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
