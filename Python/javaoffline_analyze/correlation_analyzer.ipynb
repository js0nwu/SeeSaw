{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run imports.py\n",
    "%run lib_util.py\n",
    "%run lib_plot.py\n",
    "%run lib_summary.py\n",
    "%run lib_misc.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Folder Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done loading raw data\n",
      "done loading subsample data\n",
      "done loading feature\n",
      "done loading offline\n",
      "done loading lag\n",
      "done loading flashes\n"
     ]
    }
   ],
   "source": [
    "# Get all data\n",
    "rawDataSet = loadRawData()\n",
    "subsampleDataSet = loadSubsampleData()\n",
    "featureDataSet = loadFeatureData()\n",
    "offlineCorrDataSet = loadOfflineCorrData()\n",
    "# onlineCorrDataSet = load\n",
    "lagDataSet = loadLagData()\n",
    "leftTicksSet, rightTicksSet = loadFlashData(rawDataSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key: t20180110004819_p2_sit_x1_raw.csv\n",
      "key: t20180110004849_p2_sit_x1_raw.csv\n",
      "key: t20180110004914_p2_sit_x1_raw.csv\n",
      "key: t20180110004944_p2_sit_x1_raw.csv\n",
      "key: t20180110005016_p2_sit_x1_raw.csv\n",
      "key: t20180110005045_p2_sit_x1_raw.csv\n",
      "key: t20180110005116_p2_sit_x1_raw.csv\n",
      "key: t20180110005249_p2_noise_x1_raw.csv\n",
      "key: t20180110005328_p2_noise_x1_raw.csv\n",
      "key: t20180110005402_p2_noise_x1_raw.csv\n",
      "key: t20180110005452_p2_noise_x1_raw.csv\n",
      "key: t20180110005522_p2_noise_x1_raw.csv\n",
      "key: t20180110005553_p2_noise_x1_raw.csv\n",
      "done plotting overview\n",
      "done plotting offline corr\n"
     ]
    }
   ],
   "source": [
    "vizOverview(rawDataSet, subsampleDataSet, featureDataSet, offlineCorrDataSet, lagDataSet, plotSubsample=False)\n",
    "# vizSubsampler(rawDataSet, subsampleDataSet)\n",
    "vizOfflineCorr(offlineCorrDataSet)\n",
    "# vizFeaturePlus(rawDataSet, featureDataSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done loading raw data\n",
      "done loading feature\n"
     ]
    }
   ],
   "source": [
    "rawDataSet = loadRawData()\n",
    "featureDataset = loadFeatureData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vizFeaturePlus(rawDataSet, featureDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vizMagnetometer(rawDataSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vizOverview(rawDataSet, subsampleDataSet, featureDataSet, offlineCorrDataSet, lagDataSet, plotSubsample=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "leftTicksSet, rightTicksSet = loadFlashData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fileToUse = \"t1489504654056_p1_sit_x2_raw.csv\"\n",
    "plt.clf()\n",
    "fig = plt.figure()\n",
    "\n",
    "\n",
    "\n",
    "for key in leftTicksSet.keys():\n",
    "    if key==fileToUse:\n",
    "        leftTicksSet[key]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "leftTicksSet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Overview\n",
    "\n",
    "fileToUse = \"t1489504654056_p1_sit_x2_raw.csv\"\n",
    "\n",
    "# Make folders\n",
    "paths = [figurePath + \"newfigs/\"]\n",
    "for path in paths:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "# Iterate the data set\n",
    "for key in rawDataSet:\n",
    "\n",
    "    if key==fileToUse:\n",
    "    \n",
    "        # Get data\n",
    "        rawData = rawDataSet[key]\n",
    "        subsampleData = subsampleDataSet[key]\n",
    "        featureData = featureDataSet[key[:-7]+\"feature.csv\"]\n",
    "        offlineCorrData = offlineCorrDataSet[key[:-7]+\"offline.csv\"]\n",
    "        lagData = lagDataSet[key[:-7]+\"lag.csv\"]\n",
    "\n",
    "        # Plot magnetometer and feature\n",
    "        path = figurePath + \"newfigs/\"\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        figureFile = path + key[:-4] + \".pdf\"\n",
    "\n",
    "        #df1 = rawdata, df2 = subsampledata, df3 = featuremap, df4 = offlinecorr, df5 = lag\n",
    "        fig, axs = plt.subplots(5, 1, sharex=True, sharey=False)\n",
    "        fig.set_size_inches(15,15)\n",
    "        \n",
    "        plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "        \n",
    "        subsampleData['magnitude'] = np.sqrt(np.square(subsampleData.x)+np.square(subsampleData.y)+np.square(subsampleData.z))\n",
    "        axs[0].plot(subsampleData.timestamp, subsampleData.x, '-k')\n",
    "        axs[1].plot(subsampleData.timestamp, subsampleData.y, '-k')\n",
    "        axs[2].plot(subsampleData.timestamp, subsampleData.z, '-k')\n",
    "        axs[3].plot(subsampleData.timestamp, subsampleData.magnitude, '-k')    \n",
    "        \n",
    "        axs[4].plot(featureData.timestamp, featureData.feature, '-k')\n",
    "    #     axs[5].plot(df4.timestamp, df4.correlation, '-b')\n",
    "    #     axs[5].set_ylim(-1,1)\n",
    "    #     axs[6].plot(df5.timestamp, df5.lagtime, '-b')\n",
    "        axs[0].set_title('x')\n",
    "        axs[1].set_title('y')\n",
    "        axs[2].set_title('z')\n",
    "        axs[3].set_title('magnitude')\n",
    "        axs[4].set_title('feature')\n",
    "    #     axs[5].set_title('offlinecorr')\n",
    "    #     axs[6].set_title('xcorr')\n",
    "        rawLeft = [x for x in rawData.left if x != 0]\n",
    "        rawRight = [x for x in rawData.right if x != 0]    \n",
    "        for i in rawLeft:\n",
    "            axs[0].axvline(i, color='b')\n",
    "            axs[1].axvline(i, color='b')\n",
    "            axs[2].axvline(i, color='b')      \n",
    "            axs[3].axvline(i, color='b')    \n",
    "            axs[4].axvline(i, color='b')\n",
    "    #         axs[6].axvline(i, color='b')\n",
    "        for i in rawRight:\n",
    "            axs[0].axvline(i, color='r')\n",
    "            axs[1].axvline(i, color='r')\n",
    "            axs[2].axvline(i, color='r')    \n",
    "            axs[3].axvline(i, color='r')    \n",
    "            axs[4].axvline(i, color='r')    \n",
    "    #         axs[6].axvline(i, color='r')\n",
    "    #     if(saveToFile):\n",
    "    #         fig.savefig(fileName)\n",
    "    #     else:\n",
    "        plt.show()        \n",
    "        plt.close()    \n",
    "    \n",
    "print(\"done plotting overview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "onlineCorrDataSet = loadLiveCorrData()\n",
    "    \n",
    "# Make folders\n",
    "paths = [figurePath + \"livecorr/\"]\n",
    "for path in paths:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "fig, axs = plt.subplots(1, 1, sharex=True)\n",
    "fig.set_size_inches(15, 15) \n",
    "        \n",
    "# Get all data\n",
    "for key in onlineCorrDataSet:\n",
    "\n",
    "    # Get data\n",
    "    liveCorrData = onlineCorrDataSet[key]\n",
    "\n",
    "    \n",
    "    df = liveCorrData    \n",
    "    df.timestamp = df.timestamp - df.timestamp[0]\n",
    "    axs.plot(df.timestamp, df.correlation)\n",
    "    axs.set_ylim(-1,1)\n",
    "    axs.axhline(0.9)\n",
    "\n",
    "figureFile = figurePath + \"livecorr/\" + \"pilot1corr\" + \".pdf\"\n",
    "plt.show()\n",
    "\n",
    "print(\"done plotting online corr\"    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "offlineCorrDataSet = loadOfflineCorrData()\n",
    "vizOfflineCorr(offlineCorrDataSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "thresholds = [0.70, 0.75, 0.8, 0.85, 0.9, 0.95]\n",
    "outputSet = {}\n",
    "for i in thresholds:\n",
    "    averageTimes = []\n",
    "    for key in onlineCorrDataSet.keys():\n",
    "        df = onlineCorrDataSet[key]\n",
    "        st = df.timestamp[0]\n",
    "        dfsync = df[(df.correlation >= i) & (df.timestamp - st > IGNORE_TIME)]\n",
    "        sync_time = dfsync.timestamp.values[0] - st if len(dfsync > 0) else -1\n",
    "#         print(sync_time)\n",
    "        if sync_time > 0:\n",
    "            averageTimes.append(sync_time)\n",
    "    outputSet[i] = np.mean(averageTimes) / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this block is used to specify which activity to use when generating the 3d plots\n",
    "ACTIVITY = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error in /media/jwpilly/PillowDisk/Research/Synchro/figuredata/figure12/Study_v2/P1/synchro/t20170401121118_p1_sit//data/t20170401121118_p1_sit_x2_corr\n",
      "No columns to parse from file\n",
      "error in /media/jwpilly/PillowDisk/Research/Synchro/figuredata/figure12/Study_v2/P3/synchro/t20170402163502_p3_walk//data/t20170402163502_p3_walk_x2_corr\n",
      "No columns to parse from file\n"
     ]
    }
   ],
   "source": [
    "# this block is used to calculate the true positives and false positives of the 3d plots the data is stored in tps and fps\n",
    "\n",
    "# tp = syncRateFile(\"/Users/jwpilly/Research/Synchro/Study_v2/P1/synchro/t20170401112834_p1_sit/\")\n",
    "# fp = syncRateFile(\"/Users/jwpilly/Research/Synchro/Study_v2/P1/synchro/t20170401112834_p1_sit/\", False)\n",
    "# baseFilePath = \"/Users/jwpilly/Research/Synchro/Study_v2/\"\n",
    "# baseFilePath = \"/Users/jwpilly/Research/Synchro/Study_v1_750/\"\n",
    "baseFilePath = \"/media/jwpilly/PillowDisk/Research/Synchro/figuredata/figure12/Study_v2\"\n",
    "tps = {}\n",
    "fps = {}\n",
    "ttot = {}\n",
    "ftot = {}\n",
    "for root, folders, files in os.walk(baseFilePath):\n",
    "    if not root.endswith(\"synchro\") or \"MACOSX\" in root:\n",
    "        continue\n",
    "    sfolders = sorted(folders, key=lambda x : int(x.split(\"_\")[0].replace(\"t\", \"\")))\n",
    "    efolders = [f for f in sfolders[:] if \"prep\" not in f in f]\n",
    "    if ACTIVITY != \"all\":\n",
    "        efolderpaths = [root + \"/\" + f + \"/\" for f in efolders if ACTIVITY in f]\n",
    "    else:\n",
    "        efolderpaths = [root + \"/\" + f + \"/\" for f in efolders]\n",
    "    etps = [syncRateFile(efolderpaths[i]) for i in range(len(efolderpaths))]\n",
    "    efps = [syncRateFile(efolderpaths[i], True) for i in range(len(efolderpaths))]\n",
    "    for th in thresholds:\n",
    "        if th not in tps:\n",
    "            tps[th] = np.array([0 for i in times])\n",
    "        if th not in fps:\n",
    "            fps[th] = np.array([0 for i in times])\n",
    "        if th not in ttot:\n",
    "            ttot[th] = np.array([0 for i in times])\n",
    "        if th not in ftot:\n",
    "            ftot[th] = np.array([0 for i in times])\n",
    "        for stps in etps:\n",
    "            tps[th] += np.array(stps[th][0])\n",
    "            ttot[th] += np.array(stps[th][1])\n",
    "        for sfps in efps:\n",
    "            fps[th] += np.array(sfps[th][0])\n",
    "            ftot[th] += np.array(sfps[th][1])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4c063ca0f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4c0644cda0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdkAAAF3CAYAAADpUTGcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSpLUEkNWkqSWGLKSJLXk/wEIrW9tFsSOVwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4c0635f6a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this block is used to generate the precision graph with 0.5 * tp + 0.5 tn\n",
    "\n",
    "precisions = []\n",
    "for th in thresholds:\n",
    "    for ti in range(len(times)):\n",
    "        ptime = times[ti]\n",
    "        if ptime < IGNORE_TIME / 1000:\n",
    "            continue\n",
    "        tp_rate = tps[th][ti] / ttot[th][ti]\n",
    "        fp_rate = fps[th][ti] / ftot[th][ti]\n",
    "        prec = 0.5 * tp_rate + 0.5 * (1 - fp_rate)\n",
    "#         prec = tp_rate\n",
    "        point = [th, ptime, prec]\n",
    "        precisions.append(point)\n",
    "precisions = np.array(precisions)\n",
    "xd = precisions[:,0]\n",
    "yd = precisions[:,1]\n",
    "zd = precisions[:,2]\n",
    "\n",
    "import matplotlib.mlab as mlab\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.collections import PolyCollection\n",
    "from matplotlib.colors import colorConverter\n",
    "from matplotlib import cm\n",
    "\n",
    "\n",
    "plt.clf()\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "# ax = fig.gca(projection='3d')\n",
    "ax = fig.gca()\n",
    "xi = np.linspace(np.min(xd), np.max(xd))\n",
    "yi = np.linspace(np.min(yd), np.max(yd))\n",
    "X, Y = np.meshgrid(xi, yi)\n",
    "Z = mlab.griddata(xd, yd, zd, xi, yi, interp='linear')\n",
    "# Z = precisions\n",
    "# surf = ax.plot_surface(X, Y, Z, cmap=cm.jet)\n",
    "# ax.set_zlim3d(np.min(Z), np.max(Z))\n",
    "# fig.colorbar(surf)\n",
    "# contour = ax.contour(xd, yd, zd, cmap=cm.jet)\n",
    "contour = ax.contourf(X, Y, Z, np.linspace(0.0, 1.0, 21, endpoint=True), cmap=cm.jet)\n",
    "# contour = ax.contourf(X, Y, precisions, np.linspace(0.0, 1.0, 21, endpoint=True), cmap=cm.jet)\n",
    "\n",
    "fig.colorbar(contour)\n",
    "\n",
    "\n",
    "ax.set_xlabel(\"Threshold\")\n",
    "ax.set_ylabel(\"Time (s)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jwpilly/anaconda/envs/magnetics/lib/python3.4/site-packages/matplotlib/pyplot.py:524: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "# this graph is used to generate the true positive graph\n",
    "\n",
    "precisions = []\n",
    "for th in thresholds:\n",
    "    for ti in range(len(times)):\n",
    "        ptime = times[ti]\n",
    "        if ptime < IGNORE_TIME / 1000:\n",
    "            continue\n",
    "        tp_rate = tps[th][ti] / ttot[th][ti]\n",
    "        prec = tp_rate\n",
    "        point = [th, ptime, prec]\n",
    "        precisions.append(point)\n",
    "precisions = np.array(precisions)\n",
    "xd = precisions[:,0]\n",
    "yd = precisions[:,1]\n",
    "zd = precisions[:,2]\n",
    "\n",
    "import matplotlib.mlab as mlab\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.collections import PolyCollection\n",
    "from matplotlib.colors import colorConverter\n",
    "from matplotlib import cm\n",
    "\n",
    "\n",
    "plt.clf()\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "# ax = fig.gca(projection='3d')\n",
    "ax = fig.gca()\n",
    "xi = np.linspace(np.min(xd), np.max(xd))\n",
    "yi = np.linspace(np.min(yd), np.max(yd))\n",
    "X, Y = np.meshgrid(xi, yi)\n",
    "Z = mlab.griddata(xd, yd, zd, xi, yi, interp='linear')\n",
    "# Z = precisions\n",
    "# surf = ax.plot_surface(X, Y, Z, cmap=cm.jet)\n",
    "# ax.set_zlim3d(np.min(Z), np.max(Z))\n",
    "# fig.colorbar(surf)\n",
    "# contour = ax.contour(xd, yd, zd, cmap=cm.jet)\n",
    "contour = ax.contourf(X, Y, Z, np.linspace(0.0, 1.0, 21, endpoint=True), cmap=cm.jet)\n",
    "# contour = ax.contourf(X, Y, precisions, np.linspace(0.0, 1.0, 21, endpoint=True), cmap=cm.jet)\n",
    "\n",
    "fig.colorbar(contour)\n",
    "\n",
    "\n",
    "ax.set_xlabel(\"Threshold\")\n",
    "ax.set_ylabel(\"Time (s)\")\n",
    "plt.savefig(\"3d_tp_\" + ACTIVITY + \".png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jwpilly/anaconda/envs/magnetics/lib/python3.4/site-packages/matplotlib/pyplot.py:524: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n",
      "/Users/jwpilly/anaconda/envs/magnetics/lib/python3.4/site-packages/matplotlib/contour.py:1518: UserWarning: Log scale: values of z <= 0 have been masked\n",
      "  warnings.warn('Log scale: values of z <= 0 have been masked')\n"
     ]
    }
   ],
   "source": [
    "# this block is used to generate the false positives per hour 3d graph\n",
    "\n",
    "precisions = []\n",
    "for th in thresholds:\n",
    "    for ti in range(len(times)):\n",
    "        ptime = times[ti]\n",
    "        if ptime < IGNORE_TIME / 1000:\n",
    "            continue\n",
    "        prec = (fps[th][ti] / ftot[th][ti]) / 6 * 3600\n",
    "        point = [th, ptime, prec]\n",
    "        precisions.append(point)\n",
    "precisions = np.array(precisions)\n",
    "xd = precisions[:,0]\n",
    "yd = precisions[:,1]\n",
    "zd = precisions[:,2]\n",
    "\n",
    "import matplotlib.mlab as mlab\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.collections import PolyCollection\n",
    "from matplotlib.colors import colorConverter\n",
    "from matplotlib import cm\n",
    "\n",
    "\n",
    "plt.clf()\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "# ax = fig.gca(projection='3d')\n",
    "ax = fig.gca()\n",
    "xi = np.linspace(np.min(xd), np.max(xd))\n",
    "yi = np.linspace(np.min(yd), np.max(yd))\n",
    "X, Y = np.meshgrid(xi, yi)\n",
    "Z = mlab.griddata(xd, yd, zd, xi, yi, interp='linear')\n",
    "# Z = precisions\n",
    "# surf = ax.plot_surface(X, Y, Z, cmap=cm.jet)\n",
    "# ax.set_zlim3d(np.min(Z), np.max(Z))\n",
    "# fig.colorbar(surf)\n",
    "# contour = ax.contour(xd, yd, zd, cmap=cm.jet\n",
    "from matplotlib.colors import LogNorm\n",
    "lvls = np.logspace(0, np.log10(np.max(zd)), 26, endpoint=True)\n",
    "contour = ax.contourf(X, Y, Z, levels=lvls, cmap=cm.jet, norm=LogNorm())\n",
    "# contour = ax.contourf(X, Y, precisions, np.linspace(0.0, 1.0, 21, endpoint=True), cmap=cm.jet)\n",
    "\n",
    "fig.colorbar(contour, ticks=lvls, format=\"%.1f\")\n",
    "\n",
    "\n",
    "ax.set_xlabel(\"Threshold\")\n",
    "ax.set_ylabel(\"Time (s)\")\n",
    "plt.savefig(\"3d_fp_\" + ACTIVITY + \".png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jwpilly/anaconda/envs/magnetics/lib/python3.4/site-packages/matplotlib/pyplot.py:524: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "# this graph is used to generate the false positive rate graph\n",
    "\n",
    "precisions = []\n",
    "for th in thresholds:\n",
    "    for ti in range(len(times)):\n",
    "        ptime = times[ti]\n",
    "        if ptime < IGNORE_TIME / 1000:\n",
    "            continue\n",
    "        fp_rate = fps[th][ti] / ftot[th][ti]\n",
    "        prec = fp_rate\n",
    "        point = [th, ptime, prec]\n",
    "        precisions.append(point)\n",
    "precisions = np.array(precisions)\n",
    "xd = precisions[:,0]\n",
    "yd = precisions[:,1]\n",
    "zd = precisions[:,2]\n",
    "\n",
    "import matplotlib.mlab as mlab\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.collections import PolyCollection\n",
    "from matplotlib.colors import colorConverter\n",
    "from matplotlib import cm\n",
    "\n",
    "\n",
    "plt.clf()\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "# ax = fig.gca(projection='3d')\n",
    "ax = fig.gca()\n",
    "xi = np.linspace(np.min(xd), np.max(xd))\n",
    "yi = np.linspace(np.min(yd), np.max(yd))\n",
    "X, Y = np.meshgrid(xi, yi)\n",
    "Z = mlab.griddata(xd, yd, zd, xi, yi, interp='linear')\n",
    "# Z = precisions\n",
    "# surf = ax.plot_surface(X, Y, Z, cmap=cm.jet)\n",
    "# ax.set_zlim3d(np.min(Z), np.max(Z))\n",
    "# fig.colorbar(surf)\n",
    "# contour = ax.contour(xd, yd, zd, cmap=cm.jet)\n",
    "contour = ax.contourf(X, Y, Z, np.linspace(0.0, 1.0, 21, endpoint=True), cmap=cm.jet)\n",
    "# contour = ax.contourf(X, Y, precisions, np.linspace(0.0, 1.0, 21, endpoint=True), cmap=cm.jet)\n",
    "\n",
    "fig.colorbar(contour)\n",
    "\n",
    "\n",
    "ax.set_xlabel(\"Threshold\")\n",
    "ax.set_ylabel(\"Time (s)\")\n",
    "plt.savefig(\"3d_fpr_\" + ACTIVITY + \".png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tp = syncRateFile(\"/Users/jwpilly/Research/Synchro/Study_v2/P1/synchro/t20170401112834_p1_sit/\", True)\n",
    "tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# correlateSyncAccelFile(\"/Users/jwpilly/Research/Synchro/Study_v2/P1/synchro/t20170401112834_p1_sit/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "processSyncFile(\"/Users/jwpilly/Research/Synchro/Study_v2/P1/synchro/t20170401112834_p1_sit/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this block iterates through all of the study data files to generate the sync times graph\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "# baseFilePath = \"/Users/jwpilly/Documents/Research/Synchro/Study_v2/\"\n",
    "# baseFilePath = \"/Users/jwpilly/Research/Synchro/Study_v2/\"\n",
    "baseFilePath = \"/Volumes/HanSolo/Dropbox/Georgia Tech/Synchro/Data/Study_v1/Flat/sync1000/\"\n",
    "presults = []\n",
    "for root, folders, files in os.walk(baseFilePath):\n",
    "    if not root.endswith(\"synchro\") or \"MACOSX\" in root:\n",
    "        continue\n",
    "    sfolders = sorted(folders, key=lambda x : int(x.split(\"_\")[0].replace(\"t\", \"\")))\n",
    "    efolders = [f for f in sfolders[:] if \"prep\" not in f]\n",
    "    efolderpaths = [root + \"/\" + f + \"/\" for f in efolders]\n",
    "#     print(efolderpaths)\n",
    "    efoldertimes = [(efolderpaths[i].split(\"_\")[-1][:-1] + str(i), processSyncFile(efolderpaths[i])) for i in range(len(efolderpaths))]\n",
    "    presults.append(efoldertimes)\n",
    "thresholds = [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.60, 0.65, 0.70, 0.75, 0.8, 0.85, 0.9, 0.95]\n",
    "# print(efoldertimes)\n",
    "syncresults = []\n",
    "for th in thresholds:\n",
    "    presultsa = [[i[1][th] for i in t] for t in [sorted(r) for r in presults]]\n",
    "    paverages = reduce(lambda x, y : np.array(x) + np.array(y), presultsa) / len(presultsa)\n",
    "    syncresults.append(paverages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this block iterates through all of the study data files to generate the sync times graph but for the noise files\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "# baseFilePath = \"/Users/jwpilly/Documents/Research/Synchro/Study_v2/\"\n",
    "baseFilePath = \"/Users/jwpilly/Research/Synchro/Study_v2/\"\n",
    "presults = []\n",
    "for root, folders, files in os.walk(baseFilePath):\n",
    "    if not root.endswith(\"synchro\") or \"MACOSX\" in root:\n",
    "        continue\n",
    "    sfolders = sorted(folders, key=lambda x : int(x.split(\"_\")[0].replace(\"t\", \"\")))\n",
    "    efolders = [f for f in sfolders[:] if \"prep\" not in f]\n",
    "    efolderpaths = [root + \"/\" + f + \"/\" for f in efolders]\n",
    "#     print(efolderpaths)\n",
    "    efoldertimes = [(efolderpaths[i].split(\"_\")[-1][:-1] + str(i), processSyncFile(efolderpaths[i], True)) for i in range(len(efolderpaths))]\n",
    "    presults.append(efoldertimes)\n",
    "thresholds = [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.60, 0.65, 0.70, 0.75, 0.8, 0.85, 0.9, 0.95]\n",
    "# print(efoldertimes)\n",
    "syncresults = []\n",
    "for th in thresholds:\n",
    "    presultsa = [[i[1][th] for i in t] for t in [sorted(r) for r in presults]]\n",
    "    paverages = reduce(lambda x, y : np.array(x) + np.array(y), presultsa) / len(presultsa)\n",
    "    syncresults.append(paverages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this block plots the results of the sync times graph. depending on which block was run before this one, the graph outputted is for sync data/noise data\n",
    "\n",
    "%matplotlib inline\n",
    "plt.clf()\n",
    "x = thresholds\n",
    "figure = plt.figure(figsize=(15, 7))\n",
    "figure.add_subplot(121)\n",
    "plt.title(\"Sync Times - Sitting\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Threshold\")\n",
    "plt.ylim(0, 1)\n",
    "plt.plot([syncresults[i][0] for i in range(len(x))],x, label=\"Sit Practice 1\")\n",
    "plt.plot([syncresults[i][1] for i in range(len(x))],x, label=\"Sit Practice 2\")\n",
    "plt.plot([syncresults[i][2] for i in range(len(x))],x, label=\"Sit Eval 1\")\n",
    "plt.plot([syncresults[i][3] for i in range(len(x))],x, label=\"Sit Eval 2\")\n",
    "plt.legend(loc=\"best\")\n",
    "figure.add_subplot(122)\n",
    "plt.title(\"Sync Times - Walking\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Threshold\")\n",
    "plt.ylim(0, 1)\n",
    "plt.plot([syncresults[i][4] for i in range(len(x))], x, label=\"Walk Practice 1\")\n",
    "plt.plot([syncresults[i][5] for i in range(len(x))], x, label=\"Walk Practice 2\")\n",
    "plt.plot([syncresults[i][6] for i in range(len(x))], x, label=\"Walk Eval 1\")\n",
    "plt.plot([syncresults[i][7] for i in range(len(x))], x, label=\"Walk Eval 2\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.savefig(\"synctimes.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this block iterates through the study data and calculates the data needed for the correlation over time graph\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "# baseFilePath = \"/Users/jwpilly/Documents/Research/Synchro/Study_v2/\"\n",
    "baseFilePath = \"/Users/jwpilly/Research/Synchro/Study_v2/\"\n",
    "# baseFilePath = \"/Volumes/HanSolo/Dropbox/Georgia Tech/Synchro/Data/Study_v1/Flat/\"\n",
    "# baseFilePath = \"/Volumes/HanSolo/Dropbox/Georgia Tech/Synchro/Data/Study_v2/\"\n",
    "presults = []\n",
    "for root, folders, files in os.walk(baseFilePath):\n",
    "    if not root.endswith(\"synchro\") or \"MACOSX\" in root:\n",
    "        continue\n",
    "    sfolders = sorted(folders, key=lambda x : int(x.split(\"_\")[0].replace(\"t\", \"\")))\n",
    "    efolders = [f for f in sfolders[:] if \"prep\" not in f]\n",
    "    efolderpaths = [root + \"/\" + f + \"/\" for f in efolders]\n",
    "#     print(efolderpaths)\n",
    "    efoldertimes = [(efolderpaths[i].split(\"_\")[-1][:-1] + str(i), correlateSyncFile(efolderpaths[i])) for i in range(len(efolderpaths))]\n",
    "#     print(efoldertimes)\n",
    "    presults.append(efoldertimes)\n",
    "presultsa = [[i[1] for i in t] for t in [sorted(r) for r in presults]]\n",
    "paverages = reduce(lambda x, y : np.array(x) + np.array(y), presultsa) / len(presultsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this block iterates through the study data and calculates the data needed for the correlation over time graph\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "# baseFilePath = \"/Users/jwpilly/Documents/Research/Synchro/Study_v2/\"\n",
    "# baseFilePath = \"/Users/jwpilly/Research/Synchro/Study_v2/\"\n",
    "IGNORE_TIME = 2000\n",
    "sync750 = \"/Volumes/HanSolo/Dropbox/Georgia Tech/Synchro/Data/Study_v1/Flat/sync750/\"\n",
    "sync1000 = \"/Volumes/HanSolo/Dropbox/Georgia Tech/Synchro/Data/Study_v1/Flat/sync1000/\"\n",
    "sync1250 = \"/Volumes/HanSolo/Dropbox/Georgia Tech/Synchro/Data/Study_v1/Flat/sync1250/\"\n",
    "noise750 = \"/Volumes/HanSolo/Dropbox/Georgia Tech/Synchro/Data/Study_v1/Flat/noise750/\"\n",
    "noise1000 = \"/Volumes/HanSolo/Dropbox/Georgia Tech/Synchro/Data/Study_v1/Flat/noise1000/\"\n",
    "noise1250 = \"/Volumes/HanSolo/Dropbox/Georgia Tech/Synchro/Data/Study_v1/Flat/noise1250/\"\n",
    "sit1000 = \"/Volumes/HanSolo/Dropbox/Georgia Tech/Synchro/Data/Study_v1/Flat/sync1000sitting/\"\n",
    "walk1000 = \"/Volumes/HanSolo/Dropbox/Georgia Tech/Synchro/Data/Study_v1/Flat/sync1000walking/\"\n",
    "browse1000 = \"/Volumes/HanSolo/Dropbox/Georgia Tech/Synchro/Data/Study_v1/Flat/sync1000browsing/\"\n",
    "# presults = []\n",
    "# for root, folders, files in os.walk(baseFilePath):\n",
    "#     if not root.endswith(\"synchro\") or \"MACOSX\" in root:\n",
    "#         continue\n",
    "#     sfolders = sorted(folders)    \n",
    "#     efolders = [f for f in sfolders[:] if \"prep\" not in f]\n",
    "#     efolderpaths = [root + \"/\" + f + \"/\" for f in efolders]\n",
    "# #     print(efolderpaths)\n",
    "#     efoldertimes = [(efolderpaths[i].split(\"_\")[-1][:-1] + str(i), correlateSyncFile(efolderpaths[i])) for i in range(len(efolderpaths))]\n",
    "# #     print(efoldertimes)\n",
    "#     presults.append(efoldertimes)\n",
    "#     print presults\n",
    "# presultsa = [[i[1] for i in t] for t in [sorted(r) for r in presults]]\n",
    "# print reduce(lambda x, y : np.array(x) + np.array(y), presultsa)\n",
    "# paverages = reduce(lambda x, y : np.array(x) + np.array(y), presultsa) / len(presultsa)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.ticker import MultipleLocator, FormatStrFormatter\n",
    "\n",
    "# majorLocator = MultipleLocator(1)\n",
    "# majorFormatter = FormatStrFormatter('%d')\n",
    "# minorLocator = MultipleLocator(5)\n",
    "\n",
    "\n",
    "# s750 = correlateSyncFile(sync750)\n",
    "s1000 = correlateSyncFile(sync1000)\n",
    "# s1250 = correlateSyncFile(sync1250)\n",
    "# n750 = correlateSyncFile(noise750)\n",
    "# n1000 = correlateSyncFile(noise1000)\n",
    "# n1250 = correlateSyncFile(noise1250)\n",
    "# figure, ax = plt.subplots()\n",
    "# (figsize=(22,15))\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# plt.plot(t, s)\n",
    "\n",
    "# ax.xaxis.set_major_locator(majorLocator)\n",
    "# ax.xaxis.set_major_formatter(majorFormatter)\n",
    "\n",
    "# for the minor ticks, use no labels; default NullFormatter\n",
    "# ax.xaxis.set_minor_locator(minorLocator)\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# loc = plticker.MultipleLocator(base=1.0) # this locator puts ticks at regular intervals\n",
    "# plt.x.set_major_locator(loc)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)                                                      \n",
    "\n",
    "# major ticks every 20, minor ticks every 5                                      \n",
    "major_ticks = np.arange(0, 20000, 1000)                                              \n",
    "# minor_ticks = np.arange(0, 20000, 500)                                               \n",
    "\n",
    "ax.set_xticks(major_ticks)                                                       \n",
    "# ax.set_xticks(minor_ticks, minor=True)                                           \n",
    "# ax.set_yticks(major_ticks)                                                       \n",
    "# ax.set_yticks(minor_ticks, minor=True)                                           \n",
    "\n",
    "# and a corresponding grid                                                       \n",
    "\n",
    "# ax.grid(which='both')                                                            \n",
    "\n",
    "# or if you want differnet settings for the grids:                               \n",
    "# ax.grid(which='minor', alpha=0.2)                                                \n",
    "# ax.grid(which='major', alpha=0.5) \n",
    "\n",
    "\n",
    "\n",
    "# plt.plot(s1250, '-r', label='Sync 0.8 Hz', linewidth=3)\n",
    "# plt.plot(s1000, '-b', label='Sync 1 Hz', linewidth=3)\n",
    "# plt.plot(s750, '-b', label='Sync 1.33 Hz', linewidth=3)\n",
    "# plt.plot(n1250, '--r', label='Noise 0.8 Hz', linewidth=3)\n",
    "plt.plot(n1000, '--g', label='Noise 1 Hz', linewidth=3)\n",
    "# plt.plot(n750, '--b', label='Noise 1.33 Hz', linewidth=3)\n",
    "legend = plt.legend(loc='lower right', shadow=False)\n",
    "\n",
    "# The frame is matplotlib.patches.Rectangle instance surrounding the legend.\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('0.90')\n",
    "\n",
    "# Set the fontsize\n",
    "for label in legend.get_texts():\n",
    "    label.set_fontsize('large')\n",
    "\n",
    "for label in legend.get_lines():\n",
    "    label.set_linewidth(3)  # the legend line width\n",
    "\n",
    "plt.ylim(0,1)\n",
    "plt.rcParams.update({'font.size': 9})\n",
    "plt.savefig(\"/Users/gareyes/Downloads/figure.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.ticker import MultipleLocator, FormatStrFormatter\n",
    "\n",
    "# majorLocator = MultipleLocator(1)\n",
    "# majorFormatter = FormatStrFormatter('%d')\n",
    "# minorLocator = MultipleLocator(5)\n",
    "\n",
    "\n",
    "# s750 = correlateSyncFile(sync750)\n",
    "s1000 = correlateSyncFile(sync1000)\n",
    "# s1250 = correlateSyncFile(sync1250)\n",
    "# n750 = correlateSyncFile(noise750)\n",
    "# n1000 = correlateSyncFile(noise1000)\n",
    "# n1250 = correlateSyncFile(noise1250)\n",
    "# figure, ax = plt.subplots()\n",
    "# (figsize=(22,15))\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# plt.plot(t, s)\n",
    "\n",
    "# ax.xaxis.set_major_locator(majorLocator)\n",
    "# ax.xaxis.set_major_formatter(majorFormatter)\n",
    "\n",
    "# for the minor ticks, use no labels; default NullFormatter\n",
    "# ax.xaxis.set_minor_locator(minorLocator)\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# loc = plticker.MultipleLocator(base=1.0) # this locator puts ticks at regular intervals\n",
    "# plt.x.set_major_locator(loc)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)                                                      \n",
    "\n",
    "# major ticks every 20, minor ticks every 5                                      \n",
    "# major_ticks = np.arange(0, 20000, 1000)                                              \n",
    "# minor_ticks = np.arange(0, 20000, 500)                                               \n",
    "\n",
    "# ax.set_xticks(major_ticks)                                                       \n",
    "# ax.set_xticks(minor_ticks, minor=True)                                           \n",
    "# ax.set_yticks(major_ticks)                                                       \n",
    "# ax.set_yticks(minor_ticks, minor=True)                                           \n",
    "\n",
    "# and a corresponding grid                                                       \n",
    "\n",
    "# ax.grid(which='both')                                                            \n",
    "\n",
    "# or if you want differnet settings for the grids:                               \n",
    "# ax.grid(which='minor', alpha=0.2)                                                \n",
    "# ax.grid(which='major', alpha=0.5) \n",
    "\n",
    "\n",
    "\n",
    "# plt.plot(s1250, '-r', label='Sync 0.8 Hz', linewidth=3)\n",
    "# plt.plot(s1000, '-b', label='Sync 1 Hz', linewidth=3)\n",
    "# plt.plot(s750, '-b', label='Sync 1.33 Hz', linewidth=3)\n",
    "plt.plot(n1000, '--g', label='Reference Signal', linewidth=3)\n",
    "plt.plot(n750, 'm', label='Feature Signal', linewidth=3)\n",
    "plt.plot(n1250, 'g', label='Adj Reference Signal', linewidth=3)\n",
    "# plt.plot(n750, '--b', label='Noise 1.33 Hz', linewidth=3)\n",
    "legend = plt.legend(loc='lower right', shadow=False)\n",
    "\n",
    "# The frame is matplotlib.patches.Rectangle instance surrounding the legend.\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('0.90')\n",
    "\n",
    "# Set the fontsize\n",
    "for label in legend.get_texts():\n",
    "    label.set_fontsize('large')\n",
    "\n",
    "for label in legend.get_lines():\n",
    "    label.set_linewidth(3)  # the legend line width\n",
    "\n",
    "plt.ylim(0,1)\n",
    "plt.rcParams.update({'font.size': 9})\n",
    "plt.savefig(\"/Users/gareyes/Downloads/figure.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sit1000 = correlateSyncFile(sit1000)\n",
    "walk1000 = correlateSyncFile(walk1000)\n",
    "browse1000 = correlateSyncFile(browse1000)\n",
    "plt.plot(browse1000, '-r')\n",
    "plt.plot(sit1000, '-g')\n",
    "plt.plot(walk1000, '-b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this block iterates through the study data and calculates the data needed for the magnitude over time graph\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "# baseFilePath = \"/Users/jwpilly/Documents/Research/Synchro/Study_v2/\"\n",
    "baseFilePath = \"/Users/jwpilly/Research/Synchro/Study_v2/\"\n",
    "presults = []\n",
    "for root, folders, files in os.walk(baseFilePath):\n",
    "    if not root.endswith(\"synchro\") or \"MACOSX\" in root:\n",
    "        continue\n",
    "    sfolders = sorted(folders, key=lambda x : int(x.split(\"_\")[0].replace(\"t\", \"\")))\n",
    "    efolders = [f for f in sfolders[:] if \"prep\" not in f]\n",
    "    efolderpaths = [root + \"/\" + f + \"/\" for f in efolders]\n",
    "#     print(efolderpaths)\n",
    "    efoldertimes = [(efolderpaths[i].split(\"_\")[-1][:-1] + str(i), magSyncFile(efolderpaths[i])) for i in range(len(efolderpaths))]\n",
    "#     print(efoldertimes)\n",
    "    presults.append(efoldertimes)\n",
    "presultsa = [[i[1] for i in t] for t in [sorted(r) for r in presults]]\n",
    "paverages = reduce(lambda x, y : np.array(x) + np.array(y), presultsa) / len(presultsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this block generates the graph for the magnitude over time data. \n",
    "\n",
    "%matplotlib inline\n",
    "plt.clf()\n",
    "x = corr_times / 1000\n",
    "figure = plt.figure(figsize=(15, 7))\n",
    "figure.add_subplot(121)\n",
    "plt.title(\"Magnitude Over Time - Sitting\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Magnitude\")\n",
    "# plt.ylim(0, 1)\n",
    "plt.plot(x, [paverages[0][i] for i in range(len(x))], label=\"Sit Practice 1\")\n",
    "plt.plot(x, [paverages[1][i] for i in range(len(x))], label=\"Sit Practice 2\")\n",
    "plt.plot(x, [paverages[2][i] for i in range(len(x))], label=\"Sit Eval 1\")\n",
    "plt.plot(x, [paverages[3][i] for i in range(len(x))], label=\"Sit Eval 2\")\n",
    "plt.legend(loc=\"best\")\n",
    "figure.add_subplot(122)\n",
    "plt.title(\"Magnitude Over Time - Walking\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Magnitude\")\n",
    "# plt.ylim(0, 1)\n",
    "plt.plot(x, [paverages[4][i] for i in range(len(x))], label=\"Walk Practice 1\")\n",
    "plt.plot(x, [paverages[5][i] for i in range(len(x))], label=\"Walk Practice 2\")\n",
    "plt.plot(x, [paverages[6][i] for i in range(len(x))], label=\"Walk Eval 1\")\n",
    "plt.plot(x, [paverages[7][i] for i in range(len(x))], label=\"Walk Eval 2\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.savefig(\"magnitude_over_time.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this block iterates through the study data and calculates the data needed for the magnitude over time graph but for noise\n",
    "\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "# baseFilePath = \"/Users/jwpilly/Documents/Research/Synchro/Study_v2/\"\n",
    "baseFilePath = \"/Users/jwpilly/Research/Synchro/Study_v2/\"\n",
    "presults = []\n",
    "for root, folders, files in os.walk(baseFilePath):\n",
    "    if not root.endswith(\"synchro\") or \"MACOSX\" in root:\n",
    "        continue\n",
    "    sfolders = sorted(folders, key=lambda x : int(x.split(\"_\")[0].replace(\"t\", \"\")))\n",
    "    efolders = [f for f in sfolders[:] if \"prep\" not in f]\n",
    "    efolderpaths = [root + \"/\" + f + \"/\" for f in efolders]\n",
    "#     print(efolderpaths)\n",
    "    efoldertimes = [(efolderpaths[i].split(\"_\")[-1][:-1] + str(i), magSyncFile(efolderpaths[i], True)) for i in range(len(efolderpaths))]\n",
    "#     print(efoldertimes)\n",
    "    presults.append(efoldertimes)\n",
    "presultsa = [[i[1] for i in t] for t in [sorted(r) for r in presults]]\n",
    "paverages = reduce(lambda x, y : np.array(x) + np.array(y), presultsa) / len(presultsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this block generates the graph for the magnitude over time data but for noise\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.clf()\n",
    "x = corr_times / 1000\n",
    "figure = plt.figure(figsize=(15, 7))\n",
    "figure.add_subplot(121)\n",
    "plt.title(\"Magnitude Over Time - Sitting\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Magnitude\")\n",
    "# plt.ylim(0, 1)\n",
    "plt.plot(x, [paverages[0][i] for i in range(len(x))], label=\"Sit Practice 1\")\n",
    "plt.plot(x, [paverages[1][i] for i in range(len(x))], label=\"Sit Practice 2\")\n",
    "plt.plot(x, [paverages[2][i] for i in range(len(x))], label=\"Sit Eval 1\")\n",
    "plt.plot(x, [paverages[3][i] for i in range(len(x))], label=\"Sit Eval 2\")\n",
    "plt.legend(loc=\"best\")\n",
    "figure.add_subplot(122)\n",
    "plt.title(\"Magnitude Over Time - Walking\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Magnitude\")\n",
    "# plt.ylim(0, 1)\n",
    "plt.plot(x, [paverages[4][i] for i in range(len(x))], label=\"Walk Practice 1\")\n",
    "plt.plot(x, [paverages[5][i] for i in range(len(x))], label=\"Walk Practice 2\")\n",
    "plt.plot(x, [paverages[6][i] for i in range(len(x))], label=\"Walk Eval 1\")\n",
    "plt.plot(x, [paverages[7][i] for i in range(len(x))], label=\"Walk Eval 2\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.savefig(\"magnitude_over_time_noise.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "magSyncFile(\"/Users/jwpilly/Research/Synchro/Study_v2/P1/synchro/t20170401112834_p1_sit/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this block iterates through the study data and calculates the data needed for the correlation over time graph\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "# baseFilePath = \"/Users/jwpilly/Documents/Research/Synchro/Study_v2/\"\n",
    "baseFilePath = \"/Users/jwpilly/Research/Synchro/Study_v2/\"\n",
    "# baseFilePath = \"/Volumes/HanSolo/Dropbox/Georgia Tech/Synchro/Data/Study_v1/Flat/\"\n",
    "presults = []\n",
    "for root, folders, files in os.walk(baseFilePath):\n",
    "    if not root.endswith(\"synchro\") or \"MACOSX\" in root:\n",
    "        continue\n",
    "    sfolders = sorted(folders, key=lambda x : int(x.split(\"_\")[0].replace(\"t\", \"\")))\n",
    "    efolders = [f for f in sfolders[:] if \"prep\" not in f]\n",
    "    efolderpaths = [root + \"/\" + f + \"/\" for f in efolders]\n",
    "#     print(efolderpaths)\n",
    "    efoldertimes = [(efolderpaths[i].split(\"_\")[-1][:-1] + str(i), correlateSyncFile(efolderpaths[i])) for i in range(len(efolderpaths))]\n",
    "#     print(efoldertimes)\n",
    "    presults.append(efoldertimes)\n",
    "presultsa = [[i[1] for i in t] for t in [sorted(r) for r in presults]]\n",
    "paverages = reduce(lambda x, y : np.array(x) + np.array(y), presultsa) / len(presultsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this block iterates through the study data and calculates the data needed for the correlation over time graph but for noise\n",
    "\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "# baseFilePath = \"/Users/jwpilly/Documents/Research/Synchro/Study_v2/\"\n",
    "baseFilePath = \"/Users/jwpilly/Research/Synchro/Study_v2/\"\n",
    "presults = []\n",
    "for root, folders, files in os.walk(baseFilePath):\n",
    "    if not root.endswith(\"synchro\") or \"MACOSX\" in root:\n",
    "        continue\n",
    "    sfolders = sorted(folders, key=lambda x : int(x.split(\"_\")[0].replace(\"t\", \"\")))\n",
    "    efolders = [f for f in sfolders[:] if \"prep\" not in f]\n",
    "    efolderpaths = [root + \"/\" + f + \"/\" for f in efolders]\n",
    "#     print(efolderpaths)\n",
    "    efoldertimes = [(efolderpaths[i].split(\"_\")[-1][:-1] + str(i), correlateSyncFile(efolderpaths[i], True)) for i in range(len(efolderpaths))]\n",
    "#     print(efoldertimes)\n",
    "    presults.append(efoldertimes)\n",
    "presultsa = [[i[1] for i in t] for t in [sorted(r) for r in presults]]\n",
    "paverages = reduce(lambda x, y : np.array(x) + np.array(y), presultsa) / len(presultsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this block generates the graph for the correlation over time graphs depending on which block was run before this, it is for sync/noise\n",
    "\n",
    "%matplotlib inline\n",
    "plt.clf()\n",
    "x = corr_times / 1000\n",
    "figure = plt.figure(figsize=(15, 7))\n",
    "figure.add_subplot(121)\n",
    "plt.title(\"Correlation Over Time - Sitting\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Correlation\")\n",
    "plt.ylim(0, 1)\n",
    "plt.plot(x, [paverages[0][i] for i in range(len(x))], label=\"Sit Practice 1\")\n",
    "plt.plot(x, [paverages[1][i] for i in range(len(x))], label=\"Sit Practice 2\")\n",
    "plt.plot(x, [paverages[2][i] for i in range(len(x))], label=\"Sit Eval 1\")\n",
    "plt.plot(x, [paverages[3][i] for i in range(len(x))], label=\"Sit Eval 2\")\n",
    "plt.legend(loc=\"best\")\n",
    "figure.add_subplot(122)\n",
    "plt.title(\"Correlation Over Time - Walking\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Correlation\")\n",
    "plt.ylim(0, 1)\n",
    "plt.plot(x, [paverages[4][i] for i in range(len(x))], label=\"Walk Practice 1\")\n",
    "plt.plot(x, [paverages[5][i] for i in range(len(x))], label=\"Walk Practice 2\")\n",
    "plt.plot(x, [paverages[6][i] for i in range(len(x))], label=\"Walk Eval 1\")\n",
    "plt.plot(x, [paverages[7][i] for i in range(len(x))], label=\"Walk Eval 2\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.savefig(\"correlation_over_time.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this block iterates through the study data and calculates the data needed for the correlation over time graph with accel\n",
    "\n",
    "\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "# baseFilePath = \"/Users/jwpilly/Documents/Research/Synchro/Study_v2/\"\n",
    "# baseFilePath = \"/Users/jwpilly/Research/Synchro/Study_v2/\"\n",
    "baseFilePath = \"/media/jwpilly/PillowDisk/Research/Synchro/figuredata/figure12/Study_v2/\"\n",
    "presults = []\n",
    "for root, folders, files in os.walk(baseFilePath):\n",
    "    if not root.endswith(\"synchro\") or \"MACOSX\" in root:\n",
    "        continue\n",
    "    sfolders = sorted(folders, key=lambda x : int(x.split(\"_\")[0].replace(\"t\", \"\")))\n",
    "    efolders = [f for f in sfolders[:] if \"prep\" not in f]\n",
    "    efolderpaths = [root + \"/\" + f + \"/\" for f in efolders]\n",
    "#     print(efolderpaths)\n",
    "    efoldertimes = [(efolderpaths[i].split(\"_\")[-1][:-1] + str(i), correlateSyncAccelFile(efolderpaths[i])) for i in range(len(efolderpaths))]\n",
    "#     print(efoldertimes)\n",
    "    presults.append(efoldertimes)\n",
    "presultsa = [[i[1] for i in t] for t in [sorted(r) for r in presults]]\n",
    "paverages = reduce(lambda x, y : np.array(x) + np.array(y), presultsa) / len(presultsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this block iterates through the study data and calculates the data needed for the correlation over time graph with accel but for noise\n",
    "\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "# baseFilePath = \"/Users/jwpilly/Documents/Research/Synchro/Study_v2/\"\n",
    "baseFilePath = \"/Users/jwpilly/Research/Synchro/Study_v2/\"\n",
    "presults = []\n",
    "for root, folders, files in os.walk(baseFilePath):\n",
    "    if not root.endswith(\"synchro\") or \"MACOSX\" in root:\n",
    "        continue\n",
    "    sfolders = sorted(folders, key=lambda x : int(x.split(\"_\")[0].replace(\"t\", \"\")))\n",
    "    efolders = [f for f in sfolders[:] if \"prep\" not in f]\n",
    "    efolderpaths = [root + \"/\" + f + \"/\" for f in efolders]\n",
    "#     print(efolderpaths)\n",
    "    efoldertimes = [(efolderpaths[i].split(\"_\")[-1][:-1] + str(i), correlateSyncAccelFile(efolderpaths[i], True)) for i in range(len(efolderpaths))]\n",
    "#     print(efoldertimes)\n",
    "    presults.append(efoldertimes)\n",
    "presultsa = [[i[1] for i in t] for t in [sorted(r) for r in presults]]\n",
    "paverages = reduce(lambda x, y : np.array(x) + np.array(y), presultsa) / len(presultsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this block generates the graph for the correlation over time graphs with accel. depending on which block was run before this, it is for sync/noise\n",
    "\n",
    "%matplotlib inline\n",
    "plt.clf()\n",
    "plt.figure(figsize=(8,7))\n",
    "x = corr_times / 1000\n",
    "# figure = plt.figure(figsize=(15, 7))\n",
    "# figure.add_subplot(121)\n",
    "plt.title(\"Correlation Over Time - Sitting\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Correlation\")\n",
    "plt.ylim(-1, 1)\n",
    "plt.plot(x, [paverages[0][0][i] for i in range(len(x))], label=\"Sit Practice 1\")\n",
    "plt.plot(x, [paverages[1][0][i] for i in range(len(x))], label=\"Sit Practice 2\")\n",
    "plt.plot(x, [paverages[2][0][i] for i in range(len(x))], label=\"Sit Eval 1\")\n",
    "plt.plot(x, [paverages[3][0][i] for i in range(len(x))], label=\"Sit Eval 2\")\n",
    "plt.plot(x, np.mean([paverages[pi][1][0] for pi in range(len(paverages) // 2)], axis=0) / np.ptp([paverages[pi][1][0] for pi in range(len(paverages))]) / 2, label=\"Accel x\")\n",
    "plt.plot(x, np.mean([paverages[pi][1][1] for pi in range(len(paverages) // 2)], axis=0) / np.ptp([paverages[pi][1][1] for pi in range(len(paverages))]) / 2, label=\"Accel y\")\n",
    "plt.plot(x, np.mean([paverages[pi][1][2] for pi in range(len(paverages) // 2)], axis=0) / np.ptp([paverages[pi][1][2] for pi in range(len(paverages))]) / 2, label=\"Accel z\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.savefig(\"correlation_over_time_accel_1.png\", bbox_inches=\"tight\")\n",
    "# figure.add_subplot(122)\n",
    "plt.clf()\n",
    "plt.figure(figsize=(8,7))\n",
    "plt.title(\"Correlation Over Time - Walking\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Correlation\")\n",
    "plt.ylim(-1, 1)\n",
    "plt.plot(x, [paverages[4][0][i] for i in range(len(x))], label=\"Walk Practice 1\")\n",
    "plt.plot(x, [paverages[5][0][i] for i in range(len(x))], label=\"Walk Practice 2\")\n",
    "plt.plot(x, [paverages[6][0][i] for i in range(len(x))], label=\"Walk Eval 1\")\n",
    "plt.plot(x, [paverages[7][0][i] for i in range(len(x))], label=\"Walk Eval 2\")\n",
    "plt.plot(x, np.mean([paverages[pi][1][0] for pi in range(len(paverages) // 2 + 4)], axis=0) / np.ptp([paverages[pi][1][0] for pi in range(len(paverages))]) / 2, label=\"Accel x\")\n",
    "plt.plot(x, np.mean([paverages[pi][1][1] for pi in range(len(paverages) // 2 + 4)], axis=0) / np.ptp([paverages[pi][1][1] for pi in range(len(paverages))]) / 2, label=\"Accel y\")\n",
    "plt.plot(x, np.mean([paverages[pi][1][2] for pi in range(len(paverages) // 2 + 4)], axis=0) / np.ptp([paverages[pi][1][2] for pi in range(len(paverages))]) / 2, label=\"Accel z\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.savefig(\"correlation_over_time_accel_2.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this block iterates through the study data and calculates the data needed for the correlation over time graph with accel\n",
    "\n",
    "\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "# baseFilePath = \"/Users/jwpilly/Documents/Research/Synchro/Study_v2/\"\n",
    "baseFilePath = \"/Users/jwpilly/Research/Synchro/Study_v2/\"\n",
    "presults = []\n",
    "for root, folders, files in os.walk(baseFilePath):\n",
    "    if not root.endswith(\"synchro\") or \"MACOSX\" in root:\n",
    "        continue\n",
    "    sfolders = sorted(folders, key=lambda x : int(x.split(\"_\")[0].replace(\"t\", \"\")))\n",
    "    efolders = [f for f in sfolders[:] if \"prep\" not in f]\n",
    "    efolderpaths = [root + \"/\" + f + \"/\" for f in efolders]\n",
    "#     print(efolderpaths)\n",
    "    efoldertimes = [(efolderpaths[i].split(\"_\")[-1][:-1] + str(i), correlateSyncDeltaFile(efolderpaths[i], True)) for i in range(len(efolderpaths))]\n",
    "#     print(efoldertimes)\n",
    "    presults.append(efoldertimes)\n",
    "presultsa = [[i[1] for i in t] for t in [sorted(r) for r in presults]]\n",
    "paverages = reduce(lambda x, y : np.array(x) + np.array(y), presultsa) / len(presultsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this block generates the graph for the correlation over time graphs with accel. depending on which block was run before this, it is for sync/noise\n",
    "\n",
    "%matplotlib inline\n",
    "plt.clf()\n",
    "plt.figure(figsize=(8,7))\n",
    "x = corr_times / 1000\n",
    "# figure = plt.figure(figsize=(15, 7))\n",
    "# figure.add_subplot(121)\n",
    "plt.title(\"Correlation Over Time - Sitting\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Correlation\")\n",
    "# plt.ylim(-1, 1)\n",
    "plt.plot(x, [paverages[0][0][i] for i in range(len(x))], label=\"Sit Practice 1\")\n",
    "plt.plot(x, [paverages[1][0][i] for i in range(len(x))], label=\"Sit Practice 2\")\n",
    "plt.plot(x, [paverages[2][0][i] for i in range(len(x))], label=\"Sit Eval 1\")\n",
    "plt.plot(x, [paverages[3][0][i] for i in range(len(x))], label=\"Sit Eval 2\")\n",
    "plt.plot(x, np.mean([paverages[pi][1] for pi in range(len(paverages) // 2)], axis=0), label=\"Deltas\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.savefig(\"correlation_over_time_delta_1.png\", bbox_inches=\"tight\")\n",
    "# figure.add_subplot(122)\n",
    "plt.clf()\n",
    "plt.figure(figsize=(8,7))\n",
    "plt.title(\"Correlation Over Time - Walking\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Correlation\")\n",
    "# plt.ylim(-1, 1)\n",
    "plt.plot(x, [paverages[4][0][i] for i in range(len(x))], label=\"Walk Practice 1\")\n",
    "plt.plot(x, [paverages[5][0][i] for i in range(len(x))], label=\"Walk Practice 2\")\n",
    "plt.plot(x, [paverages[6][0][i] for i in range(len(x))], label=\"Walk Eval 1\")\n",
    "plt.plot(x, [paverages[7][0][i] for i in range(len(x))], label=\"Walk Eval 2\")\n",
    "plt.plot(x, np.mean([paverages[pi][1] for pi in range(len(paverages) // 2 + 4)], axis=0), label=\"Deltas\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.savefig(\"correlation_over_time_delta_2.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this block iterates through the study data and calculates the data needed for the correlation over time graph with accel\n",
    "\n",
    "\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "# baseFilePath = \"/Users/jwpilly/Documents/Research/Synchro/Study_v2/\"\n",
    "baseFilePath = \"/Users/jwpilly/Research/Synchro/Study_v2/\"\n",
    "presults = []\n",
    "for root, folders, files in os.walk(baseFilePath):\n",
    "    if not root.endswith(\"synchro\") or \"MACOSX\" in root:\n",
    "        continue\n",
    "    sfolders = sorted(folders, key=lambda x : int(x.split(\"_\")[0].replace(\"t\", \"\")))\n",
    "    efolders = [f for f in sfolders[:] if \"prep\" not in f]\n",
    "    efolderpaths = [root + \"/\" + f + \"/\" for f in efolders]\n",
    "#     print(efolderpaths)\n",
    "    efoldertimes = [(efolderpaths[i].split(\"_\")[-1][:-1] + str(i), correlateSyncMagFile(efolderpaths[i], True)) for i in range(len(efolderpaths))]\n",
    "#     print(efoldertimes)\n",
    "    presults.append(efoldertimes)\n",
    "presultsa = [[i[1] for i in t] for t in [sorted(r) for r in presults]]\n",
    "paverages = reduce(lambda x, y : np.array(x) + np.array(y), presultsa) / len(presultsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this block generates the graph for the correlation over time graphs with accel. depending on which block was run before this, it is for sync/noise\n",
    "\n",
    "%matplotlib inline\n",
    "plt.clf()\n",
    "plt.figure(figsize=(8,7))\n",
    "x = corr_times / 1000\n",
    "# figure = plt.figure(figsize=(15, 7))\n",
    "# figure.add_subplot(121)\n",
    "plt.title(\"Correlation Over Time - Sitting\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Correlation\")\n",
    "# plt.ylim(-1, 1)\n",
    "plt.plot(x, [paverages[0][0][i] for i in range(len(x))], label=\"Sit Practice 1\")\n",
    "plt.plot(x, [paverages[1][0][i] for i in range(len(x))], label=\"Sit Practice 2\")\n",
    "plt.plot(x, [paverages[2][0][i] for i in range(len(x))], label=\"Sit Eval 1\")\n",
    "plt.plot(x, [paverages[3][0][i] for i in range(len(x))], label=\"Sit Eval 2\")\n",
    "plt.plot(x, np.mean([paverages[pi][1] for pi in range(len(paverages) // 2)], axis=0), label=\"Mags\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.savefig(\"correlation_over_time_mag_1.png\", bbox_inches=\"tight\")\n",
    "# figure.add_subplot(122)\n",
    "plt.clf()\n",
    "plt.figure(figsize=(8,7))\n",
    "plt.title(\"Correlation Over Time - Walking\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Correlation\")\n",
    "# plt.ylim(-1, 1)\n",
    "plt.plot(x, [paverages[4][0][i] for i in range(len(x))], label=\"Walk Practice 1\")\n",
    "plt.plot(x, [paverages[5][0][i] for i in range(len(x))], label=\"Walk Practice 2\")\n",
    "plt.plot(x, [paverages[6][0][i] for i in range(len(x))], label=\"Walk Eval 1\")\n",
    "plt.plot(x, [paverages[7][0][i] for i in range(len(x))], label=\"Walk Eval 2\")\n",
    "plt.plot(x, np.mean([paverages[pi][1] for pi in range(len(paverages) // 2 + 4)], axis=0), label=\"Mags\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.savefig(\"correlation_over_time_mag_2.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this block generates the graph for the correlation over time graphs with accel. depending on which block was run before this, it is for sync/noise\n",
    "\n",
    "%matplotlib inline\n",
    "plt.clf()\n",
    "plt.figure(figsize=(8,7))\n",
    "x = corr_times / 1000\n",
    "# figure = plt.figure(figsize=(15, 7))\n",
    "# figure.add_subplot(121)\n",
    "plt.title(\"Correlation Over Time - Sitting\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Correlation\")\n",
    "# plt.ylim(-1, 1)\n",
    "plt.plot(x, [paverages[0][0][i] for i in range(len(x))], label=\"Sit Practice 1\")\n",
    "plt.plot(x, [paverages[1][0][i] for i in range(len(x))], label=\"Sit Practice 2\")\n",
    "plt.plot(x, [paverages[2][0][i] for i in range(len(x))], label=\"Sit Eval 1\")\n",
    "plt.plot(x, [paverages[3][0][i] for i in range(len(x))], label=\"Sit Eval 2\")\n",
    "plt.plot(x, np.mean([paverages[pi][1][0] for pi in range(len(paverages) // 2)], axis=0), label=\"Accel x\")\n",
    "plt.plot(x, np.mean([paverages[pi][1][1] for pi in range(len(paverages) // 2)], axis=0), label=\"Accel y\")\n",
    "plt.plot(x, np.mean([paverages[pi][1][2] for pi in range(len(paverages) // 2)], axis=0), label=\"Accel z\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.savefig(\"correlation_over_time_accel_1.png\", bbox_inches=\"tight\")\n",
    "# figure.add_subplot(122)\n",
    "plt.clf()\n",
    "plt.figure(figsize=(8,7))\n",
    "plt.title(\"Correlation Over Time - Walking\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Correlation\")\n",
    "# plt.ylim(-1, 1)\n",
    "plt.plot(x, [paverages[4][0][i] for i in range(len(x))], label=\"Walk Practice 1\")\n",
    "plt.plot(x, [paverages[5][0][i] for i in range(len(x))], label=\"Walk Practice 2\")\n",
    "plt.plot(x, [paverages[6][0][i] for i in range(len(x))], label=\"Walk Eval 1\")\n",
    "plt.plot(x, [paverages[7][0][i] for i in range(len(x))], label=\"Walk Eval 2\")\n",
    "plt.plot(x, np.mean([paverages[pi][1][0] for pi in range(len(paverages) // 2 + 4)], axis=0), label=\"Accel x\")\n",
    "plt.plot(x, np.mean([paverages[pi][1][1] for pi in range(len(paverages) // 2 + 4)], axis=0), label=\"Accel y\")\n",
    "plt.plot(x, np.mean([paverages[pi][1][2] for pi in range(len(paverages) // 2 + 4)], axis=0), label=\"Accel z\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.savefig(\"correlation_over_time_accel_2.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "# baseFilePath = \"/Users/jwpilly/Documents/Research/Synchro/Study_v2/\"\n",
    "baseFilePath = \"/Users/jwpilly/Research/Synchro/Study_v2/\"\n",
    "presults = []\n",
    "presultsn = []\n",
    "for root, folders, files in os.walk(baseFilePath):\n",
    "    if not root.endswith(\"synchro\") or \"MACOSX\" in root:\n",
    "        continue\n",
    "    sfolders = sorted(folders, key=lambda x : int(x.split(\"_\")[0].replace(\"t\", \"\")))\n",
    "    efolders = [f for f in sfolders[:] if \"prep\" not in f]\n",
    "    efolderpaths = [root + \"/\" + f + \"/\" for f in efolders]\n",
    "#     print(efolderpaths)\n",
    "    efoldertimes = [(efolderpaths[i].split(\"_\")[-1][:-1] + str(i), generateMagDist(efolderpaths[i])) for i in range(len(efolderpaths))]\n",
    "#     print(efoldertimes)\n",
    "    presults.append(efoldertimes)\n",
    "    efoldertimesn = [(efolderpaths[i].split(\"_\")[-1][:-1] + str(i), generateMagDist(efolderpaths[i], True)) for i in range(len(efolderpaths))]\n",
    "    presultsn.append(efoldertimesn)\n",
    "presultsa = [[i[1] for i in t] for t in [sorted(r) for r in presults]]\n",
    "presultsaname = [[i[0] for i in t] for t in [sorted(r) for r in presults]]\n",
    "pmags_sit = reduce(lambda x, y : x + y, [[presultsa[i][li] for li in range(len(presultsa[i])) if \"sit\" in presultsaname[i][li]] for i in range(len(presults))])\n",
    "pmags_walk = reduce(lambda x, y : x + y, [[presultsa[i][li] for li in range(len(presultsa[i])) if \"walk\" in presultsaname[i][li]] for i in range(len(presults))])\n",
    "\n",
    "\n",
    "\n",
    "presultsan = [[i[1] for i in t] for t in [sorted(r) for r in presultsn]]\n",
    "presultsanname = [[i[0] for i in t] for t in [sorted(r) for r in presultsn]]\n",
    "pmags_sitn = reduce(lambda x, y : x + y, [[presultsan[i][li] for li in range(len(presultsan[i])) if \"sit\" in presultsanname[i][li]] for i in range(len(presultsn))])\n",
    "pmags_walkn = reduce(lambda x, y : x + y, [[presultsan[i][li] for li in range(len(presultsan[i])) if \"walk\" in presultsanname[i][li]] for i in range(len(presultsn))])\n",
    "\n",
    "# Calculate overlap between the two KDEs.\n",
    "def y_pts(ker_a, ker_b, pt):\n",
    "    y_pt = min(ker_a(pt), ker_b(pt))\n",
    "    return y_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(presultsaname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "sm = stats.gaussian_kde(flatten(pmags_sit))\n",
    "smn = stats.gaussian_kde(flatten(pmags_sitn))\n",
    "wm = stats.gaussian_kde(flatten(pmags_walk))\n",
    "wmn = stats.gaussian_kde(flatten(pmags_walkn))\n",
    "\n",
    "xs = np.linspace(-20, 50, 201)\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(xs, sm(xs))\n",
    "plt.plot(xs, smn(xs))\n",
    "plt.show()\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(xs, wm(xs))\n",
    "plt.plot(xs, wmn(xs))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "# baseFilePath = \"/Users/jwpilly/Documents/Research/Synchro/Study_v2/\"\n",
    "baseFilePath = \"/Users/jwpilly/Research/Synchro/Study_v2/\"\n",
    "presults = []\n",
    "presultsn = []\n",
    "for root, folders, files in os.walk(baseFilePath):\n",
    "    if not root.endswith(\"synchro\") or \"MACOSX\" in root:\n",
    "        continue\n",
    "    sfolders = sorted(folders, key=lambda x : int(x.split(\"_\")[0].replace(\"t\", \"\")))\n",
    "    efolders = [f for f in sfolders[:] if \"prep\" not in f]\n",
    "    efolderpaths = [root + \"/\" + f + \"/\" for f in efolders]\n",
    "#     print(efolderpaths)\n",
    "    efoldertimes = [(efolderpaths[i].split(\"_\")[-1][:-1] + str(i), generateDeltaDist(efolderpaths[i])) for i in range(len(efolderpaths))]\n",
    "#     print(efoldertimes)\n",
    "    presults.append(efoldertimes)\n",
    "    efoldertimesn = [(efolderpaths[i].split(\"_\")[-1][:-1] + str(i), generateDeltaDist(efolderpaths[i], True)) for i in range(len(efolderpaths))]\n",
    "    presultsn.append(efoldertimesn)\n",
    "presultsa = [[i[1] for i in t] for t in [sorted(r) for r in presults]]\n",
    "presultsaname = [[i[0] for i in t] for t in [sorted(r) for r in presults]]\n",
    "pmags_sit = reduce(lambda x, y : x + y, [[presultsa[i][li] for li in range(len(presultsa[i])) if \"sit\" in presultsaname[i][li]] for i in range(len(presults))])\n",
    "pmags_walk = reduce(lambda x, y : x + y, [[presultsa[i][li] for li in range(len(presultsa[i])) if \"walk\" in presultsaname[i][li]] for i in range(len(presults))])\n",
    "\n",
    "\n",
    "\n",
    "presultsan = [[i[1] for i in t] for t in [sorted(r) for r in presultsn]]\n",
    "presultsanname = [[i[0] for i in t] for t in [sorted(r) for r in presultsn]]\n",
    "pmags_sitn = reduce(lambda x, y : x + y, [[presultsan[i][li] for li in range(len(presultsan[i])) if \"sit\" in presultsanname[i][li]] for i in range(len(presultsn))])\n",
    "pmags_walkn = reduce(lambda x, y : x + y, [[presultsan[i][li] for li in range(len(presultsan[i])) if \"walk\" in presultsanname[i][li]] for i in range(len(presultsn))])\n",
    "\n",
    "# Calculate overlap between the two KDEs.\n",
    "def y_pts(ker_a, ker_b, pt):\n",
    "    y_pt = min(ker_a(pt), ker_b(pt))\n",
    "    return y_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "sm = stats.gaussian_kde(flatten(pmags_sit))\n",
    "smn = stats.gaussian_kde(flatten(pmags_sitn))\n",
    "wm = stats.gaussian_kde(flatten(pmags_walk))\n",
    "wmn = stats.gaussian_kde(flatten(pmags_walkn))\n",
    "\n",
    "xs = np.linspace(-20, 50, 201)\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(xs, sm(xs))\n",
    "plt.plot(xs, smn(xs))\n",
    "plt.show()\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(xs, wm(xs))\n",
    "plt.plot(xs, wmn(xs))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "# baseFilePath = \"/Users/jwpilly/Documents/Research/Synchro/Study_v2/\"\n",
    "baseFilePath = \"/Users/jwpilly/Research/Synchro/Study_v2/\"\n",
    "presults = []\n",
    "presultsn = []\n",
    "for root, folders, files in os.walk(baseFilePath):\n",
    "    if not root.endswith(\"synchro\") or \"MACOSX\" in root:\n",
    "        continue\n",
    "    sfolders = sorted(folders, key=lambda x : int(x.split(\"_\")[0].replace(\"t\", \"\")))\n",
    "    efolders = [f for f in sfolders[:] if \"prep\" not in f]\n",
    "    efolderpaths = [root + \"/\" + f + \"/\" for f in efolders]\n",
    "#     print(efolderpaths)\n",
    "    efoldertimes = [(efolderpaths[i].split(\"_\")[-1][:-1] + str(i), generateAccelDist(efolderpaths[i])) for i in range(len(efolderpaths))]\n",
    "#     print(efoldertimes)\n",
    "    presults.append(efoldertimes)\n",
    "    efoldertimesn = [(efolderpaths[i].split(\"_\")[-1][:-1] + str(i), generateAccelDist(efolderpaths[i], True)) for i in range(len(efolderpaths))]\n",
    "    presultsn.append(efoldertimesn)\n",
    "presultsa = [[i[1] for i in t] for t in [sorted(r) for r in presults]]\n",
    "presultsaname = [[i[0] for i in t] for t in [sorted(r) for r in presults]]\n",
    "paccels_sit = reduce(lambda x, y : [x[d] + y[d] for d in range(3)], [[presultsa[i][li] for li in range(len(presultsa[i])) if \"sit\" in presultsaname[i][li]] for i in range(len(presults))])\n",
    "paccels_walk = reduce(lambda x, y : [x[d] + y[d] for d in range(3)], [[presultsa[i][li] for li in range(len(presultsa[i])) if \"walk\" in presultsaname[i][li]] for i in range(len(presults))])\n",
    "\n",
    "\n",
    "\n",
    "presultsan = [[i[1] for i in t] for t in [sorted(r) for r in presultsn]]\n",
    "presultsanname = [[i[0] for i in t] for t in [sorted(r) for r in presultsn]]\n",
    "paccels_sitn = reduce(lambda x, y : [x[d] + y[d] for d in range(3)], [[presultsan[i][li] for li in range(len(presultsan[i])) if \"sit\" in presultsanname[i][li]] for i in range(len(presultsn))])\n",
    "paccels_walkn = reduce(lambda x, y : [x[d] + y[d] for d in range(3)], [[presultsan[i][li] for li in range(len(presultsan[i])) if \"walk\" in presultsanname[i][li]] for i in range(len(presultsn))])\n",
    "\n",
    "# Calculate overlap between the two KDEs.\n",
    "def y_pts(ker_a, ker_b, pt):\n",
    "    y_pt = min(ker_a(pt), ker_b(pt))\n",
    "    return y_pt\n",
    "# Store overlap value.\n",
    "# overlap = quad(lambda x : y_pts(gd[1], gdn[1], x), -np.inf, np.inf)\n",
    "# print(overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "presults[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[[i[0] for i in t] for t in [sorted(r) for r in presults]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sx = np.mean(np.array((flatten(paccels_sit[0]))))\n",
    "sy = np.mean(np.array((flatten(paccels_sit[1]))))\n",
    "sz = np.mean(np.array((flatten(paccels_sit[2]))))\n",
    "\n",
    "wx = np.mean(np.array((flatten(paccels_walk[0]))))\n",
    "wy = np.mean(np.array((flatten(paccels_walk[1]))))\n",
    "wz = np.mean(np.array((flatten(paccels_walk[2]))))\n",
    "\n",
    "sxn = np.mean(np.array((flatten(paccels_sitn[0]))))\n",
    "syn = np.mean(np.array((flatten(paccels_sitn[1]))))\n",
    "szn = np.mean(np.array((flatten(paccels_sitn[2]))))\n",
    "\n",
    "wxn = np.mean(np.array((flatten(paccels_walkn[0]))))\n",
    "wyn = np.mean(np.array((flatten(paccels_walkn[1]))))\n",
    "wzn = np.mean(np.array((flatten(paccels_walkn[2]))))\n",
    "              \n",
    "print(sx, \",\", sy, \",\", sz)\n",
    "print(wx, \",\", wy, \",\", wz)\n",
    "\n",
    "print(sxn, \",\", syn, \",\", szn)\n",
    "print(wxn, \",\", wyn, \",\", wzn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "sx = stats.gaussian_kde(flatten(paccels_sit[0]))\n",
    "sy = stats.gaussian_kde(flatten(paccels_sit[1]))\n",
    "sz = stats.gaussian_kde(flatten(paccels_sit[2]))\n",
    "\n",
    "wx = stats.gaussian_kde(flatten(paccels_walk[0]))\n",
    "wy = stats.gaussian_kde(flatten(paccels_walk[1]))\n",
    "wz = stats.gaussian_kde(flatten(paccels_walk[2]))\n",
    "\n",
    "sxn = stats.gaussian_kde(flatten(paccels_sitn[0]))\n",
    "syn = stats.gaussian_kde(flatten(paccels_sitn[1]))\n",
    "szn = stats.gaussian_kde(flatten(paccels_sitn[2]))\n",
    "\n",
    "wxn = stats.gaussian_kde(flatten(paccels_walkn[0]))\n",
    "wyn = stats.gaussian_kde(flatten(paccels_walkn[1]))\n",
    "wzn = stats.gaussian_kde(flatten(paccels_walkn[2]))\n",
    "\n",
    "xs = np.linspace(-20, 20, 201)\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(xs, sx(xs))\n",
    "plt.plot(xs, sxn(xs))\n",
    "plt.show()\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(xs, sy(xs))\n",
    "plt.plot(xs, syn(xs))\n",
    "plt.show()\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(xs, sz(xs))\n",
    "plt.plot(xs, szn(xs))\n",
    "plt.show()\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(xs, wx(xs))\n",
    "plt.plot(xs, wxn(xs))\n",
    "plt.show()\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(xs, wy(xs))\n",
    "plt.plot(xs, wyn(xs))\n",
    "plt.show()\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(xs, wz(xs))\n",
    "plt.plot(xs, wzn(xs))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graphPath = \"/Users/jwpilly/Research/Synchro/Study_v2/P6/synchro/t20170403152443_p6_walk/\"\n",
    "\n",
    "ssg = generateAccelDist(graphPath)\n",
    "ssgn = generateAccelDist(graphPath, True)\n",
    "ssx = stats.gaussian_kde(ssg[0])\n",
    "ssy = stats.gaussian_kde(ssg[1])\n",
    "ssz = stats.gaussian_kde(ssg[2])\n",
    "ssxn = stats.gaussian_kde(ssgn[0])\n",
    "ssyn = stats.gaussian_kde(ssgn[1])\n",
    "sszn = stats.gaussian_kde(ssgn[2])\n",
    "plt.clf()\n",
    "plt.plot(xs, ssx(xs), color=\"r\")\n",
    "plt.plot(xs, ssxn(xs), color=\"g\")\n",
    "plt.show()\n",
    "plt.clf()\n",
    "plt.plot(xs, ssy(xs), color=\"r\")\n",
    "plt.plot(xs, ssyn(xs), color=\"g\")\n",
    "plt.show()\n",
    "plt.clf()\n",
    "plt.plot(xs, ssz(xs), color=\"r\")\n",
    "plt.plot(xs, sszn(xs), color=\"g\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "baseFilePath = \"/Users/jwpilly/Research/Synchro/Study_v2/\"\n",
    "for root, folders, files in os.walk(baseFilePath):\n",
    "    if not root.endswith(\"synchro\") or \"MACOSX\" in root:\n",
    "        continue\n",
    "    sfolders = sorted(folders, key=lambda x : int(x.split(\"_\")[0].replace(\"t\", \"\")))\n",
    "    efolders = [f for f in sfolders[:] if \"prep\" not in f]\n",
    "    efolderpaths = [root + \"/\" + f + \"/\" for f in efolders]\n",
    "    efoldertimes = [segmentNoiseFile(efolderpaths[i]) for i in range(len(efolderpaths))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "baseFilePath = \"/Users/jwpilly/Research/Synchro/Study_v2/\"\n",
    "presults = []\n",
    "for root, folders, files in os.walk(baseFilePath):\n",
    "    if not root.endswith(\"swipe\"):\n",
    "        continue\n",
    "    sfolders = sorted(folders, key=lambda x : int(x.split(\"_\")[0].replace(\"t\", \"\")))\n",
    "    efolders = [f for f in sfolders[:] if \"prep\" not in f]\n",
    "    efolderpaths = [root + \"/\" + f + \"/\" for f in efolders]\n",
    "    efoldertimes = [(efolderpaths[i].split(\"_\")[-1][:-1] + str(i), processSwipeFile(efolderpaths[i])) for i in range(len(efolderpaths))]\n",
    "    presults.append(efoldertimes)\n",
    "presultsa = [[i[1] for i in t] for t in [sorted(r) for r in presults]]\n",
    "paverages = reduce(lambda x, y : np.array(x) + np.array(y), presultsa) / len(presultsa)\n",
    "print(paverages)\n",
    "swipe_sit = np.mean([paverages[i] for i in range(4)])\n",
    "swipe_walk = np.mean([paverages[i + 4] for i in range(4)])\n",
    "swipe_times = paverages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.clf()\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "objects = ('Practice 1', 'Practice 2', 'Eval 1', 'Eval 2')\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = paverages\n",
    "# plt.bar(y_pos, performance, align='center', alpha=0.5)\n",
    "plt.plot(y_pos, [paverages[i] for i in range(4)], label=\"Sit\")\n",
    "plt.plot(y_pos, [paverages[i + 4] for i in range(4)], label=\"Walk\")\n",
    "plt.xticks(y_pos, objects)\n",
    "plt.ylim((1.5,2.5))\n",
    "plt.ylabel('Time (s)')\n",
    "plt.title('Swipe Times')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.savefig(\"swipetimes.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(syncresults)\n",
    "print(thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 10, 6\n",
    "plt.clf()\n",
    "labels = [\"Swipe\", \"0.6\", \"0.7\", \"0.8\", \"0.9\"]\n",
    "width= 0.3\n",
    "ind = np.arange(len(labels))\n",
    "plt.bar(ind, [np.mean(swipe_times[:4]), np.mean(syncresults[2][:4]), np.mean(syncresults[4][:4]), np.mean(syncresults[6][:4]), np.mean(syncresults[8][:4])], width=width, label=\"Sit\")\n",
    "plt.bar(ind + width * 1.2, [np.mean(swipe_times[4:]), np.mean(syncresults[2][4:]), np.mean(syncresults[4][4:]), np.mean(syncresults[6][4:]), np.mean(syncresults[8][4:])], width=width, label=\"Walk\")\n",
    "plt.xticks( 0.1 + ind + width / 1.5, labels)\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Sync Time (s)\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.savefig(\"overall_performance_bar.png\", bbox_inches='tight')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install -U matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l = [0,1,2,2,4,4,56]\n",
    "# [x+1 for x in l if x >= 45]\n",
    "list2 = [ind for ind, x in enumerate(l) if x > 45]\n",
    "print(list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list2 = [x for ind, x in enumerate(list1) if 4 > ind > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "windowDataSet = loadWindows()\n",
    "vizWindows(windowDataSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "liveCorrDataSet = loadLiveCorrData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(x for x in liveCorrDataSet[liveCorrDataSet.keys()[0]].direction if x != null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate all reference graph\n",
    "syncList = []\n",
    "for f in windowDataSet.keys():        \n",
    "    syncList.append('_'.join(f.split(\"_\")[0:3]))\n",
    "    syncList = sorted(set(syncList))\n",
    "\n",
    "for i, element in enumerate(syncList):    \n",
    "    \n",
    "    # Make folders\n",
    "    paths = [figurePath + \"reference/\"]\n",
    "    for path in paths:\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "\n",
    "    figureFile = figurePath + \"reference/\" + element + \"_summary.pdf\"\n",
    "    fig, axs = plt.subplots(2, 1, sharex=True, sharey=False)\n",
    "    fig.set_size_inches(15,15)\n",
    "    for key in windowDataSet:\n",
    "        if syncList[i] in key:\n",
    "            df = windowDataSet[key]\n",
    "            axs[0].plot(df.timestamp, df.feature, '-r', label=\"feature\")\n",
    "            axs[0].plot(df.timestamp, df.reference, '-b', label=\"reference\")\n",
    "            axs[1].plot(df.timestamp, df.feature, '-g', label=\"feature\")\n",
    "            axs[1].plot(df.timestamp, df.adjusted, '-b', label=\"adjusted\")    \n",
    "    axs[0].set_title(\"feat-ref:\")\n",
    "    axs[1].set_title(\"feat-adjref:\")\n",
    "    fig.savefig(figureFile)\n",
    "    plt.close()\n",
    "print(\"done generating all references\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize sets\n",
    "avg={}\n",
    "leftPeaksSet={}\n",
    "rightPeaksSet={}\n",
    "angles={}\n",
    "\n",
    "for key in rawDataSet:\n",
    "    \n",
    "    print(key)\n",
    "\n",
    "    # Initialize data structures\n",
    "    anglesList=[]\n",
    "    avgList=[]\n",
    "    left=[]\n",
    "    leftPeaks = []\n",
    "    rightPeaks = []\n",
    "\n",
    "    # Get data\n",
    "    df = subsampleDataSet[key]\n",
    "    left = leftTicksSet[key]\n",
    "    right = rightTicksSet[key]\n",
    "    \n",
    "    \n",
    "    # Per window based on number of left peaks\n",
    "    for i in range(1, len(left)):\n",
    "\n",
    "        # Get slice of window from all data\n",
    "        dataFrame = df[df['timestamp'].between(left[i-1], left[i], inclusive=True)]\n",
    "\n",
    "        # Calculate average time and vector across window\n",
    "        avgTime = np.sum(dataFrame[0:].timestamp)/len(dataFrame)\n",
    "        avgVector = [np.sum(dataFrame[0:].x)/len(dataFrame), np.sum(dataFrame[0:].y)/len(dataFrame), np.sum(dataFrame[0:].z)/len(dataFrame)]\n",
    "        averageVector=np.array(avgVector)\n",
    "\n",
    "        windowAngles = []\n",
    "        windowTimes=[]\n",
    "        \n",
    "        # Per point in window\n",
    "        for j in range(0,len(dataFrame)):\n",
    "            \n",
    "            # Save timestamps per window\n",
    "            windowTimes.append(dataFrame.timestamp.values[j])\n",
    "\n",
    "            # Save angle per window \n",
    "            currentVector = []\n",
    "            currentVector = np.array([dataFrame.x.values[j], dataFrame.y.values[j], dataFrame.z.values[j]])            \n",
    "            angle = angle_between(currentVector, averageVector) \n",
    "            windowAngles.append(angle)\n",
    "            \n",
    "            # Construct list with timestamp and angle\n",
    "            anglesList.append(np.array([dataFrame.timestamp.values[j], angle]))                         \n",
    "\n",
    "        angle1index = windowAngles.index(np.max(windowAngles[0:5]))\n",
    "        angle2index = windowAngles.index(np.max(windowAngles[5:10]))\n",
    "        angle1vector = np.array([dataFrame.x.values[angle1index], dataFrame.y.values[angle1index], dataFrame.z.values[angle1index]])            \n",
    "        angle2vector = np.array([dataFrame.x.values[angle2index], dataFrame.y.values[angle2index], dataFrame.z.values[angle2index]])            \n",
    "        angle1time = windowTimes[windowAngles.index(np.max(windowAngles[0:5]))]\n",
    "        angle2time = windowTimes[windowAngles.index(np.max(windowAngles[5:10]))]\n",
    "  \n",
    "        meanmag = get_magnitude(averageVector)\n",
    "        angle1mag = get_magnitude(angle1vector)\n",
    "        angle2mag = get_magnitude(angle2vector)\n",
    "        \n",
    "#         print(angle1mag, meanmag, angle2mag)\n",
    "        \n",
    "        if angle1mag > meanmag:\n",
    "            print(\"right\")\n",
    "        elif angle2mag > meanmag:\n",
    "            print(\"left\")\n",
    "\n",
    "        left1time = left[i-1]\n",
    "        righttime = right[i-1]\n",
    "        left2time = left[i]\n",
    "        \n",
    "#         print(left1time < righttime < left2time)\n",
    "\n",
    "        A = np.square(left1time - angle1time)\n",
    "        B = np.square(righttime - angle2time)\n",
    "        C = np.square(left2time - angle2time) \n",
    "        D = np.square(righttime - angle1time)\n",
    "        \n",
    "        score1 = np.sqrt(A + C)\n",
    "        score2 = np.sqrt(B + D)\n",
    "        \n",
    "#         print(score1, score2)\n",
    "        \n",
    "#         if(score1 < score2):\n",
    "#             print(\"left\")\n",
    "#         else:\n",
    "#             print(\"right\")\n",
    "        \n",
    "#         print(\"----------\")\n",
    "\n",
    "#         print(score1, score2)\n",
    "            \n",
    "#         previousLefts = []\n",
    "#         for i in range(0, len(left)):\n",
    "#             if left[i] < firstTimestamp:\n",
    "#                 previousLefts.append(left[i])\n",
    "        \n",
    "#         previousRights = []\n",
    "#         for i in range(0, len(right)):\n",
    "#             if right[i] < secondTimestamp:\n",
    "#                 previousRights.append(right[i])        \n",
    "        \n",
    "#         leftIndicator = np.min(np.abs(previousLefts - firstTimestamp))\n",
    "#         rightIndicator = np.min(np.abs(previousRights - secondTimestamp))        \n",
    "        \n",
    "#         if leftIndicator < rightIndicator:\n",
    "# #             windowLabels.append(\"left\")\n",
    "#             print(\"left\")\n",
    "#         else:\n",
    "# #             windowLabels.append(\"right\")\n",
    "#             print(\"right\")\n",
    "\n",
    "        # Find left and right peak using max angle difference\n",
    "        leftPeaks.append(np.array([angle1time]))\n",
    "        rightPeaks.append(np.array([angle2time]))\n",
    "#         leftPeaks.append(np.array([windowTimes[windowAngles.index(np.sort(windowAngles[0:5])[-3:][0])]]))\n",
    "#         rightPeaks.append(np.array([windowTimes[windowAngles.index(np.sort(windowAngles[5:10])[-3:][0])]]))        \n",
    "\n",
    "        # Save average time and vector\n",
    "        tempList=[]\n",
    "        tempList.append(avgTime)\n",
    "        tempList.extend(avgVector)\n",
    "        avgList.append(np.array(tempList))    \n",
    "\n",
    "    # Save sets\n",
    "    angles[key] = anglesList\n",
    "    avg[key] = avgList\n",
    "    leftPeaksSet[key] = leftPeaks\n",
    "    rightPeaksSet[key] = rightPeaks    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get mean feature\n",
    "for key in rawDataSet:\n",
    "#     fig = plt.figure()\n",
    "    df = subsampleDataSet[key]\n",
    "    lefts = leftTicksSet[key]\n",
    "    rights = rightTicksSet[key]\n",
    "    avgs = avg[key]\n",
    "    angleVal = angles[key]\n",
    "    leftPeak = leftPeaksSet[key]\n",
    "    rightPeak = rightPeaksSet[key]\n",
    "    \n",
    "    fig, axs = plt.subplots(2, 1, sharex=True, sharey=False)\n",
    "    \n",
    "    axs[0].plot(df.timestamp, df.x, '-r')\n",
    "    axs[0].plot(df.timestamp, df.y, '-g')\n",
    "    axs[0].plot(df.timestamp, df.z, '-b')\n",
    "    \n",
    "    for i in lefts:\n",
    "        axs[0].axvline(i, color='b')\n",
    "        axs[1].axvline(i, color='b')\n",
    "    for i in rights:\n",
    "        axs[0].axvline(i, color='r')\n",
    "        axs[1].axvline(i, color='r')        \n",
    "    \n",
    "    for i in leftPeak:\n",
    "        axs[0].axvline(i, color='g')\n",
    "        axs[1].axvline(i, color='g')\n",
    "\n",
    "    for i in rightPeak:\n",
    "        axs[0].axvline(i, color='k')\n",
    "        axs[1].axvline(i, color='k')\n",
    "    \n",
    "    for i in range(0, len(avgs)):\n",
    "        axs[0].scatter(avgs[i][0], avgs[i][1])\n",
    "        axs[0].scatter(avgs[i][0], avgs[i][2])\n",
    "        axs[0].scatter(avgs[i][0], avgs[i][3])\n",
    "        \n",
    "    for i in range(0, len(angleVal)):\n",
    "        axs[1].scatter(angleVal[i][0], angleVal[i][1])\n",
    "#         axs[1].text(angleVal[i][0], math.degrees(angleVal[i][1])/10, str(angleVal[i][2]))\n",
    "        \n",
    "    plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize sets\n",
    "avg={}\n",
    "leftPeaksSet={}\n",
    "rightPeaksSet={}\n",
    "angles={}\n",
    "\n",
    "for key in rawDataSet:\n",
    "\n",
    "    # Initialize data structures\n",
    "    anglesList=[]\n",
    "    avgList=[]\n",
    "    left=[]\n",
    "    leftPeaks = []\n",
    "    rightPeaks = []\n",
    "\n",
    "    # Get data\n",
    "    df = subsampleDataSet[key]\n",
    "    left = leftTicksSet[key]\n",
    "    right = rightTicksSet[key]\n",
    "    \n",
    "    # Per window based on number of left peaks\n",
    "    for i in range(1, len(left)):\n",
    "\n",
    "        # Get slice of window from all data\n",
    "        dataFrame = df[df['timestamp'].between(left[i-1], left[i], inclusive=True)]\n",
    "\n",
    "        # Calculate average time and vector across window\n",
    "        avgTime = np.sum(dataFrame[0:].timestamp)/len(dataFrame)\n",
    "        avgVector = [np.sum(dataFrame[0:].x)/len(dataFrame), np.sum(dataFrame[0:].y)/len(dataFrame), np.sum(dataFrame[0:].z)/len(dataFrame)]\n",
    "        averageVector=np.array(avgVector)\n",
    "\n",
    "        windowAngles = []\n",
    "        windowTimes=[]\n",
    "        \n",
    "        # Per point in window\n",
    "        for i in range(0,len(dataFrame)):\n",
    "            \n",
    "            # Save timestamps per window\n",
    "            windowTimes.append(dataFrame.timestamp.values[i])\n",
    "\n",
    "            # Save angle per window \n",
    "            currentVector = []\n",
    "            currentVector = np.array([dataFrame.x.values[i], dataFrame.y.values[i], dataFrame.z.values[i]])            \n",
    "            angle = angle_between(currentVector, averageVector) \n",
    "            windowAngles.append(angle)\n",
    "            \n",
    "            # Construct list with timestamp and angle\n",
    "            anglesList.append(np.array([dataFrame.timestamp.values[i], angle]))                         \n",
    "\n",
    "        firstTimestamp = windowTimes[windowAngles.index(np.max(windowAngles[0:5]))]\n",
    "        secondTimestamp = windowTimes[windowAngles.index(np.max(windowAngles[5:10]))]\n",
    "            \n",
    "        # Find left and right peak using max angle difference\n",
    "        leftPeaks.append(np.array([firstTimestamp]))\n",
    "        rightPeaks.append(np.array([windowTimes[windowAngles.index(np.max(windowAngles[5:10]))]]))        \n",
    "#         leftPeaks.append(np.array([windowTimes[windowAngles.index(np.sort(windowAngles[0:5])[-3:][0])]]))\n",
    "#         rightPeaks.append(np.array([windowTimes[windowAngles.index(np.sort(windowAngles[5:10])[-3:][0])]]))        \n",
    "\n",
    "        # Save average time and vector\n",
    "        tempList=[]\n",
    "        tempList.append(avgTime)\n",
    "        tempList.extend(avgVector)\n",
    "        avgList.append(np.array(tempList))    \n",
    "\n",
    "    # Save sets\n",
    "    angles[key] = anglesList\n",
    "    avg[key] = avgList\n",
    "    leftPeaksSet[key] = leftPeaks\n",
    "    rightPeaksSet[key] = rightPeaks    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize sets\n",
    "avg={}\n",
    "leftPeaksSet={}\n",
    "rightPeaksSet={}\n",
    "angles={}\n",
    "\n",
    "for key in rawDataSet:\n",
    "    \n",
    "    print(key)\n",
    "\n",
    "    # Initialize data structures\n",
    "    anglesList=[]\n",
    "    avgList=[]\n",
    "    left=[]\n",
    "    leftPeaks = []\n",
    "    rightPeaks = []\n",
    "\n",
    "    # Get data\n",
    "    df = subsampleDataSet[key]\n",
    "    left = leftTicksSet[key]\n",
    "    right = rightTicksSet[key]\n",
    "    \n",
    "    # Per window based on number of left peaks\n",
    "    for i in range(1, len(left)):\n",
    "\n",
    "        # Get slice of window from all data\n",
    "        dataFrame = df[df['timestamp'].between(left[i-1], left[i], inclusive=True)]\n",
    "\n",
    "        # Calculate average time and vector across window\n",
    "        avgTime = np.sum(dataFrame[0:].timestamp)/len(dataFrame)\n",
    "        avgVector = [np.sum(dataFrame[0:].x)/len(dataFrame), np.sum(dataFrame[0:].y)/len(dataFrame), np.sum(dataFrame[0:].z)/len(dataFrame)]\n",
    "        averageVector=np.array(avgVector)\n",
    "\n",
    "        windowAngles = []\n",
    "        windowTimes=[]\n",
    "        \n",
    "        # Per point in window\n",
    "        for i in range(0,len(dataFrame)):\n",
    "            \n",
    "            # Save timestamps per window\n",
    "            windowTimes.append(dataFrame.timestamp.values[i])\n",
    "\n",
    "            # Save angle per window \n",
    "            currentVector = []\n",
    "            currentVector = np.array([dataFrame.x.values[i], dataFrame.y.values[i], dataFrame.z.values[i]])            \n",
    "            angle = angle_between(currentVector, averageVector) \n",
    "            windowAngles.append(angle)\n",
    "            \n",
    "            # Construct list with timestamp and angle\n",
    "            anglesList.append(np.array([dataFrame.timestamp.values[i], angle]))                         \n",
    "\n",
    "        firstTimestamp = windowTimes[windowAngles.index(np.max(windowAngles[0:5]))]\n",
    "        secondTimestamp = windowTimes[windowAngles.index(np.max(windowAngles[5:10]))]\n",
    "            \n",
    "        previousLefts = []\n",
    "        for i in range(0, len(left)):\n",
    "            if left[i] < firstTimestamp:\n",
    "                previousLefts.append(left[i])\n",
    "        \n",
    "        previousRights = []\n",
    "        for i in range(0, len(right)):\n",
    "            if right[i] < secondTimestamp:\n",
    "                previousRights.append(right[i])        \n",
    "        \n",
    "        leftIndicator = np.min(np.abs(previousLefts - firstTimestamp))\n",
    "        rightIndicator = np.min(np.abs(previousRights - secondTimestamp))        \n",
    "        \n",
    "        if leftIndicator < rightIndicator:\n",
    "#             windowLabels.append(\"left\")\n",
    "            print(\"left\")\n",
    "        else:\n",
    "#             windowLabels.append(\"right\")\n",
    "            print(\"right\")\n",
    "\n",
    "        # Find left and right peak using max angle difference\n",
    "        leftPeaks.append(np.array([firstTimestamp]))\n",
    "        rightPeaks.append(np.array([secondTimestamp]))\n",
    "#         leftPeaks.append(np.array([windowTimes[windowAngles.index(np.sort(windowAngles[0:5])[-3:][0])]]))\n",
    "#         rightPeaks.append(np.array([windowTimes[windowAngles.index(np.sort(windowAngles[5:10])[-3:][0])]]))        \n",
    "\n",
    "        # Save average time and vector\n",
    "        tempList=[]\n",
    "        tempList.append(avgTime)\n",
    "        tempList.extend(avgVector)\n",
    "        avgList.append(np.array(tempList))    \n",
    "\n",
    "    # Save sets\n",
    "    angles[key] = anglesList\n",
    "    avg[key] = avgList\n",
    "    leftPeaksSet[key] = leftPeaks\n",
    "    rightPeaksSet[key] = rightPeaks    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize sets\n",
    "avg={}\n",
    "leftPeaksSet={}\n",
    "rightPeaksSet={}\n",
    "angles={}\n",
    "\n",
    "for key in rawDataSet:\n",
    "\n",
    "    # Initialize data structures\n",
    "    anglesList=[]\n",
    "    avgList=[]\n",
    "    left=[]\n",
    "    leftVectors = []\n",
    "    rightVectors = []\n",
    "    windowLabels = []\n",
    "\n",
    "    # Get data\n",
    "    df = subsampleDataSet[key]\n",
    "    left = leftTicksSet[key]\n",
    "    right = rightTicksSet[key]\n",
    "    \n",
    "    print(key)\n",
    "    print(\"-----------------\")\n",
    "\n",
    "    \n",
    "    # Per window based on number of left peaks\n",
    "    for i in range(1, len(left)):\n",
    "\n",
    "        # Get slice of window from all data\n",
    "        dataFrame = df[df['timestamp'].between(left[i-1], left[i], inclusive=True)]\n",
    "\n",
    "        # Calculate average time and vector across window\n",
    "        avgTime = np.sum(dataFrame[0:].timestamp)/len(dataFrame)\n",
    "        avgVector = [np.sum(dataFrame[0:].x)/len(dataFrame), np.sum(dataFrame[0:].y)/len(dataFrame), np.sum(dataFrame[0:].z)/len(dataFrame)]\n",
    "        averageVector=np.array(avgVector)\n",
    "\n",
    "        windowAngles = []\n",
    "        windowTimes=[]\n",
    "        \n",
    "        maxAngle = 0\n",
    "        firstIndex = 0\n",
    "        secondIndex = 0\n",
    "        \n",
    "        # Per point in window\n",
    "        for i in range(0,len(dataFrame)):\n",
    "            \n",
    "            for j in range(0,len(dataFrame)):\n",
    "                \n",
    "                if j<i:                    \n",
    "                    # Construct list with timestamp and angle\n",
    "#                     anglesList.append(np.array([dataFrame.timestamp.values[i], angle, -1]))                         \n",
    "                    continue\n",
    "                elif i==j:\n",
    "                    continue\n",
    "                else:  \n",
    "                    \n",
    "                    # Save timestamps per window\n",
    "                    windowTimes.append(dataFrame.timestamp.values[i])\n",
    "\n",
    "                    # Save angle per window \n",
    "                    currentVector = []\n",
    "                    currentVector = np.array([dataFrame.x.values[i], dataFrame.y.values[i], dataFrame.z.values[i]])\n",
    "                    nextVector = np.array([dataFrame.x.values[j], dataFrame.y.values[j], dataFrame.z.values[j]])\n",
    "                    angle = angle_between(currentVector, nextVector) \n",
    "#                     print(i, j, angle, maxAngle)\n",
    "                    if angle > maxAngle:\n",
    "                        maxAngle = angle\n",
    "                        firstIndex = i\n",
    "                        secondIndex = j\n",
    "                        firstTimestamp = dataFrame.timestamp.values[i]\n",
    "                        secondTimestamp = dataFrame.timestamp.values[j]\n",
    "                        \n",
    "\n",
    "                    windowAngles.append(np.array([angle, i, j]))\n",
    "                    \n",
    "                    # Construct list with timestamp and angle\n",
    "                    anglesList.append(np.array([dataFrame.timestamp.values[i], angle, j]))                         \n",
    "#         print(firstIndex, secondIndex)\n",
    "\n",
    "#         print(windowAngles)\n",
    "#         print(windowAngles.index(np.max(windowAngles[0].all())), len(windowAngles))\n",
    "#         highestAngleTime = windowTimes[windowAngles.index(np.max(windowAngles))]\n",
    "#         lowestAngleTime = windowTimes[windowAngles.index(np.min(windowAngles))]\n",
    "        newLefts1 = firstTimestamp - left\n",
    "        newLefts2 = firstTimestamp - right\n",
    "\n",
    "        newRight1 = secondTimestamp - left\n",
    "        newRight2 = secondTimestamp - right\n",
    "        \n",
    "#         print(left)\n",
    "        \n",
    "        previousLefts = []\n",
    "        for i in range(0, len(left)):\n",
    "            if left[i] < firstTimestamp:\n",
    "                previousLefts.append(left[i])\n",
    "        \n",
    "        previousRights = []\n",
    "        for i in range(0, len(right)):\n",
    "            if right[i] < secondTimestamp:\n",
    "                previousRights.append(right[i])        \n",
    "        \n",
    "        leftIndicator = np.min(np.abs(previousLefts - firstTimestamp))\n",
    "        rightIndicator = np.min(np.abs(previousRights - secondTimestamp))        \n",
    "        \n",
    "        if leftIndicator < rightIndicator:\n",
    "            windowLabels.append(\"left\")\n",
    "            print(\"left\")\n",
    "        else:\n",
    "            windowLabels.append(\"right\")\n",
    "            print(\"right\")\n",
    "\n",
    "#         print(np.min(np.abs(previousRights - firstTimestamp))        )\n",
    "#         print(np.min(np.abs(previousLefts - secondTimestamp)))\n",
    "\n",
    "#         min(previousLefts, firstTimestamp)\n",
    "        \n",
    "#         print(left[x]left[for x in left < firstTimestamp]])\n",
    "        \n",
    "#         print(\"first vector distance from left: \" + str(np.min(newLefts1)))\n",
    "#         print(\"first vector distance from right: \" + str(np.min(newLefts2)))\n",
    "#         print(\"second vector distance from left: \" + str(np.min(newRight1)))\n",
    "#         print(\"second vector distance from right: \" + str(np.min(newRight2)))\n",
    "#         if np.abs(np.min(newLefts1)) <= np.abs(np.min(newRight1)):\n",
    "#             print(\"left\")\n",
    "#         else:\n",
    "#             print(\"right\")\n",
    "#         if np.abs(np.min(newLefts2)) <= np.abs(np.min(newRight2)):\n",
    "#             print(\"right\")\n",
    "#         else:\n",
    "#             print(\"left\")\n",
    "        \n",
    "        \n",
    "#         print(np.abs(np.min(newLefts1)), np.abs(np.min(newRight1)))\n",
    "#         print(np.abs(np.min(newLefts2)), np.abs(np.min(newRight2)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         closestTimeFromRight = right - highestAngleTime\n",
    "#         minLeft = np.min(closestTimeFromLeft)\n",
    "#         minRight = np.min(closestTimeFromRight) \n",
    "#         if(np.abs(minLeft) < np.abs(minRight)):\n",
    "#             print(\"left\")\n",
    "#             print(minLeft, minRight)\n",
    "#         else:\n",
    "#             print(\"right\")\n",
    "#             print(minLeft, minRight)\n",
    "\n",
    "        # Find left and right peak using max angle difference\n",
    "#         highestAngleTime = windowTimes[windowAngles.index(np.max(windowAngles[0:len(windowAngles)/2]))]]\n",
    "#         left\n",
    "        \n",
    "#         maxAngle1 = np.array([)\n",
    "#         maxAngle2 = np.array([windowTimes[windowAngles.index(np.max(windowAngles[len(windowAngles)/2:len(windowAngles)]))]]) \n",
    "#         leftPeaks.append(maxAngle1)\n",
    "#         rightPeaks.append(maxAngle2)\n",
    "        leftVectors.append(np.array([firstTimestamp]))\n",
    "        rightVectors.append(np.array([secondTimestamp]))\n",
    "\n",
    "        # Save average time and vector\n",
    "        tempList=[]\n",
    "        tempList.append(avgTime)\n",
    "        tempList.extend(avgVector)\n",
    "        avgList.append(np.array(tempList))    \n",
    "\n",
    "    # Save sets\n",
    "    angles[key] = anglesList\n",
    "    avg[key] = avgList\n",
    "    leftPeaksSet[key] = leftVectors\n",
    "    rightPeaksSet[key] = rightVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(windowAngles)\n",
    "value = np.max([windowAngles[x][0] for x in range(0,len(windowAngles))])\n",
    "print(windowAngles.index(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = [1, 3, 5]\n",
    "b = a\n",
    "a[:] = [x + 2 for x in a]\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate some data that lies along a line\n",
    "\n",
    "x = np.mgrid[-2:5:120j]\n",
    "y = np.mgrid[1:9:120j]\n",
    "z = np.mgrid[-5:3:120j]\n",
    "# \n",
    "data = np.concatenate((x[:, np.newaxis], \n",
    "                       y[:, np.newaxis], \n",
    "                       z[:, np.newaxis]), \n",
    "                      axis=1)\n",
    "\n",
    "# Perturb with some Gaussian noise\n",
    "data += np.random.normal(size=data.shape) * 0.4\n",
    "\n",
    "# Calculate the mean of the points, i.e. the 'center' of the cloud\n",
    "datamean = data.mean(axis=0)\n",
    "\n",
    "# Do an SVD on the mean-centered data.\n",
    "uu, dd, vv = np.linalg.svd(data - datamean)\n",
    "\n",
    "# Now vv[0] contains the first principal component, i.e. the direction\n",
    "# vector of the 'best fit' line in the least squares sense.\n",
    "\n",
    "# Now generate some points along this best fit line, for plotting.\n",
    "\n",
    "# I use -7, 7 since the spread of the data is roughly 14\n",
    "# and we want it to have mean 0 (like the points we did\n",
    "# the svd on). Also, it's a straight line, so we only need 2 points.\n",
    "linepts = vv[0] * np.mgrid[-7:7:2j][:, np.newaxis]\n",
    "\n",
    "# shift by the mean to get the line in the right place\n",
    "linepts += datamean\n",
    "\n",
    "# Verify that everything looks right.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mpl_toolkits.mplot3d as m3d\n",
    "\n",
    "ax = m3d.Axes3D(plt.figure())\n",
    "ax.scatter3D(*data.T)\n",
    "ax.plot3D(*linepts.T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "highestAngleTime = windowTimes[windowAngles.index(np.max(windowAngles))]\n",
    "closestTimeFromLeft = left - highestAngleTime\n",
    "closestTimeFromRight = right - highestAngleTime\n",
    "minLeft = np.min(closestTimeFromLeft)\n",
    "minRight = np.min(closestTimeFromRight)\n",
    "if(minLeft < minRight):\n",
    "    print(\"left\")\n",
    "    print(minLeft, minRight)\n",
    "else:\n",
    "    print(\"right\")\n",
    "    print(minLeft, minRight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folderPath = \"/Users/gareyes/Downloads/\"\n",
    "lefts = pd.read_csv(folderPath + \"lefts.csv\", delimiter=\",\", header=None)\n",
    "lefts.columns = ['x', 'y', 'z']\n",
    "rights = pd.read_csv(folderPath + \"rights.csv\", delimiter=\",\", header=None)\n",
    "rights.columns = ['x', 'y', 'z']\n",
    "means = pd.read_csv(folderPath + \"means.csv\", delimiter=\",\", header=None)\n",
    "means.columns = ['x', 'y', 'z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, sharex=True, sharey=False)\n",
    "axs[0].plot(lefts, '-r')\n",
    "axs[0].plot(means, '-g')\n",
    "axs[0].plot(rights, '-b')\n",
    "axs[1].plot(np.sqrt(np.square(lefts.x)+np.square(lefts.y)+np.square(lefts.z)), '-r')\n",
    "axs[1].plot(np.sqrt(np.square(means.x)+np.square(means.y)+np.square(means.z)), '-g')     \n",
    "axs[1].plot(np.sqrt(np.square(rights.x)+np.square(rights.y)+np.square(rights.z)), '-b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "for i in range(0,len(vectors)):    \n",
    "    plt.plot([0, vectors[i][0]], [0, vectors[i][1]], [0, vectors[i][2]], '-r')    \n",
    "# avgVector = np.array([np.sum(dataFrame[0:].x), np.sum(dataFrame[0:].y), np.sum(dataFrame[0:].z)])/len(dataFrame)\n",
    "plt.plot([0, avgVector[0]], [0, avgVector[1]], [0, avgVector[2]], '-k')\n",
    "plt.show()\n",
    "# plt.plot(dataFrame[0:].x)\n",
    "# plt.plot(dataFrame[0:].y)\n",
    "# plt.plot(dataFrame[0:].z)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "for i in range(1,len(dataFrame)+1):\n",
    "    plt.plot([0, dataFrame[i-1:i].x], [0, dataFrame[i-1:i].y], [0, dataFrame[i-1:i].z], '-r')    \n",
    "avgVector = np.array([np.sum(dataFrame[0:].x), np.sum(dataFrame[0:].y), np.sum(dataFrame[0:].z)])/len(dataFrame)\n",
    "plt.plot([0, avgVector[0]], [0, avgVector[1]], [0, avgVector[2]], '-k')\n",
    "plt.show()\n",
    "# plt.plot(dataFrame[0:].x)\n",
    "# plt.plot(dataFrame[0:].y)\n",
    "# plt.plot(dataFrame[0:].z)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lefts = np.array(lefts)\n",
    "rights = np.array(rights)\n",
    "# lm = lefts.mean(axis = 0)\n",
    "# rm = rights.mean(axis = 0)\n",
    "# dm = lm - rm\n",
    "deltas = [lefts[vi] - rights[vi] for vi in range(min(len(lefts), len(rights)))]# deltas = [lefts[vi] - rights[vi] \n",
    "deltas = np.array(deltas)\n",
    "dm = deltas.mean(axis=0)\n",
    "\n",
    "vectors = np.array(vectors)\n",
    "soa =np.array(plotVector) \n",
    "def get_mid(v):\n",
    "    return np.min(v) + ((np.max(v) - np.min(v)) / 2)\n",
    "\n",
    "X,Y,Z,U,V,W = zip(*soa)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.quiver(X,Y,Z,U,V,W, pivot=\"tail\", arrow_length_ratio=0.02, length=magv(vectors.mean(axis=0)), linewidths=0.3)\n",
    "# dv = np.array([np.array(get_mid(U)), get_mid(np.array(V)), get_mid(np.array(W))])\n",
    "dv = rights.mean(axis=0)\n",
    "pvectors = [[dv[0], dv[1], dv[2], dm[0], dm[1], dm[2]]]\n",
    "# pvectors = [[0, 0, 0, dm[0], dm[1], dm[2]]]\n",
    "pvectors = np.array(pvectors)\n",
    "X2,Y2,Z2,U2,V2,W2 = zip(*pvectors)\n",
    "ax.quiver(X2,Y2,Z2,U2,V2,W2, pivot=\"tail\", arrow_length_ratio=0.02,color=\"red\", length=magv(deltas.mean(axis=0)), linewidths=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot([0,1],[0,1],[0,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "timevectors = np.array(timevectors)\n",
    "# from sklearn.decomposition import PCA\n",
    "# pca = PCA(n_components=1)\n",
    "# pca.fit(vectors)\n",
    "# pa = pca.components_[0]\n",
    "# pvectors = [[0,0,0,pa[0],pa[1],pa[2]]]\n",
    "# pvectors = np.array(pvectors)\n",
    "soa =np.array(plotVector) \n",
    "\n",
    "X,Y,Z,U,V,W = zip(*soa)\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "# ax.quiver(X,Y,Z,U,V,W, pivot=\"tail\", arrow_length_ratio=0.05)\n",
    "# X2,Y2,Z2,U2,V2,W2 = zip(*pvectors)\n",
    "# ax.quiver(X2,Y2,Z2,U2,V2,W2, pivot=\"tail\", arrow_length_ratio=0.05,color=\"red\")\n",
    "# ax.set_xlim([-1,1])\n",
    "# ax.set_ylim([-1,1])\n",
    "# ax.set_zlim([-1,1])\n",
    "\n",
    "plt.ion()\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.set_xlim(-50, 50)\n",
    "ax.set_ylim(-50, 50)\n",
    "ax.set_zlim(-50, 50)\n",
    "prev_time = timevectors[0][0] - .001\n",
    "total_time = timevectors[timevectors.shape[0]-1][0] - timevectors[0][0]\n",
    "print((total_time))\n",
    "# for i in range(timevectors.shape[0]):\n",
    "i = 0\n",
    "import time as system_time\n",
    "while i < timevectors.shape[0]:\n",
    "    # print((timevectors[i]))\n",
    "    time = timevectors[i][0]\n",
    "    x = timevectors[i][1]\n",
    "    y = timevectors[i][2]\n",
    "    z = timevectors[i][3]\n",
    "    ax.scatter(x, y, z)\n",
    "    pause_time = time - prev_time\n",
    "    pause_time /= 100000\n",
    "    if pause_time == 0.0:\n",
    "        pause_time = .00001\n",
    "    print((pause_time))\n",
    "    # system_time.sleep(pause_time)\n",
    "    plt.pause(pause_time)\n",
    "    prev_time = timevectors[i][0]\n",
    "    i += 5\n",
    "while True:\n",
    "    plt.pause(0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "timevectors = np.array(timevectors)\n",
    "timevectors[:,1] = timevectors[:,1][::-1]\n",
    "timevectors[:,2] = timevectors[:,2][::-1]\n",
    "timevectors[:,3] = timevectors[:,3][::-1]\n",
    "soa = np.array(plotVector)\n",
    "\n",
    "X,Y,Z,U,V,W = zip(*soa)\n",
    "\n",
    "plt.ion()\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.set_xlim(-200, 200)\n",
    "ax.set_ylim(-200, 200)\n",
    "ax.set_zlim(-300, 200)\n",
    "# prev_time = timevectors[0][0]\n",
    "# total_time = timevectors[timevectors.shape[0]-1][0] - timevectors[0][0]\n",
    "# time = timevectors[i][0]\n",
    "i=0\n",
    "while i < timevectors.shape[0]:\n",
    "    x = timevectors[i][1]\n",
    "    y = timevectors[i][2]\n",
    "    z = timevectors[i][3]\n",
    "    ax.scatter(x, y, z)\n",
    "    plt.pause(0.0000001)\n",
    "    i += 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ax.quiver(X, Y, Z, U, V, W)\n",
    "\n",
    "soa = np.array([[0, 0, 3, 2], [0, 0, 1, 1], [0, 0, 9, 9]])\n",
    "X, Y, U, V = zip(*soa)\n",
    "plt.figure()\n",
    "ax = plt.gca()\n",
    "ax.quiver(X, Y, U, V, angles='xy', scale_units='xy', scale=1)\n",
    "ax.set_xlim([-1, 10])\n",
    "ax.set_ylim([-1, 10])\n",
    "plt.draw()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = pd.Series([0, 3, 0, 4])\n",
    "print(s)\n",
    "print(s.nonzero())\n",
    "print(s.iloc[s.nonzero()[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # Get all data\n",
    "    for key in rawSet:\n",
    "\n",
    "        # Get data\n",
    "        print(key)\n",
    "        liveCorrData = liveCorrSet[key[:-7]+\"corr.csv\"]\n",
    "        offlineCorrData = offlineCorrSet[key[:-7]+\"offline.csv\"]\n",
    "\n",
    "        # Plot combined correlation\n",
    "        figureFile = figurePath + \"correlation/\" + key[:-7] + \"correlation.pdf\"\n",
    "        plot_corrs(liveCorrData, offlineCorrData, fileName=figureFile, saveToFile=True)        \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Get all data\n",
    "for key in windowDataSet:\n",
    "\n",
    "    # Plot windows\n",
    "    fig, axs = plt.subplots(2, 1, sharex=True, sharey=False)\n",
    "    fig.set_size_inches(15,15)\n",
    "\n",
    "    \n",
    "    # Get data\n",
    "    df = windowDataSet[key]    \n",
    "    axs[0].plot(df.timestamp, df.feature, '-r', label=\"feature\")\n",
    "    axs[0].plot(df.timestamp, df.reference, '-b', label=\"reference\")\n",
    "    axs[1].plot(df.timestamp, df.feature, '-g', label=\"feature\")\n",
    "    axs[1].plot(df.timestamp, df.adjusted, '-b', label=\"adjusted\")    \n",
    "axs[0].set_title(\"feat-ref: \" + str(scipy.stats.pearsonr(df.feature, df.reference)[0]))\n",
    "axs[1].set_title(\"feat-adjref: \" + str(scipy.stats.pearsonr(df.feature, df.adjusted)[0]))\n",
    "plt.show()\n",
    "print(\"done plotting windows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(dataFrame.left.nonzero())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "windowDataSet = loadWindows()\n",
    "# windowDataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mpl.rcParams['legend.fontsize'] = 10\n",
    "\n",
    "# fig = plt.figure()\n",
    "# ax = fig.gca(projection='3d')\n",
    "theta = np.linspace(-4 * np.pi, 4 * np.pi, 100)\n",
    "z = np.linspace(-2, 2, 100)\n",
    "r = z**2 + 1\n",
    "x = r * np.sin(theta)\n",
    "y = r * np.cos(theta)\n",
    "# ax.plot(x, y, z, label='parametric curve')\n",
    "# ax.legend()\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "\n",
    "soa = np.array([[0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1, 2]])\n",
    "\n",
    "X, Y, Z, U, V, W = zip(*soa)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.quiver(X, Y, Z, U, V, W)\n",
    "ax.set_xlim([-1, 1])\n",
    "ax.set_ylim([-1, 1])\n",
    "ax.set_zlim([-1, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get all data\n",
    "rawDataSet = loadRawData()\n",
    "featureDataSet = loadFeatureData()\n",
    "offlineCorrDataSet = loadOfflineCorrData()\n",
    "\n",
    "# Iterate the data set\n",
    "for key in rawDataSet:\n",
    "\n",
    "    lefts = []\n",
    "    rights = []\n",
    "    \n",
    "    # Get data\n",
    "    rawData, lefts, rights = rawDataSet[key]\n",
    "    \n",
    "    # Plot magnetometer and feature\n",
    "    print(lefts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for User Study Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Rename all study files\n",
    "\n",
    "path = \"/Volumes/HanSolo/Dropbox/Georgia Tech/Synchro/Data/User Study/Flat/noise/\"\n",
    "for root, folders, files in os.walk(path):\n",
    "    for f in files:\n",
    "        oldName = root + f\n",
    "        newName = root + f.replace(\"ssensorData\", \"raw\")\n",
    "        os.rename(oldName, newName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove extra comma at the end of csv file\n",
    "\n",
    "path = \"/Volumes/HanSolo/Dropbox/Georgia Tech/Synchro/Data/User Study/Flat/noise/\"\n",
    "inputPath = path + \"data/\"\n",
    "outputPath = path + \"out/\"\n",
    "if not os.path.exists(outputPath):\n",
    "    os.makedirs(outputPath)\n",
    "for root, folders, files in os.walk(inputPath):\n",
    "    for f in files:       \n",
    "        with open(root + f, 'rb') as input_file:\n",
    "            with open(outputPath + f, 'wb') as output_file:\n",
    "                data = csv.reader(input_file)\n",
    "                writer = csv.writer(output_file)\n",
    "                for line in data:\n",
    "                    newLine = line[:-1]\n",
    "                    writer.writerow(newLine)            \n",
    "        input_file.close()\n",
    "        output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Synthetic Data and Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EWMA Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputData = [6.0,8.0,1.0,2.0,3.0,10.0,5.0,7.0,1.0,3.0]\n",
    "# 6.0,6.2,5.68,5.311999999999999,5.080799999999999,5.5727199999999995,5.515447999999999,5.663903199999999,5.19751288,4.977761591999999,\n",
    "# 6.0,7.800000000000001,1.6800000000000002,1.968,2.8968000000000003,9.28968,5.428968,6.8428968,1.58428968,2.858428968,\n",
    "# outputData = [7.0,6.8,6.62,6.258,6.4322,5.98898,5.490082,5.741073800000001,6.066966420000001,5.560269778]\n",
    "\n",
    "# Python version\n",
    "def get_ewma(df, alpha=1):\n",
    "    # Compute alpha and exponential moving average\n",
    "    span = (2-alpha)/alpha\n",
    "    df_smooth = pd.ewma(df, span=span, adjust=False)\n",
    "    return df_smooth\n",
    "newDf = pd.DataFrame(inputData, columns=[\"data\"])\n",
    "pythonValues = get_ewma(newDf, alpha=0.1)\n",
    "# print(pythonValues)\n",
    "\n",
    "# Android version\n",
    "alpha = 0.1\n",
    "androidValues = []\n",
    "for i in inputData:\n",
    "    avgValue = (alpha * avgValue) + ((1 - alpha)*i)\n",
    "    androidValues.append(avgValue)\n",
    "# print(androidValues)\n",
    "\n",
    "# Java version\n",
    "alpha = 0.1\n",
    "javaValues = []\n",
    "oldValue = inputData[0]\n",
    "for i in inputData:\n",
    "    # Jason\n",
    "    newValue = oldValue + alpha * (i - oldValue);\n",
    "    oldValue = newValue;\n",
    "    javaValues.append(newValue)\n",
    "# print(javaValues)\n",
    "\n",
    "plt.plot(inputData, '-r')\n",
    "plt.plot(androidValues, '-g')\n",
    "plt.plot(javaValues, '-b')\n",
    "plt.plot(pythonValues, '-k')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
